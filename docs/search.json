[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stan and R for Pharmacometrics",
    "section": "",
    "text": "Welcome to the book on Stan and R for Pharmacometrics. Here we provide an introduction to Bayesian principles, building blocks for starting to code in Stan, technical details on how to apply Stan and R for standard pharmacometrics workflows in drug research and development along with some hands on exercises.\nThis will serve as a repository that can be freely accessed by anyone who would like to learn as well as contribute to the content. We plan to continuously add and enrich the content here when new code, approaches, or methods become available.\nWe hope you will all benefit from this resource.\nSincerely,  Casey, Yasong, Arya, and Pavan"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction to Bayesian Methods for Inference",
    "section": "",
    "text": "Code\n# library(collapsibleTree)\nlibrary(kableExtra)\nlibrary(patchwork)\nlibrary(plotly)\nlibrary(latex2exp)\nlibrary(magick)\nlibrary(gganimate)\nlibrary(bayesplot)\nlibrary(tidybayes)\nlibrary(loo)\nlibrary(posterior)\nlibrary(cmdstanr)\nlibrary(tidyverse)\n\ntheme_set(theme_bw(base_size = 16, base_line_size = 2))\nregister_knitr_engine()"
  },
  {
    "objectID": "intro.html#introduction",
    "href": "intro.html#introduction",
    "title": "1  Introduction to Bayesian Methods for Inference",
    "section": "1.1 Introduction",
    "text": "1.1 Introduction\nThis document is is meant to introduce you to the most basic elements of Bayesian inference. It is in no way comprehensive, but it will hopefully give you a platform of understanding so that you can get as much as possible out of this tutorial.\nHere are a few references that we’ve found useful in our Bayesian lives:\n\nBayesian Data Analysis\nStatistical Rethinking\nRegression and Other Stories\nJose Storopoli’s slides on Bayesian Statistics\nStan Discourse"
  },
  {
    "objectID": "intro.html#statistical-inference",
    "href": "intro.html#statistical-inference",
    "title": "1  Introduction to Bayesian Methods for Inference",
    "section": "1.2 Statistical Inference",
    "text": "1.2 Statistical Inference\n\nIn any experiment, survey, observational study, or clinical trial, we are using sample data to try to answer some question(s) about the population of interest.\nWe collect (sample) data \\(\\left(X\\right)\\) to estimate parameters \\(\\left(\\theta\\right)\\) and perform some sort of inference (point estimation, confidence intervals, hypothesis tests, ) to say something/make decisions about the “population.”\nHowever, learning from the data is complicated by the natural variability of the measurements, so we can’t find the “correct” values of the parameters.\nWe want to quantify our knowledge/uncertainty about the parameters with point estimates, i.e., “typical” values, and uncertainty estimates such as standard errors, CV%, and confidence intervals.\n\n\n1.2.1 Motivating Examples\nWe want to estimate the proportion of the population that likes Mountain Dew Cheesecake.\n\n\n\n\n1.2.1.1 Example 1\nAfter collecting \\(n = 6\\) data points where \\(x = 5\\) people liked it, we want to make inferences about the population parameter \\(\\theta\\), the proportion of people in the population that likes Mt. Dew Cheesecake.\n\n\n1.2.1.2 Example 2\nAfter collecting \\(n = 60\\) data points where \\(x = 50\\) people liked it, we want to make inferences about the population parameter \\(\\theta\\)"
  },
  {
    "objectID": "intro.html#frequentistlikelihoodist-approach",
    "href": "intro.html#frequentistlikelihoodist-approach",
    "title": "1  Introduction to Bayesian Methods for Inference",
    "section": "1.3 Frequentist/Likelihoodist Approach",
    "text": "1.3 Frequentist/Likelihoodist Approach\nThe most common methods for parameter estimation in non-Bayesian paradigms involve some sort of optimization. For this problem, we’ll use maximum likelihood, 1 where we find the value of \\(\\theta\\) that maximizes the likelihood2, or, in other words, we find the value of \\(\\theta\\) that maximizes the probability of observing the data that we have observed.\nIn the above example, we assume the data has a binomial distribution with \\(n\\) trials and probability of success, \\(\\theta\\), i.e. \\(X \\sim Bin(n,\\theta)\\). Then we can write out the density3 \\(f(x \\;| \\; \\theta)\\), the probability that we would would observe \\(x\\) “successes” out of \\(n\\) trials for a given value of \\(\\theta\\) for any value of \\(x \\in \\{0, 1, 2, \\ldots, n\\}\\) and \\(0 \\leq \\theta \\leq 1\\): \\[ \\begin{align}\nf(x | \\theta) &= P(X = x \\;| \\;\\theta) \\\\\n&= {n \\choose x}\\theta^x(1 - \\theta)^{n-x},\n\\;\\; x = 0, \\; 1, \\; 2, \\;\\ldots, \\; n\n\\end{align}\\]\nFor Example 1 above with \\(n = 6\\), for \\(\\theta\\) values of 0.4 and 0.75, the density of \\(X\\) looks like this:\n\n\nCode\nn_1 <- 6\nprobs <- c(0.40, 0.75)\n\nbinom_data <- tibble(x = rep(0:n_1, times = length(probs)), \n                    theta = rep(probs, each = n_1 + 1)) %>% \n  mutate(density = dbinom(x, n_1, prob = theta))\n\n\nbase_plot <- ggplot(mapping = aes(x = x, y = density,\n                                 text = paste0(\"x: \", x, \"</br></br>density: \", \n                                               round(density, 3)))) +\n  scale_x_continuous(name = \"x\",\n                     breaks = 0:n_1,\n                     labels = 0:n_1) +\n  ggtitle(\"Binomial Density\") +\n  theme(plot.title = element_text(hjust = 0.5))\n\np1 <- (base_plot + \n         geom_bar(data = filter(binom_data, theta == probs[1]),\n                  stat = \"identity\")) %>% \n  ggplotly(tooltip = \"text\") %>% \n  layout(yaxis = list(title = str_c(\"P(X = x | \\U03B8 = \", probs[1],\")\")),\n         xaxis = list(title = \"x\"))\n\np2 <- (base_plot + \n         geom_bar(data = filter(binom_data, theta == probs[2]),\n                  stat = \"identity\")) %>% \n  ggplotly(tooltip = \"text\") %>% \n  layout(yaxis = list(title = str_c(\"P(X = x | \\U03B8 = \", probs[2],\")\")),\n         xaxis = list(title = \"x\"))\n\n\nannot_base <- list(y = 1.0,\n                   font = list(size = 16), \n                   xref = \"paper\", \n                   yref = \"paper\", \n                   xanchor = \"center\", \n                   yanchor = \"bottom\", \n                   showarrow = FALSE)\n\na1 <- c(annot_base,\n        x = 0.2,\n        text = str_c(\"\\U03B8 = \", probs[1])) \n\na2 <- c(annot_base,\n        x = 0.8,\n        text = str_c(\"\\U03B8 = \", probs[2])) \n\n\nsubplot(p1, p2, titleY = TRUE, titleX = TRUE, margin = 0.08) %>% \n  layout(annotations = list(a1, a2))\n\n\n\n\n\nFigure 1.1: Two Binomial Densities\n\n\n\nBut we want to maximize the likelihood function, \\(\\mathcal{L}(\\theta \\; | \\; x)\\). Luckily for us, it is the same as the density, but is a function of \\(\\theta\\) for a given \\(x\\), instead of \\(X\\) for a given \\(\\theta\\). That is, \\(\\mathcal{L}(\\theta \\; | \\; x) = f(x \\;| \\; \\theta)\\). For Example 1 with \\(n = 6\\) and \\(x = 5\\) the likelihood is as below \\[\\begin{align}\n\\mathcal{L}(\\theta \\; | \\; x)  &= {n \\choose x}\\theta^x(1 - \\theta)^{n-x} \\\\\n&= {6 \\choose 5}\\theta^5(1 - \\theta)^{6 - 5}, \\;\\; 0 \\leq \\theta \\leq 1\n\\end{align}\\]\n\n\nCode\nn_1 <- 6\nx_1 <- 5\n\n(binom_like_plot_1 <- tibble(theta = seq(0, 1, by = 0.01)) %>% \n    mutate(likelihood = dbinom(x_1, n_1, prob = theta)) %>% \n    ggplot(aes(x = theta, y = likelihood)) +\n    geom_line(size = 2) +\n    ylab(str_c(\"L(\\U03B8 | X = \", x_1, \")\")) +\n    xlab(\"\\U03B8\") +\n    ggtitle(str_c(\"Binomial Likelihood, n = \", n_1, \", X = \", x_1)) +\n    theme(plot.title = element_text(hjust = 0.5),\n          plot.subtitle = element_text(hjust = 0.5)))\n\n\n\n\n\nFigure 1.2: Binomial likelihood.\n\n\n\n\nThe maximum likelihood estimate (MLE), \\(\\hat\\theta\\), is the value of \\(\\theta\\) that maximizes this function. That is, \\[\\hat\\theta = \\underset{\\theta}{\\mathrm{argmax}} \\;\n\\mathcal{L}(\\theta \\; | \\; x)\\]\nIntuitively, it is the value of \\(\\theta\\) that is “most likely” given the observed data. For example, in our example it doesn’t seem likely that we would observe our data if \\(\\theta = 0.25\\), but it seems more likely that we could observe this data if \\(\\theta = 0.8\\) or so.\nWe also want to quantify the uncertainty of this estimate, typically with a standard error. A larger standard error means we are more uncertain about our estimate than a smaller standard error, and in a sense, the standard error is a measure of the “pointiness” of the likelihood. The (asymptotic) standard error can be calculated as the square root of the diagonals of the inverse of the Fisher information matrix evaluated at the MLE (the observed Fisher information. See here for a more thorough discussion of MLEs, Fisher information, and the form when \\(\\theta\\) is a vector). An intuitive explanation of the relationship of the standard errors to the Fisher information matrix is that the standard error is a measure of the curvature of the likelihood. Roughly, more information in the data \\(\\implies\\) large negative values in the observed Fisher information matrix (more curvature) \\(\\implies\\) smaller values after inversion to get the variance.\nThis particular example has a simple, closed-form solution, but most of our problems in the PK/PD world require a numerical optimization, typically by some gradient-based method. So for our example, we can do this numerical optimization in R\n\n\nCode\nx_1 <- 5\nn_2 <- 6\n\nx_2 <- 50\nn_2 <- 60\n\nexample_mle_1 <- optim(par = 0.3, \n                       fn = function(theta, n, x) \n                         -dbinom(x, n, theta, log = TRUE), \n                       n = n_1, x = x_1, \n                       method = \"Brent\", \n                       hessian = TRUE, lower = 0, upper = 1)\n\nexample_mle_2 <- optim(par = 0.3, \n                       fn = function(theta, n, x) \n                         -dbinom(x, n, theta, log = TRUE), \n                       n = n_2, x = x_2, \n                       method = \"Brent\", \n                       hessian = TRUE, lower = 0, upper = 1)\n\ntribble(~Example, ~MLE, ~SE,\n        1, example_mle_1$par, sqrt(1/as.double(example_mle_1$hessian)),\n        2, example_mle_2$par, sqrt(1/as.double(example_mle_2$hessian))) %>% \n  knitr::kable(format = \"html\", digits = 3, align = \"c\",\n               caption = \"Numerical MLE\") %>% \n  kableExtra::kable_styling(bootstrap_options = \"striped\", full_width = FALSE,\n                            position = \"center\")\n\n\n\n\nNumerical MLE\n \n  \n    Example \n    MLE \n    SE \n  \n \n\n  \n    1 \n    0.833 \n    0.152 \n  \n  \n    2 \n    0.833 \n    0.048 \n  \n\n\n\n\n\n\n\nCode\nx_ticks <- sort(c(example_mle_1$par, seq(0, 1, by = 0.25)))\ntick_colors <- if_else(x_ticks == example_mle_1$par, \"red\", \"black\")\n\np1 <- binom_like_plot_1 +\n  geom_segment(aes(x = example_mle_1$par, y = 0, \n                   xend = example_mle_1$par, \n                   yend = dbinom(x_1, n_1, example_mle_1$par)),\n               color = \"red\") +\n  scale_x_continuous(breaks = x_ticks,\n                     labels = as.character(round(x_ticks, 3))) +\n  theme(axis.text.x = element_text(color = tick_colors)) +\n  ggtitle(str_c(\"n = \", n_1, \", X = \", x_1))\n\n\np2 <- tibble(theta = seq(0, 1, by = 0.01)) %>% \n  mutate(likelihood = dbinom(x_2, n_2, prob = theta)) %>% \n  ggplot(aes(x = theta, y = likelihood)) +\n  geom_line(size = 2) +\n  ylab(str_c(\"L(\\U03B8 | X = \", x_2, \")\")) +\n  xlab(\"\\U03B8\") +\n  ggtitle(str_c(\"Binomial Likelihood, n = \", n_2, \", X = \", x_2)) +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5)) +\n  geom_segment(aes(x = example_mle_2$par, y = 0, \n                   xend = example_mle_2$par, \n                   yend = dbinom(x_2, n_2, example_mle_2$par)),\n               color = \"red\") +\n  scale_x_continuous(breaks = x_ticks,\n                     labels = as.character(round(x_ticks, 3))) +\n  theme(axis.text.x = element_text(color = tick_colors)) +\n  ggtitle(str_c(\"n = \", n_2, \", X = \", x_2))\n\n(ggpubr::ggarrange(p1, p2, \n                  ncol = 2) %>% \n    ggpubr::annotate_figure(\n      top = ggpubr::text_grob(\"Binomial Likelihoods with MLE\", \n                              size = 24))) \n\n\n\n\n\nTwo binomial likelihoods with MLE."
  },
  {
    "objectID": "intro.html#bayesian-approach",
    "href": "intro.html#bayesian-approach",
    "title": "1  Introduction to Bayesian Methods for Inference",
    "section": "1.4 Bayesian Approach",
    "text": "1.4 Bayesian Approach\nClassical methods treat the parameter(s) as fixed and the data random and then find a point estimate and standard error using only information from the data4. In contrast, Bayesian methods treat the parameter(s) as a random variable and consider the data fixed and then make inferences based on a proper distribution. These methods initially allocate probability to all possible parameter values via a prior distribution and then reallocate probability when new information is gained. Bayesian inference depends on our ability to quantify the posterior distribution of the parameters conditioned on the data. This posterior distribution contains all of our knowledge about \\(\\theta\\) (prior knowledge and knowledge obtained from the data).\n\n1.4.1 Bayes’ Theorem\nThe form of the posterior distribution follows from Bayes’ Theorem5: \\[\\begin{align}\n\\color{blue}{p\\left( \\theta | x\\right)} &=\n\\frac{p(x, \\theta)}{\\color{red}{f\\left( x \\right)}} \\notag \\\\\n&= \\frac{\\color{green}{f\\left( x | \\theta\\right)}\\color{orange}{p\\left( \\theta\n\\right)}}{\\color{red}{f\\left( x \\right)}} \\notag \\\\\n&= \\frac{\\color{green}{f\\left( x | \\theta\\right)}\\color{orange}{p\\left( \\theta\n\\right)}}{\\color{red}{\\int \\limits_{\\Theta}f\\left( x | \\theta\\right)\np\\left( \\theta \\right) \\mathrm{d}\\theta}}\n\\end{align} \\tag{1.1}\\]\n\n1.4.1.1 Prior Distribution\nWe begin with the prior distribution to quantify our knowledge/beliefs about \\(\\theta\\) before we collect data. For our examples, we might assume\n\na “noninformative” prior distribution6 and use a \\(Uniform(0, 1)\\) (equivalent to a \\(Beta(1,1)\\)7 distribution for \\(p(\\theta)\\)). This is a simple way to express our ignorance about \\(\\theta\\).\nan informative prior that expresses our belief that most people will not like Mt. Dew Cheesecake. We might quantify this with a \\(Beta(2, 3)\\) distribution.\n\n\n\nCode\nalpha_1 <- 1\nbeta_1 <- 1\n\nalpha_2 <- 2\nbeta_2 <- 3\n\ntheta <- seq(0, 1, .01)\n\ndf_prior <- tibble(theta = theta, prior_1 = dbeta(theta, alpha_1, beta_1),\n                   prior_2 = dbeta(theta, alpha_2, beta_2)) %>% \n  pivot_longer(c(prior_1, prior_2), names_to = \"example\", values_to = \"value\") %>% \n  arrange(example)\n\n\n(prior_plot <- ggplot(data = df_prior, aes(x = theta, y = value, \n                                          color = example)) +\n  geom_line(size = 2) +\n  ylab(str_c(\"p(\\U03B8)\")) +\n  xlab(\"\\U03B8\") +\n  scale_color_manual(name = \"Prior\",\n                     breaks = c(\"prior_1\", \"prior_2\"),\n                     values = c(\"purple\", \"orange\"),\n                     labels = c(\"Uniform (Beta(1, 1))\",\n                                \"Informative (Beta(2, 3))\")))\n\n\n\n\n\nFigure 1.3: Two beta priors.\n\n\n\n\n\n\n1.4.1.2 Likelihood\nThe likelihood is the same (see Figure 1.2) as we had when we were using optimization methods (recall that \\(\\mathcal{L}(\\theta \\; | \\; x) = f(x \\;| \\; \\theta)\\)).\n\n\nCode\ndf_likelihood <- tibble(theta = theta) %>%\n  mutate(likelihood_1 = dbinom(x_1, n_1, prob = theta),\n         likelihood_2 = dbinom(x_2, n_2, prob = theta)) %>%\n  pivot_longer(c(likelihood_1, likelihood_2), names_to = \"example\",\n               values_to = \"value\") %>%\n  arrange(example)\n\nplot_likelihood <- ggplot(data = df_likelihood, aes(x = theta, y = value,\n                                                  color = example)) +\n  geom_line(size = 2) +\n  ylab(str_c(\"f(x | (\\U03B8) = L(\\U03B8 | X)\")) +\n  xlab(\"\\U03B8\") +\n  scale_color_manual(name = \"Likelihood\",\n                     breaks = c(\"likelihood_1\", \"likelihood_2\"),\n                     values = c(\"green\", \"purple\"),\n                     labels = c(str_c(\"x = \", x_1, \", n = \", n_1),\n                                str_c(\"x = \", x_2, \", n = \", n_2)))\nplot_likelihood\n\n\n\n\n\nUnnormalized Likelihoods.\n\n\n\n\nOne thing to notice for each of these likelihoods is that they do not integrate to 18, one of the requirements for a function to be a probability distribution. However, the likelihood can be normalized to integrate to 1 by multiplying by a constant. This will be touched upon again in the section on marginal distributions and in Appendix A, and all future plots will plot a scaled likelihood for visual purposes.\n\n\nCode\nlike_binom <- function(theta, n, x) dbinom(x, n, theta)\nintegral_1 <- integrate(like_binom, n = n_1, x = x_1, \n                        lower = 0, upper = 1)$value\nintegral_2 <- integrate(like_binom, n = n_2, x = x_2, \n                        lower = 0, upper = 1)$value\n\ntribble(~Example, ~Integral,\n        \"1\", integral_1,\n        \"2\", integral_2) %>% \n  knitr::kable(format = \"html\", digits = 3, align = \"c\",\n               caption = \"Likelihood Integrals\") %>% \n  kableExtra::kable_styling(bootstrap_options = \"striped\", full_width = FALSE,\n                            position = \"center\")\n\n\n\n\n\nTable 1.1:  Likelihood Integrals \n \n  \n    Example \n    Integral \n  \n \n\n  \n    1 \n    0.143 \n  \n  \n    2 \n    0.016 \n  \n\n\n\n\n\n\nRegardless, the likelihood is a key part of Bayes’ Theorem and contains the information about \\(\\theta\\) obtained from the data.\n\n\n1.4.1.3 Marginal Distribution\nConsider the case where \\(X\\) has the sampling density \\(f(x \\;|\\; \\theta)\\) and \\(\\theta\\) is a random variable with density \\(p(\\theta)\\). Then the joint density of \\(X\\) and \\(\\theta\\) is \\[f(x, \\; \\theta) = f(x \\;|\\; \\theta) \\; p(\\theta),\\] which you will recognize as the numerator in Equation 1.1. The marginal distribution9 of \\(X\\) is then \\[\\begin{align}\nf(x) &= \\int \\limits_{\\Theta}f(x, \\; \\theta) \\; \\mathrm{d}\\theta \\notag \\\\\n&= \\int \\limits_{\\Theta}f\\left( x | \\theta\\right) p\\left( \\theta \\right)\n\\mathrm{d}\\theta\n\\end{align} \\tag{1.2}\\]\nThat is, the marginal density of \\(X\\) is equal to the conditional sampling density of \\(X\\) averaged over all possible values of \\(\\theta\\). It can also be thought of as a description of the predictions we would make for \\(X\\) given only our prior knowledge, which is why this is sometimes called the “prior predictive distribution”10. See Appendix A for more discussion on the marginal distribution.\nIn practice, the marginal distribution is often analytically intractable, and it is often just cast aside so that we have the posterior distribution up to a constant: \\[\\begin{align}\n\\color{blue}{p( \\theta \\; | \\; x)} &=\n\\frac{\\color{green}{f( x \\; | \\; \\theta)}\\; \\color{orange}{p( \\theta )}}\n{\\color{red}{f\\left( x \\right)}} \\notag \\\\\n&\\propto \\color{green}{f( x \\; | \\; \\theta)}\\; \\color{orange}{p( \\theta )}\n\\end{align} \\tag{1.3}\\]\nThis inability to find a closed-form for the marginal distribution is not a problem. Modern computational methods such as Markov Chain Monte Carlo (MCMC) allow us to sample from the posterior in such a way that the sample represents the true posterior distribution arbitrarily closely, and we can perform our inference based on this sample.\n\n\n1.4.1.4 Posterior Distribution\nThe posterior distribution (See Appendix B for a derivation of the posterior for our examples) is the key to all Bayesian inference. It combines the prior distribution and the likelihood and contains all of the available information about \\(\\theta\\) (prior knowledge and information obtained from the data):\n\n\nCode\ndf_all <- tibble(theta = theta, \n                 posterior_1_1 = dbeta(theta, alpha_1 + x_1, beta_1 + n_1 - x_1),\n                 posterior_1_2 = dbeta(theta, alpha_1 + x_2, beta_1 + n_2 - x_2),\n                 posterior_2_1 = dbeta(theta, alpha_2 + x_1, beta_2 + n_1 - x_1),\n                 posterior_2_2 = dbeta(theta, alpha_2 + x_2, beta_2 + n_2 - x_2)) %>% \n  pivot_longer(starts_with(\"posterior\"), names_to = \"example\", \n               values_to = \"value\") %>% \n  bind_rows(df_prior, \n            df_likelihood) \n\ndf_all %>% \n  filter(example %in% c(\"prior_2\", \"likelihood_1\", \"posterior_2_1\")) %>% \n  mutate(value = if_else(example == \"likelihood_1\", value/(1/n_1), value)) %>% \n  ggplot(aes(x = theta, y = value, group = example, color = example)) +\n  geom_line(size = 1.25) +\n  xlab(\"\\U03B8\") + \n  ylab(NULL) +\n  scale_color_manual(name = NULL,\n                     breaks = c(\"prior_2\", \"likelihood_1\", \"posterior_2_1\"),\n                     values = c(\"orange\", \"green4\", \"blue\"),\n                     labels = c(\"Prior\", \"Likelihood\", \"Posterior\"))\n\n\n\n\n\nPosterior as a combination of the likelihood and prior.\n\n\n\n\nOnce we have our posterior distribution, we can make similar inferences as in frequentist inference:\n\nPoint estimates - We have a proper distribution now, so we can report the posterior mean, median, or mode ( 0.636, 0.645, and 0.667, respectively).\n\n\n\nCode\nposterior_point_estimates <- tribble(\n  ~type, ~value,\n  \"mean\", (alpha_2 + x_1)/(alpha_2 + x_1 + beta_2 + n_1 - x_1),\n  \"median\", qbeta(0.5, alpha_2 + x_1, beta_2 + n_1 - x_1),\n  \"mode\", (alpha_2 + x_1 - 1)/(alpha_2 + x_1 + beta_2 + n_1 - x_1 - 2)) %>% \n  mutate(density = dbeta(value, alpha_2 + x_1, beta_2 + n_1 - x_1))\n\n(p_posterior_with_point_estimates <- df_all %>% \n  filter(example == \"posterior_2_1\") %>%  \n  ggplot(aes(x = theta, y = value)) +\n  geom_line(size = 1.25, color = \"blue\") +\n  xlab(\"\\U03B8\") + \n  ylab(str_c(\"p(\\U03B8 | x)\")) +\n  geom_segment(data = posterior_point_estimates, \n             mapping = aes(x = value, y = 0, xend = value, yend = density,\n                           color = type),\n             size = 1.15) +\n  scale_color_manual(name = \"Point Estimate\",\n                     breaks = c(\"mean\", \"median\", \"mode\"),\n                     labels = c(\"Mean\", \"Median\", \"Mode\"),\n                     values = c(\"red\", \"purple\", \"black\")))\n\n\n\n\n\nPosterior point estimates.\n\n\n\n\n\nStandard deviation (analogous to a standard error) - for our example the standard deviation is 0.139\nEven better, we can make interval estimates, e.g.credible intervals, with a natural probabilistic interpretation:\n\n“There is a 95% chance that the true proportion of people who like Mt. Dew cheesecake is between 0.348 and 0.878.”\n“There is an 82.81% chance that the true proportion of people who like Mt. Dew cheesecake is at least 0.5.”\n\n\n\n\nCode\ninterval_base <- df_all %>% \n  filter(example == \"posterior_2_1\") %>%\n  bind_rows(tibble(theta = c(qbeta(0.025, alpha_2 + x_1, beta_2 + n_1 - x_1),\n                             qbeta(0.975, alpha_2 + x_1, beta_2 + n_1 - x_1)),\n                   example = \"posterior_2_1\") %>% \n              mutate(value = dbeta(theta, alpha_2 + x_1, beta_2 + n_1 - x_1))) %>% \n  arrange(theta) %>% \n  ggplot(aes(x = theta, y = value)) +\n  geom_line(size = 1.25, color = \"blue\") +\n  xlab(\"\\U03B8\") + \n  ylab(str_c(\"p(\\U03B8 | x)\"))\n\nx_ticks <- sort(c(qbeta(c(0.025, 0.975), alpha_2 + x_1, beta_2 + n_1 - x_1), \n                  seq(0, 1, by = 0.25)))\ntick_colors <- if_else(x_ticks %in% \n                         qbeta(c(0.025, 0.975), \n                               alpha_2 + x_1, beta_2 + n_1 - x_1), \n                       \"red\", \"black\")\n\np_ci <- interval_base +\n  geom_area(data = df_all %>% \n              filter(example == \"posterior_2_1\",\n                     between(theta, \n                             qbeta(0.025, alpha_2 + x_1, beta_2 + n_1 - x_1), \n                             qbeta(0.975, alpha_2 + x_1, beta_2 + n_1 - x_1))),\n            fill = \"blue\", alpha = 0.25) + \n  ggtitle(\"95% credible interval\") +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  scale_x_continuous(breaks = x_ticks,\n                     labels = as.character(round(x_ticks, 3))) +\n  theme(axis.text.x = element_text(color = tick_colors))\n\np_gt_50 <- interval_base +\n  geom_area(data = df_all %>% \n              filter(example == \"posterior_2_1\",\n                     between(theta, 0.50, 1)),\n            fill = \"blue\", alpha = 0.25) +\n  ggtitle(\"P(\\U03B8 > 0.50 | X = 5)\") +\n  theme(plot.title = element_text(hjust = 0.5))\n\np_ci + p_gt_50\n\n\n\n\n\n\n\n\n\n\nWe can also look at the posterior predictive distribution (See Appendix C for a derivation of the posterior predictive distribution for our examples). This is the distribution of possible unobserved values conditional on our observed values. For example, we could look at the density for a future observation, \\(x^*\\), for different future values of \\(n, n^*\\).\n\n\n\nCode\nx <- 5\nn <- 6\n\nn_star <- c(6, 10)\n\n(p_analytical_ppd <- tibble(x_star = c(0:n_star[1], 0:n_star[2]),\n       n_star = c(rep(n_star[1], times = n_star[1] + 1),\n                  rep(n_star[2], times = n_star[2] + 1))) %>% \n  mutate(density = extraDistr::dbbinom(x_star, n_star, \n                                       alpha_2 + x, beta_2 + n - x)) %>% \n  ggplot() +\n  geom_bar(aes(x = x_star, y = density),\n           stat = \"identity\") +\n  scale_x_continuous(name = latex2exp::TeX(\"$x^*$\"),\n                     breaks = 0:max(n_star),\n                     labels = 0:max(n_star)) +\n  ylab(latex2exp::TeX(\"$p(x^* | \\\\; x) = P(X^*= x^* | \\\\; x)\")) +\n  ggtitle(\"Posterior Predictive Density\") +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  facet_wrap(~n_star, scales = \"free_x\", \n             labeller = label_bquote(n^\"*\" == .(n_star))))"
  },
  {
    "objectID": "intro.html#mcmc",
    "href": "intro.html#mcmc",
    "title": "1  Introduction to Bayesian Methods for Inference",
    "section": "1.5 Markov Chain Monte Carlo",
    "text": "1.5 Markov Chain Monte Carlo\nAs mentioned in the section on marginal distributions, we often have an analytically intractable marginal distribution, which means we cannot get a closed-form solution for the posterior distribution11. Markov Chain Monte Carlo (MCMC) methods allow us to sample from the posterior distribution, and we can perform our inference based on numerical integration of the sample, rather than analytical integration when the closed-form is known.\nTraditional Monte Carlo methods include the Gibbs sampler and Metropolis-Hastings. These methods are fast and easy-to-implement, but often lead to inefficient sampling from the posterior. Stan12 implements a more modern method called the No U-Turn Sampler (NUTS) that is itself an extension of Hamiltonian Monte Carlo. While more computationally intensive than Gibbs sampling or Metropolis-Hastings, it samples more efficiently from the posterior than those more traditional methods.\n\n1.5.1 MCMC - An Illustration\nLet’s assume we know the posterior density up to a constant, as in Equation 1.3):\n\n\n\n\n\nSince we don’t have the marginal distribution, we can’t analytically integrate our posterior, but we can sample from it using MCMC methods:\nVideo\nIn practice we use multiple chains to sample from the target distribution:\n\n\nCode\n#---------- time series\nstatic_tsplot <- df %>%\n  rename(Chain = \"chain\") %>% \n  ggplot(aes(x = iteration, y = theta, group = Chain, color = Chain)) +\n  geom_line(size = 1, alpha = 0.7) + \n  scale_linetype_manual(name = \"Chain\", \n                        values = c(2,2)) + \n  labs(color = \"Chain\", x = \"Iteration\", y = \"\\U03B8\") +\n  theme(legend.position = \"none\") +\n  facet_wrap(~Chain, nrow = 4, labeller = label_both)\n\n# animate\nanimated_tsplot <- static_tsplot +\n  transition_reveal(along = iteration, \n                    range = as.integer(c(1, max(df$iteration) + 50))) \n\n# save\na_gif <- animate(animated_tsplot,\n                 width = 600, \n                 height = 600)\n\n#---------- histogram\n\n# histogram\nstatic_hist <- df %>% \n  rename(Chain = \"chain\") %>% \n  split(.$iteration) %>% \n  accumulate(~ bind_rows(.x, .y)) %>% \n  bind_rows(.id = \"frame\") %>% \n  mutate(frame = as.integer(frame)) %>%\n  ggplot(aes(x = theta, fill = Chain)) +\n  geom_histogram(#aes(y = ..density..), \n    color = \"white\", bins = 15, alpha = 0.7, position = \"identity\") + \n  labs(x = \"\\U03B8\", fill = \"Chain\") +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.title.y = element_blank(),\n        legend.position = \"none\") +\n  facet_wrap(~Chain, nrow = 4, labeller = label_both) \n\n# animate\nanim_hist <- static_hist + \n  transition_manual(frame) +\n  ease_aes(\"linear\") +\n  enter_fade() +\n  exit_fade()\n\n# save\nb_gif <- animate(anim_hist,\n                 width = 600, \n                 height = 600)\n\n\na_mgif <- image_read(a_gif)\nb_mgif <- image_read(b_gif)\n\n#---------- put side-by-side\nnew_gif <- image_append(c(a_mgif[1], b_mgif[1]))\nfor(i in 2:min(length(a_mgif))){\n  combined <- image_append(c(a_mgif[i], b_mgif[i]))\n  new_gif <- c(new_gif, combined)\n}\n\nnew_gif\n\n\n\nWhen we have collected all of our samples, we combine the chains, and the resulting samples should be distributed according to the true posterior:\n\n\nCode\nmcmc_hist(samples$draws(\"theta\"), freq = FALSE) +\n  geom_line(data = data_1, mapping = aes(x = theta, y = density)) +\n  xlab(\"\\U03B8\")\n\n\n\n\n\nSamples overlayed with the true distribution.\n\n\n\n\n\n\n1.5.2 MCMC for Our Examples\nOur examples are very simple and have a simple closed form for the posterior distribution (see Appendix B) and posterior predictive distribution (see Appendix C). But let’s imagine the marginal distribution was intractable, and we weren’t able to find the closed form for the posterior. We’ll write the model in Stan and then sample from the posterior.\n\n\nCode\n\ndata{\n\n  int<lower = 0> x; // observed positive responses\n  int<lower = x> n; // number of responses\n  \n  real<lower = 0> alpha; // Value of alpha for the prior distribution\n  real<lower = 0> beta;  // Value of beta for the prior distribution\n\n  int n_new;               // length of new n values you want to simulate for \n  array[n_new] int n_star; // Number of future respondents for posterior predictions\n  \n}\nparameters{\n\n  real<lower = 0, upper = 1> theta;\n\n}\nmodel{\n  // Priors\n  theta ~ beta(alpha, beta);\n  \n  // Likelihood\n  x ~ binomial(n, theta);\n}\ngenerated quantities{\n  \n  array[n_new] int x_star = binomial_rng(n_star, theta); \n  \n}\n\n\n\n\nCode\nstan_data <- list(x = 5,\n                  n = 6,\n                  alpha = 2,\n                  beta = 3,\n                  n_new = 2,\n                  n_star = c(6, 10))\n\nfit <- model_beta_binomial$sample(data = stan_data,\n                                  iter_warmup = 1000,\n                                  iter_sampling = 1000,\n                                  chains = 4,\n                                  refresh = 0) \n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 1.5 seconds.\n\n\nCode\nsummary <- summarize_draws(fit$draws(\"theta\"), \n                           mean, median, sd, pr_gt_half = ~ mean(. >= 0.5),\n                           ~quantile2(.x, probs = c(0.025, 0.975)), rhat,\n                           ess_bulk, ess_tail)\n\n\nNow we can perform inference using our samples. We can look at the full posterior distributions with either histograms or density plots:\n\n\nCode\ncolor_scheme_set(\"blue\")\n\npost_hist <- mcmc_hist(fit$draws(\"theta\"), freq = FALSE) +\n  ylab(str_c(\"p(\\U03B8 | x)\")) +\n  scale_x_continuous(name = \"\\U03B8\",\n                     breaks = c(0, 0.25, 0.5, 0.75, 1),\n                     labels = c(0, 0.25, 0.5, 0.75, 1),\n                     limits = c(0, 1))\n\npost_dens <- mcmc_dens(fit$draws(\"theta\")) +\n  ylab(str_c(\"p(\\U03B8 | x)\")) +\n  scale_x_continuous(name = \"\\U03B8\",\n                     breaks = c(0, 0.25, 0.5, 0.75, 1),\n                     labels = c(0, 0.25, 0.5, 0.75, 1),\n                     limits = c(0, 1))\n\npost_hist /\n  post_dens\n\n\n\n\n\n\n\n\n\nWe can also make similar inferences as before:\n\nPoint estimates - for these samples, the posterior mean and median are 0.634 and 0.643, respectively.\n\n\n\nCode\npost_dens +\n  geom_vline(data = summary %>% \n               select(mean, median) %>% \n               rename_all(str_to_title) %>% \n               pivot_longer(Mean:Median, names_to = \"Estimate\"), \n             mapping = aes(xintercept = value, color = Estimate),\n               size = 1.15) +\n  scale_color_manual(name = \"Point Estimate\",\n                     breaks = c(\"Mean\", \"Median\"),\n                     labels = c(\"Mean\", \"Median\"),\n                     values = c(\"red\", \"purple\"))\n\n\n\n\n\n\n\n\n\n\nThe posterior standard deviation for \\(\\theta\\) is 0.137.\nInterval estimates\n\n95% credible interval - “There is a 95% chance that the true proportion of people who like Mt. Dew cheesecake is between 0.341 and 0.869.”\n“There is an 82.6% chance that the true proportion of people who like Mt. Dew cheesecake is at least 0.5.”\n\n\n\n\nCode\nx_ticks <- sort(c(summary$q2.5, summary$q97.5,\n                  seq(0, 1, by = 0.25)))\ntick_colors <- if_else(x_ticks %in% c(summary$q2.5, summary$q97.5), \n                       \"red\", \"black\")\n\nsample_ci <- mcmc_areas(fit$draws(\"theta\"), prob = 0.95, point_est = \"none\") +\n  ggtitle(\"95% credible interval\") +\n  scale_x_continuous(name = \"\\U03B8\",\n                     breaks = x_ticks,\n                     labels = as.character(round(x_ticks, 3)),\n                     limits = c(0, 1)) +\n  scale_y_discrete(breaks = \"theta\",\n                   limits = \"theta\",\n                   labels = c(\"theta\" = \"\"),\n                   expand = expansion(add = c(0, 0))) +\n  theme(axis.text.x = element_text(color = tick_colors),\n        plot.title = element_text(hjust = 0.5))\n\n\n\nblah <- mcmc_dens(fit$draws(\"theta\"), alpha = 0) +\n  ggtitle(\"P(\\U03B8 > 0.50 | X = 5)\") +\n  scale_x_continuous(name = \"\\U03B8\",\n                     limits = c(0, 1)) +\n  theme(plot.title = element_text(hjust = 0.5))\n\nblah_d <- ggplot_build(blah)$data[[1]]\n\nsample_gt_half <- blah +\n  geom_area(data = subset(blah_d, x >= 0.5), aes(x = x, y = y), fill = \"blue\", \n            alpha = 0.25)\n\n\nsample_ci +\n  sample_gt_half\n\n\n\n\n\n\n\n\n\nWe can look at the posterior predictive distribution13 for \\(n^* = 6\\) and \\(n^* = 10\\):\n\n\nCode\n(p_sample_ppd <- fit$draws(format = \"draws_df\") %>% \n  spread_draws(x_star[i]) %>% \n  ungroup() %>% \n  mutate(n_star = stan_data$n_star[i]) %>% \n  ggplot() +\n  geom_bar(aes(x = x_star, group = n_star, y = ..prop..)) +\n  scale_x_continuous(name = latex2exp::TeX(\"$x^*$\"),\n                     breaks = 0:max(stan_data$n_star),\n                     labels = 0:max(stan_data$n_star)) +\n  ylab(latex2exp::TeX(\"$p(x^* | \\\\; x) = P(X^*= x^* | \\\\; x)\")) +\n  ggtitle(\"Posterior Predictive Density\") +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  facet_wrap(~n_star, scales = \"free_x\", \n             labeller = label_bquote(n^\"*\" == .(n_star))))\n\n\n\n\n\n\n\n\n\n\n1.5.2.1 Comparison with the Analytical Posterior\nTo compare the true, analytical posterior with the sampled posterior we can look at the full densities:\n\n\nCode\nmcmc_dens(fit$draws(\"theta\")) +\n  geom_line(data = df_all %>% \n              filter(example == \"posterior_2_1\"),\n            mapping = aes(x = theta, y = value), color = \"red\", size = 2) +\n  ylab(str_c(\"p(\\U03B8 | x)\")) +\n  scale_x_continuous(name = \"\\U03B8\",\n                     breaks = c(0, 0.25, 0.5, 0.75, 1),\n                     labels = c(0, 0.25, 0.5, 0.75, 1),\n                     limits = c(0, 1))\n\n\n\n\n\n\n\n\n\nand posterior parameter and quantile estimates:\n\n\nCode\nmcmc_estimates <- summary %>% \n  pivot_longer(c(mean, median, sd, starts_with(\"q\")), \n               names_to = \"Variable\", values_to = \"MCMC\") %>% \n  select(Variable, MCMC) \n\nanalytical_estimates <- tibble(Variable = mcmc_estimates$Variable) %>% \n  mutate(Analytical = case_when(Variable == \"mean\" ~ \n                                  (alpha_2 + x_1)/(alpha_2 + x_1 + beta_2 + n_1 - x_1),\n                                Variable == \"median\" ~ \n                                  qbeta(0.5, alpha_2 + x_1, beta_2 + n_1 - x_1),\n                                Variable == \"sd\" ~\n                                  sqrt((alpha_2 + x_1)*(beta_2 + n_1 - x_1)/\n                                         ((alpha_2 + beta_2 + n_1)^2*(alpha_2 + beta_2 + n_1 + 1))),\n                                Variable == \"q2.5\" ~\n                                  qbeta(0.025, alpha_2 + x_1, beta_2 + n_1 - x_1),\n                                Variable == \"q97.5\" ~\n                                  qbeta(0.975, alpha_2 + x_1, beta_2 + n_1 - x_1),\n                                TRUE ~ NA_real_))\n\nestimates <- analytical_estimates %>% \n  inner_join(mcmc_estimates, by = \"Variable\") \n\n         \n\nestimates %>% \n  mutate(across(where(is.numeric), round, 3),\n         Variable = case_when(Variable == \"mean\" ~ \"Mean\",\n                              Variable == \"median\" ~ \"Median\",\n                              Variable == \"sd\" ~ \"Std. Dev.\",\n                              Variable == \"q2.5\" ~ \"2.5th percentile\",\n                              Variable == \"q97.5\" ~ \"97.5th percentile\",\n                              TRUE ~ NA_character_)) %>% \n  # kbl(caption = \"<center>Posterior Estimates for &theta;<center>\") %>%\n  kbl(caption = \"Posterior Estimates for \\U03B8\") %>%\n  kable_classic(full_width = F) %>% \n  add_header_above(c(\" \" = 1, \"Estimate\" = 2))\n\n\n\n\nPosterior Estimates for θ\n \n\n\nEstimate\n\n  \n    Variable \n    Analytical \n    MCMC \n  \n \n\n  \n    Mean \n    0.636 \n    0.634 \n  \n  \n    Median \n    0.645 \n    0.643 \n  \n  \n    Std. Dev. \n    0.139 \n    0.137 \n  \n  \n    2.5th percentile \n    0.348 \n    0.341 \n  \n  \n    97.5th percentile \n    0.878 \n    0.869 \n  \n\n\n\n\n\nand the posterior predictive densities:\n\n\nCode\nd1 <- ggplot_build(p_sample_ppd)$data[[1]] %>% \n  mutate(type = \"MCMC\")\nd2 <- ggplot_build(p_analytical_ppd)$data[[1]] %>% \n  mutate(type = \"Analytical\")\n\nbind_rows(d1, d2) %>% \n  select(x, y, type) %>% \n  mutate(n_star = rep(c(rep(n_star[1], times = n_star[1] + 1),\n                        rep(n_star[2], times = n_star[2] + 1)), times = 2)) %>% \n  ggplot() +\n  geom_bar(aes(x = x, y = y, group = type, fill = type),\n           stat = \"identity\", position = \"dodge\") +\n  scale_x_continuous(name = latex2exp::TeX(\"$x^*$\"),\n                     breaks = 0:max(n_star),\n                     labels = 0:max(n_star)) +\n  scale_fill_manual(name = NULL,\n                    breaks = c(\"Analytical\", \"MCMC\"),\n                    values = c(\"red\", \"blue\")) +\n  ylab(latex2exp::TeX(\"$p(x^* | \\\\; x) = P(X^*= x^* | \\\\; x)\")) +\n  ggtitle(\"Posterior Predictive Density\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        legend.position = \"bottom\") +\n  facet_wrap(~n_star, scales = \"free_x\", \n             labeller = label_bquote(n^\"*\" == .(n_star)))\n\n\n\n\n\n\n\n\n\nYou can see that all posterior quantities are similar between the analytical solutions and those estimated through MCMC."
  },
  {
    "objectID": "intro.html#summarytldr",
    "href": "intro.html#summarytldr",
    "title": "1  Introduction to Bayesian Methods for Inference",
    "section": "1.6 Summary/tl;dr",
    "text": "1.6 Summary/tl;dr\n\nClassical methods find point estimates through optimization of some objective function (often some function of the likelihood) and quantify uncertainty by examining the curvature of the likelihood at the point estimate.\nBayesian methods quantify our knowledge about the parameter(s) with a distribution. From this distribution, we can obtain point estimates, uncertainty estimates, and make predictions for future data.\nThe posterior distribution is a combination of our prior distribution and the data and contains all of our knowledge about the parameter(s).\nThe likelihood contains all of the information from the data.\nThe use of the prior distribution can range from being a nuisance that serves simply as a catalyst that allows us to express uncertainty via Bayes’theorem to a means to stabilize the sampling algorithm to actually incorporating knowledge about the parameter(s) before collecting data.\nWe often can’t get the posterior distribution in closed-form, but we can generally use Markov Chain Monte Carlo methods to obtain a sample that approximates the posterior.\nStan is a great tool for performing the MCMC sampling."
  },
  {
    "objectID": "intro.html#appendices",
    "href": "intro.html#appendices",
    "title": "1  Introduction to Bayesian Methods for Inference",
    "section": "1.7 Appendices",
    "text": "1.7 Appendices\n\n1.7.1 Appendix A - More on the Marginal Distribution\nWhile the marginal distribution is often analytically intractable, there are some cases where we can find the closed form. For our examples, we have assumed \\[\\begin{align}\nX|\\theta &\\sim Binomial(n, \\; \\theta) \\\\\n\\theta &\\sim Beta(\\alpha, \\; \\beta)\n\\end{align}\\] Then \\[\\begin{align}\nf(x) &= \\int \\limits_{\\Theta} p\\left( x, \\; \\theta \\right) \\; \\mathrm{d}\\theta \\\\\n&= \\int \\limits_{\\Theta}f\\left( x | \\theta\\right) p\\left( \\theta \\right) \\;\n\\mathrm{d}\\theta  \\notag \\\\\n&= \\int_0^1 {n \\choose x}\\theta^x(1 - \\theta)^{n-x}\n\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\n\\theta^{\\alpha - 1}(1 - \\theta)^{\\beta - 1} \\; \\mathrm{d}\\theta \\notag \\\\\n&= {n \\choose x} \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\; \\Gamma(\\beta)}\n\\frac{\\Gamma(\\alpha + x) \\;\n\\Gamma(\\beta + n - x)}{\\Gamma(\\alpha + \\beta + n)} \\notag \\\\\n&= {n \\choose x} \\frac{B(\\alpha + x, \\; \\beta + n - x)}{B(\\alpha, \\; \\beta)},\n\\; x \\in \\{0, 1, \\ldots, n\\}, \\;\\;\\; \\alpha, \\; \\beta > 0\n\\end{align} \\tag{1.4}\\] a beta-binomial distribution14.\nIf we assume the prior distribution \\(\\theta \\sim Beta(1, 1)\\) to express our ignorance of \\(\\theta\\), then Equation 1.4 evaluates to \\[f(x) = \\frac{1}{n+1}, \\; x = 0, 1, \\ldots, n\\] the density for a discrete uniform.\nIf we assume the prior distribution \\(\\theta \\sim Beta(2, 3)\\) to express our prior knowledge of \\(\\theta\\), then \\[\\begin{align}\nf(x) &= \\frac{n!}{x!\\;(n-x)!}\\;\\frac{4!}{1!\\;2!} \\;\n\\frac{(x+1)! \\; (n-x+2)!}{(n+4)!}, \\;\\; x = 0, 1, \\ldots, n\n\\end{align}\\]\nThese two marginal distributions look like this (for Example 1 with \\(n = 6\\)):\n\n\nCode\nn <- 6\nx <- 0:n\n\nalpha_1 <- 1\nbeta_1 <- 1\n\nalpha_2 <- 2\nbeta_2 <- 3\n\ndbetabinomial <- function(x, n, alpha, beta){\n  \n  choose(n, x) * beta(alpha + x, beta + n - x)/(beta(alpha, beta)) \n    \n}\n\nmarg_dists <- tibble(x = x) %>% \n  mutate(marginal_1 = dbetabinomial(x, n, alpha_1, beta_1),\n         marginal_2 = dbetabinomial(x, n, alpha_2, beta_2)) %>% \n  pivot_longer(c(marginal_1, marginal_2), names_to = \"example\", \n               values_to = \"density\") %>% \n  arrange(example)\n\nbase_plot <- ggplot(mapping = aes(x = x, y = density,\n                                  text = paste0(\"x: \", x, \"</br></br>density: \",\n                                                round(density, 3)))) +\n  scale_x_continuous(name = \"x\",\n                     breaks = 0:n,\n                     labels = 0:n) +\n  ggtitle(\"Marginal Density\") +\n  theme(plot.title = element_text(hjust = 0.5))\n\np1 <- (base_plot +\n         geom_bar(data = filter(marg_dists, example == \"marginal_1\"),\n                  stat = \"identity\")) %>%\n  ggplotly(tooltip = \"text\") %>%\n  layout(yaxis = list(title = str_c(\"f(x) = P(X = x)\")),\n         xaxis = list(title = \"x\"))\n\np2 <- (base_plot +\n         geom_bar(data = filter(marg_dists, example == \"marginal_2\"),\n                  stat = \"identity\")) %>%\n  ggplotly(tooltip = \"text\") %>%\n  layout(yaxis = list(title = str_c(\"f(x) = P(X = x)\")),\n         xaxis = list(title = \"x\"))\n\nannot_base <- list(y = 1.0,\n                  font = list(size = 16),\n                  xref = \"paper\",\n                  yref = \"paper\",\n                  xanchor = \"center\",\n                  yanchor = \"bottom\",\n                  showarrow = FALSE)\n\na_1 <- c(annot_base,\n        x = 0.225,\n        text = str_c(\"\\U03B1 = \", alpha_1, \", \\U03B2 = \", beta_1))\n\na_2 <- c(annot_base,\n        x = 0.775,\n        text = str_c(\"\\U03B1 = \", alpha_2, \", \\U03B2 = \", beta_2))\n\nsubplot(p1, p2, titleY = TRUE, titleX = TRUE, margin = 0.08) %>%\n  layout(annotations = list(a_1, a_2))\n\n\n\n\nTwo marginal densities.\n\n\nStopping to think about this, these plots make sense: if we have no knowledge of \\(\\theta\\) (recall that a \\(Beta(1,1)\\) distribution is “noninformative”), then any value of \\(x\\) should be no more or less likely than any other possible value of \\(x\\) conditional on our current knowledge of \\(\\theta\\). If we have some idea of \\(\\theta\\) (a \\(Beta(2, 3)\\) describes our belief that it is likely that \\(\\theta < 0.5\\), see Figure 1.3), then we would also expect the marginal distribution of \\(X\\) to be skewed towards lower values, as seen above.\nHaving integrated \\(\\theta\\) out of the joint distribution of \\(X\\) and \\(\\theta\\), we can see that the marginal distribution is a constant in \\(\\theta\\). This constant is exactly the value needed to normalize the numerator in Equation 1.1, making the posterior distribution a true distribution.\nFor example, if \\(\\theta \\sim Beta(1,1)\\), then \\(p(\\theta) = 1, \\; 0 \\leq \\theta \\leq 1\\). So the numerator in Equation 1.1 is \\[\\begin{align}\nf(x \\; | \\; \\theta)\\;p(\\theta) &= f(x \\;| \\; \\theta) \\times 1 \\\\\n&= f(x \\; | \\; \\theta) \\\\\n&= \\mathcal{L}(\\theta \\; | \\; x)\n\\end{align}\\] and we have already integrated our likelihoods for Examples 1 and 2 in Table 1.1. If we divide the values in this table by \\(f(x) = \\frac{1}{n+1}\\) for the corresponding \\(n\\), we get exactly 1 for both, i.e., we normalized the numerator, and so the posterior distribution is now a true distribution15.\n\n\n1.7.2 Appendix B - Derivation of the Posterior in Our Examples\nFor our examples, we have assumed \\[\\begin{align}\nX|\\theta &\\sim Binomial(n, \\; \\theta) \\\\\n\\theta &\\sim Beta(\\alpha, \\; \\beta)\n\\end{align}\\] Then \\[\\begin{align}\np( \\theta \\; | \\; x) &=\n\\frac{f( x \\; | \\; \\theta)\\; p( \\theta )}{f\\left( x \\right)} \\notag \\\\\n&\\propto f( x \\; | \\; \\theta)\\; p( \\theta ) \\notag \\\\\n&= {n \\choose x}\\theta^x(1 - \\theta)^{n-x} \\;\n\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\;\\Gamma(\\beta)} \\;\n\\theta^{\\alpha - 1}\\;(1-\\theta)^{\\beta - 1} \\notag \\\\\n&\\propto \\theta^{\\alpha + x - 1}\\;(1 - \\theta)^{\\beta + n - x - 1} \\notag \\\\\n&\\implies \\theta\\;|\\;x \\sim Beta(\\alpha + x, \\; \\beta + n - x)\n\\end{align}\\]\n\n\n1.7.3 Appendix C - Derivation of the Posterior Predictive Distribution in Our Examples\nIn Appendix B we derived the posterior distribution of \\(\\theta\\). Here we will derive the posterior predictive distribution used for posterior predictive checking and to simulate/predict future data.\nAfter collecting \\(x\\) positive responses out of \\(n\\) respondents, we want the density for the number of positive responses, \\(x^*\\), out of \\(n^*\\) future respondents: \\[\\begin{align}\nf(x^* | x) &= \\int \\limits_{\\Theta} p\\left( x^*, \\; \\theta  | x \\right) \\;\n\\mathrm{d}\\theta \\notag \\\\\n&= \\int \\limits_{\\Theta}f\\left( x^* | \\theta, x \\right)\np\\left( \\theta | x \\right) \\; \\mathrm{d}\\theta  \\notag \\\\\n&= \\int \\limits_{\\Theta}f\\left( x^* | \\theta \\right)\np\\left( \\theta | x \\right) \\; \\mathrm{d}\\theta \\\\\n&= \\int_0^1 {n^* \\choose x^*}\\theta^{x^*}(1 - \\theta)^{n^* - x^*}\n\\frac{\\Gamma(\\alpha + \\beta + n)}{\\Gamma(\\alpha + x)\\Gamma(\\beta + n - x)}\n\\theta^{\\alpha + x - 1}(1 - \\theta)^{\\beta + n - x - 1} \\;\n\\mathrm{d}\\theta \\notag \\\\\n&= {n^* \\choose x^*} \\frac{\\Gamma(\\alpha + \\beta + n)}{\\Gamma(\\alpha + x) \\; \\Gamma(\\beta + n - x)}\n\\frac{\\Gamma(\\alpha + x + x^*) \\;\n\\Gamma(\\beta + n - x + n^* - x^*)}{\\Gamma(\\alpha + \\beta + n + n^*)} \\notag \\\\\n&= {n^* \\choose x^*}\n\\frac{B(\\alpha + x + x^*, \\; \\beta + n - x + n^* - x^*)}{B(\\alpha + x, \\; \\beta + n - x)},\n\\; x^* \\in \\{0, 1, \\ldots, n^*\\}\n\\end{align}\\] another beta-binomial distribution.\nYou can see that this is similar to Equation 1.2 in that it shows the posterior predictive distribution as an average of conditional predictions over the posterior distribution of \\(\\theta\\). That is, it is equal to the conditional sampling density of \\(X^*\\)averaged over all possible values of \\(\\theta\\) conditioned on our data."
  },
  {
    "objectID": "stancodeblocks.html",
    "href": "stancodeblocks.html",
    "title": "2  Introduction to Stan",
    "section": "",
    "text": "Stan is a platform for statistical modeling and statistical computation. While it can perform maximum likelihood estimation (similar to NONMEM’s FOCE), it is mainly used for Bayesian inference."
  },
  {
    "objectID": "stancodeblocks.html#the-no-u-turn-sampler",
    "href": "stancodeblocks.html#the-no-u-turn-sampler",
    "title": "2  Introduction to Stan",
    "section": "2.1 The No-U-Turn-Sampler",
    "text": "2.1 The No-U-Turn-Sampler\nSee here for an illustration of Stan’s No-U-Turn Sampler (NUTS)."
  },
  {
    "objectID": "stancodeblocks.html#the-stan-language",
    "href": "stancodeblocks.html#the-stan-language",
    "title": "2  Introduction to Stan",
    "section": "2.2 The Stan Language",
    "text": "2.2 The Stan Language\nThe Stan language is strongly statically typed compiled language (similar to C or C++), meaning we declare a type (int, real, array, vector, matrix, ...) for each declared variable, and this type cannot change. This is contrary to interpreted languages like R and Python where we don’t have to declare a variable type, and we can overwrite and change a variable throughout the program.\nWe write a Stan model down in a .stan file, after which the Stan program is internally translated to C++ and compiled."
  },
  {
    "objectID": "stancodeblocks.html#stan-code-blocks",
    "href": "stancodeblocks.html#stan-code-blocks",
    "title": "2  Introduction to Stan",
    "section": "2.3 Stan Code Blocks",
    "text": "2.3 Stan Code Blocks\nA stan model is written in code blocks, similarly to NONMEM with $PROB, $DATA, $PK, .... There is a good explanation of the Stan code blocks here. Here will give a brief overview:\n\n2.3.1 functions\n\nThe functions block is an optional block at the beginning of the program where user-defined functions appear.\nUser defined random number generator functions and probability distributions can be defined here\nVoid functions (those that return no value) are allowed\nExample: Function to define a one-compartment model\n\n\nfunctions{\n  \n  real depot_1cmt(real dose, real cl, real v, real ka, \n                  real time_since_dose){\n    \n    real ke = cl/v;\n    \n    real cp = dose/v * ka/(ka - ke) * \n              (exp(-ke*time_since_dose) - exp(-ka*time_since_dose));\n    \n    return cp;\n    \n  }\n  \n}\n\n\n\n2.3.2 data\n\nData are specified upfront and remain fixed\n\nThey are either specified in the block or read from outside\n\nThey are read once at the beginning of the process\n\nExample: Define observed PK (dv) data (and PD if you have it). We can also define our independent variables (time), parameters for our prior distributions (scale_x), covariates, times at which we want to make predictions (time_pred), or anything else we want to input into the model.\n\n\ndata{\n  \n  int n_obs;                    // Number of observations\n  real<lower = 0> dose;         // Dose amount\n  array[n_obs] real time;       // Times at which we have observations\n  real time_of_first_dose;      // Time of first dose\n  vector[n_obs] dv;             // Observed PK data\n  \n  real<lower = 0> scale_cl;     // Prior Scale parameter for CL\n  real<lower = 0> scale_v;      // Prior Scale parameter for V\n  real<lower = 0> scale_ka;     // Prior Scale parameter for KA\n  \n  real<lower = 0> scale_sigma;  // Prior Scale parameter for lognormal error\n  \n  int n_pred;                   // Number of new times at which to make a prediction\n  array[n_pred] real time_pred; // New times at which to make a prediction\n \n}\n\n\n\n2.3.3 transformed data\n\nWe declare and define variables that do not need to be changed when running the program.\nWe can hard code variables here (n_cmt).\nWe can also manipulate our data variables into a form we will use later in the Stan program.\nThe statements in transformed data are executed only once and directly after reading the data in the data block.\n\n\ntransformed data{ \n  \n  vector[n_obs] time_since_dose = to_vector(time) - time_of_first_dose;\n  vector[n_pred] time_since_dose_pred = to_vector(time_pred) - \n                                        time_of_first_dose;\n  int n_cmt = 2;                                        \n  \n}\n\n\n\n2.3.4 parameters\n\nParameters are altered during the sampling process. They are sampled by Stan\n\nThese are the ones we provide priors and initial estimates for later on\n\nCan specify bounds here\nExample: Define the parameters for the one-compartment depot model and constrain the absorption rate constant to be larger than elimination to ensure no flip-flop kinetics.\n\n\nparameters{  \n  \n  real<lower = 0> CL;     \n  real<lower = 0> V;\n  real<lower = CL/V> KA;\n  \n  real<lower = 0> sigma;\n  \n}\n\n\n\n2.3.5 transformed parameters\n\nWe define and calculate variables that are needed for the calculation of the posterior density or other values we want to keep. In practice, this means we calculate values needed to compute the likelihood.\nIf parameters depend on both data and parameters, we specify them in the transformed parameters block.\nIf parameters depend on only data, they should be specified in transformed data.\nThe statements in transformed parameters are calculated at every leapfrog step in the NUTS algorithm, so the calculation is relatively expensive. Quantities that you wish to keep but aren’t necessary for computing the posterior density should be computed in generated quantities.\nExample: Calculate the PK expected value (ipred) before accounting for the residual error\nThis calculation can be done here or in the model block but usually model is reserved for stochastic elements where as this block is typically used for deterministic calculations.\n\n\ntransformed parameters{\n  vector[n_obs] ipred;\n  \n  for(i in 1:n_obs){\n    ipred[i] = depot_1cmt(dose, CL, V, KA, time_since_dose[i]);\n  }\n  \n}\n\n\n\n2.3.6 model\n\nWe define the model here\nStochastic definitions and sampling statements are included here\n\nConstraints on parameters and the statements in this block define prior distributions\n\nLikelihood statement is defined here\nExample: Specifying the prior distributions (CL ~ , V ~, KA ~)\nExample: Likelihood dv is defined in vectorized notation here.\n\n\nmodel{ \n  \n  // Priors\n  CL ~ cauchy(0, scale_cl);\n  V ~ cauchy(0, scale_v);\n  KA ~ normal(0, scale_ka) T[CL/V, ];\n  \n  sigma ~ normal(0, scale_sigma);\n  \n  // Likelihood\n  dv ~ lognormal(log(ipred), sigma);\n}\n\n\n\n2.3.7 generated quantities\n\nUsed to calculate a derived quantity or some other quantity you wish to keep in the output\nUsed to make predictions\nThis block is executed only once per iteration, so is computationally inexpensive.\nExample: Posterior predictive check (dv_ppc) or a prediction of plasma concentration (cp) or a measurement of a plasma concentration (dv_pred) at an unobserved time.\nExample: We might want to draw samples for the elimination rate constant, KE, but it did not play a role in the model, so we do that here rather than in transformed parameters.\n\n\ngenerated quantities{\n  \n  real<lower = 0> KE = CL/V;\n  real<lower = 0> sigma_sq = square(sigma);\n  vector[n_obs] dv_ppc;\n  vector[n_obs] log_lik;\n  vector[n_pred] cp;\n  vector[n_pred] dv_pred;\n  vector[n_obs] ires = log(dv) - log(ipred);\n  vector[n_obs] iwres = ires/sigma;\n  \n  for(i in 1:n_obs){\n    dv_ppc[i] = lognormal_rng(log(ipred[i]), sigma);\n    log_lik[i] = lognormal_lpdf(dv[i] | log(ipred[i]), sigma);\n  }\n  for(j in 1:n_pred){\n    if(time_since_dose_pred[j] <= 0){\n      cp[j] = 0;\n      dv_pred[j] = 0;\n    }else{\n      cp[j] = depot_1cmt(dose, CL, V, KA, time_since_dose_pred[j]);\n      dv_pred[j] = lognormal_rng(log(cp[j]), sigma);\n    }\n  }\n}"
  },
  {
    "objectID": "poppk.html#introduction",
    "href": "poppk.html#introduction",
    "title": "3  A Bayesian Approach to PK/PD using Stan and Torsten",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nWe can use Stan and Torsten for the whole PK/PD workflow. In this section we will talk briefly about simulation and extensively about fitting a PopPK model to observed data and simulating/predicting future data given the results of the model fit."
  },
  {
    "objectID": "poppk.html#simple-example---single-dose-single-individual",
    "href": "poppk.html#simple-example---single-dose-single-individual",
    "title": "3  A Bayesian Approach to PK/PD using Stan and Torsten",
    "section": "3.2 Simple Example - Single Dose, Single Individual",
    "text": "3.2 Simple Example - Single Dose, Single Individual\nFirst we will show a very simple example - a single oral dose for a single individual:\n\n3.2.1 PK Model\nThe data-generating model is:\n\\[\\begin{align}\nC_i &= f(\\mathbf{\\theta}, t_i)*e^{\\epsilon_i}, \\; \\epsilon_i \\sim\nN(0, \\sigma^2) \\notag \\\\\n&= \\frac{D}{V}\\frac{k_a}{k_a - k_e}\\left(e^{-k_e(t-t_D)} - e^{-k_a(t-t_D)}\n\\right)*e^{\\epsilon_i}\n\\end{align}\\]\nwhere \\(\\mathbf{\\theta} = \\left[k_a, CL, V\\right]^\\top\\) is a vector containing the individual parameters for this individual, \\(k_e = \\frac{CL}{V}\\), \\(D\\) is the dose amount, and \\(t_D\\) is the time of the dose. We will have observations at times 0.5, 1, 2, 4, 12, and 24 and simulate the data with a dose of 200 mg and true parameter values as follows:\n\n\n\nTrue Parameter Values for Single Individual\n\n\n\n\n\n\n\n\nParameter\nValue\nUnits\nDescription\n\n\n\n\n\\(CL\\)\n5.0\n\\(\\frac{L}{h}\\)\nClearance\n\n\n\\(V\\)\n50.0\n\\(L\\)\nCentral compartment volume\n\n\n\\(k_a\\)\n0.5\n\\(h^{-1}\\)\nAbsorption rate constant\n\n\n\\(\\sigma\\)\n0.2\n-\nStandard deviation for lognormal residual error\n\n\n\n\n\n\n\n3.2.2 Simulating Data\nMany of you who simulate data in R probably use a package like mrgsolve or RxODE, and those are perfectly good tools, but we can also do our simulations directly in Stan.\n\nStanStan + Torsten\n\n\n\n\nCode\nmodel_simulate_stan <- cmdstan_model(\n  \"Stan/Simulate/depot_1cmt_lognormal_single.stan\")  \n\nmodel_simulate_stan$print()\n\n\n// First Order Absorption (oral/subcutaneous)\n// One-compartment PK Model\n// Single subject\n// lognormal error - DV = CP*exp(eps)\n// Closed form solution using a self-written function\n\nfunctions{\n  \n  real depot_1cmt(real dose, real cl, real v, real ka, \n                  real time_since_dose){\n    \n    real ke = cl/v;\n    \n    real cp = dose/v * ka/(ka - ke) * \n              (exp(-ke*time_since_dose) - exp(-ka*time_since_dose));\n    \n    return cp;\n    \n  }\n  \n}\n\ndata{\n  \n  int n_obs;\n  real<lower = 0> dose;\n  array[n_obs] real time;\n  real time_of_first_dose;\n  \n  real<lower = 0> CL;     \n  real<lower = 0> V;      \n  real<lower = CL/V> KA;     \n  \n  real<lower = 0> sigma;  \n \n}\ntransformed data{ \n  \n  vector[n_obs] time_since_dose = to_vector(time) - time_of_first_dose;\n  \n}\nmodel{ \n\n}\ngenerated quantities{\n\n  vector[n_obs] cp;\n  vector[n_obs] dv;\n\n  for(i in 1:n_obs){\n    \n    if(time_since_dose[i] <= 0){\n      cp[i] = 0;\n      dv[i] = 0;\n    }else{\n      cp[i] = depot_1cmt(dose, CL, V, KA, time_since_dose[i]);\n      dv[i] = lognormal_rng(log(cp[i]), sigma);\n    }\n    \n  }\n}\n\n\n\n\nCode\ntimes_to_observe <- c(0.5, 1, 2, 4, 12, 24)\n\ntimes_to_simulate <- times_to_observe %>% \n  c(seq(0, 24, by = 0.25)) %>% \n  sort() %>% \n  unique()\n\nstan_data_simulate <- list(n_obs = length(times_to_simulate),\n                           dose = 200,\n                           time = times_to_simulate,\n                           time_of_first_dose = 0,\n                           CL = 5,\n                           V = 50,\n                           KA = 0.5, \n                           sigma = 0.2)\n\nsimulated_data_stan <- model_simulate_stan$sample(data = stan_data_simulate,\n                                                  fixed_param = TRUE,\n                                                  seed = 1,\n                                                  iter_warmup = 0,\n                                                  iter_sampling = 1,\n                                                  chains = 1,\n                                                  parallel_chains = 1,\n                                                  show_messages = TRUE) \n\ndata_stan <- simulated_data_stan$draws(format = \"draws_df\") %>%\n  spread_draws(cp[i], dv[i]) %>% \n  mutate(time = times_to_simulate[i]) %>%\n  ungroup() %>% \n  select(time, cp, dv)\n\nobserved_data_stan <- data_stan %>%  \n  filter(time %in% times_to_observe) %>% \n  select(time, dv)\n\n\n\n\nCode\nobserved_data_stan %>% \n  mutate(dv = round(dv, 3)) %>% \n  knitr::kable(col.names = c(\"Time\", \"Concentration\"),\n               caption = \"Observed Data for a Single Individual\") %>% \n  kableExtra::kable_styling(full_width = FALSE)\n\n\n\n\nObserved Data for a Single Individual\n \n  \n    Time \n    Concentration \n  \n \n\n  \n    0.5 \n    0.537 \n  \n  \n    1.0 \n    1.692 \n  \n  \n    2.0 \n    2.299 \n  \n  \n    4.0 \n    2.853 \n  \n  \n    12.0 \n    1.833 \n  \n  \n    24.0 \n    0.507 \n  \n\n\n\n\n\nAnd here we can see the observed data overlayed on top of the “truth”.\n\n\nCode\nggplot(mapping = aes(x = time)) +\n  geom_line(data = data_stan,\n            mapping = aes(y = cp)) +\n  geom_point(data = observed_data_stan,\n             mapping = aes(y = dv), \n             color = \"red\", size = 3) +\n  theme_bw(18) +\n  scale_x_continuous(name = \"Time (h)\") +\n  scale_y_continuous(name = \"Drug Concentration (ug/mL)\")\n\n\n\n\n\n\n\n\n\nCode\nset_cmdstan_path(\"~/Torsten/cmdstan/\")\n\nmodel_simulate_torsten <- cmdstan_model(\n  \"Torsten/Simulate/depot_1cmt_lognormal_single.stan\") \n  \nmodel_simulate_torsten$print()\n\n\n// First Order Absorption (oral/subcutaneous)\n// One-compartment PK Model\n// Single subject\n// lognormal error - DV = CP*exp(eps)\n// Closed form solution using a Torsten function\n\ndata{\n  \n  int n_obs;\n  array[n_obs] real amt;\n  array[n_obs] int cmt;\n  array[n_obs] int evid;\n  array[n_obs] real rate;\n  array[n_obs] real ii;\n  array[n_obs] int addl;\n  array[n_obs] int ss;\n  array[n_obs] real time;\n  \n  real<lower = 0> CL;     \n  real<lower = 0> V;      \n  real<lower = CL/V> KA;     \n  \n  real<lower = 0> sigma;  \n \n}\nmodel{ \n\n}\ngenerated quantities{\n\n  vector[n_obs] cp;\n  vector[n_obs] dv;\n\n  {\n    \n    matrix[n_obs, 2] x_cp;\n    array[3] real theta_params = {CL, V, KA}; \n    \n    x_cp = pmx_solve_onecpt(time,\n                            amt,\n                            rate,\n                            ii,\n                            evid,\n                            cmt,\n                            addl,\n                            ss,\n                            theta_params)';\n                               \n    cp = x_cp[, 2] ./ V;\n    \n  }\n\n  for(i in 1:n_obs){\n    if(cp[i] == 0){\n      dv[i] = 0;\n    }else{\n      dv[i] = lognormal_rng(log(cp[i]), sigma);  \n    }\n    \n  }\n}\n\n     \n\n\n\n\nCode\ntimes_to_observe <- c(0.5, 1, 2, 4, 12, 24)\n\ntimes_to_simulate <- times_to_observe %>% \n  c(seq(0.25, 24, by = 0.25)) %>% \n  sort() %>% \n  unique()\n\nnonmem_data_single <- mrgsolve::ev(ID = 1, amt = 200, cmt = 1, evid = 1,\n                                   rate = 0, ii = 0, addl = 0, ss = 0) %>%\n  as_tibble() %>%\n  bind_rows(tibble(ID = 1, time = times_to_simulate, amt = 0, cmt = 2, evid = 0,\n                   rate = 0, ii = 0, addl = 0, ss = 0))\n\ntorsten_data_simulate <- with(nonmem_data_single,\n                              list(n_obs = nrow(nonmem_data_single),\n                                   amt = amt,\n                                   cmt = cmt,\n                                   evid = evid,\n                                   rate = rate,\n                                   ii = ii,\n                                   addl = addl,\n                                   ss = ss,\n                                   time = time,\n                                   CL = 5,\n                                   V = 50,\n                                   KA = 0.5,\n                                   sigma = 0.2))\n\nsimulated_data_torsten <- model_simulate_torsten$sample(data = torsten_data_simulate,\n                                                        fixed_param = TRUE,\n                                                        seed = 1,\n                                                        iter_warmup = 0,\n                                                        iter_sampling = 1,\n                                                        chains = 1,\n                                                        parallel_chains = 1,\n                                                        show_messages = TRUE)\n\ndata_torsten <- simulated_data_torsten$draws(format = \"draws_df\") %>%\n  spread_draws(cp[i], dv[i]) %>%\n  mutate(time = times_to_simulate[i]) %>%\n  ungroup() %>%\n  select(time, cp, dv)\n\nobserved_data_torsten <- data_torsten %>%\n  filter(time %in% times_to_observe) %>%\n  select(time, dv)\n\n\n\n\nCode\nobserved_data_torsten %>%\n  mutate(dv = round(dv, 3)) %>%\n  knitr::kable(col.names = c(\"Time\", \"Concentration\"),\n               caption = \"Observed Data for a Single Individual\") %>%\n  kableExtra::kable_styling(full_width = FALSE)\n\n\n\n\nObserved Data for a Single Individual\n \n  \n    Time \n    Concentration \n  \n \n\n  \n    0.5 \n    0.474 \n  \n  \n    1.0 \n    1.668 \n  \n  \n    2.0 \n    2.059 \n  \n  \n    4.0 \n    2.931 \n  \n  \n    12.0 \n    1.502 \n  \n  \n    24.0 \n    0.615 \n  \n\n\n\n\n\nAnd here we can see the observed data overlayed on top of the “truth”.\n\n\n\n\n\n\n\n\n\n\n3.2.3 Fitting the Data\nNow we want to fit the data1 to our model. We write the model in a .stan file2 (analogous to a .ctl or .mod file in NONMEM):\n\nStanStan + Torsten\n\n\nI’ve first written a model using pure Stan code. Let’s look at the model.\n\n\nCode\nmodel_fit_stan <- cmdstan_model(\n  \"Stan/Fit/depot_1cmt_lognormal_single.stan\")  \n\nmodel_fit_stan$print()\n\n\n// First Order Absorption (oral/subcutaneous)\n// One-compartment PK Model\n// Single subject\n// lognormal error - DV = CP*exp(eps)\n// Closed form solution using a self-written function\n\nfunctions{\n  \n  real depot_1cmt(real dose, real cl, real v, real ka, \n                  real time_since_dose){\n    \n    real ke = cl/v;\n    \n    real cp = dose/v * ka/(ka - ke) * \n              (exp(-ke*time_since_dose) - exp(-ka*time_since_dose));\n    \n    return cp;\n    \n  }\n  \n}\n\ndata{\n  \n  int n_obs;\n  real<lower = 0> dose;\n  array[n_obs] real time;\n  real time_of_first_dose;\n  vector[n_obs] dv;\n  \n  real<lower = 0> scale_cl;     // Prior Scale parameter for CL\n  real<lower = 0> scale_v;      // Prior Scale parameter for V\n  real<lower = 0> scale_ka;     // Prior Scale parameter for KA\n  \n  real<lower = 0> scale_sigma;  // Prior Scale parameter for lognormal error\n  \n  int n_pred;                   // Number of new times at which to make a prediction\n  array[n_pred] real time_pred; // New times at which to make a prediction\n \n}\ntransformed data{ \n  \n  vector[n_obs] time_since_dose = to_vector(time) - time_of_first_dose;\n  vector[n_pred] time_since_dose_pred = to_vector(time_pred) - \n                                        time_of_first_dose;\n  \n}\nparameters{  \n  \n  real<lower = 0> CL;\n  real<lower = 0> V;\n  real<lower = CL/V> KA;\n  \n  real<lower = 0> sigma;\n  \n}\ntransformed parameters{\n\n  vector[n_obs] ipred;\n  \n  for(i in 1:n_obs){\n    ipred[i] = depot_1cmt(dose, CL, V, KA, time_since_dose[i]);\n  }\n  \n}\n\nmodel{ \n  \n  // Priors\n  CL ~ cauchy(0, scale_cl);\n  V ~ cauchy(0, scale_v);\n  KA ~ normal(0, scale_ka) T[CL/V, ];\n  \n  sigma ~ normal(0, scale_sigma);\n  \n  // Likelihood\n  dv ~ lognormal(log(ipred), sigma);\n\n}\ngenerated quantities{\n  \n  real<lower = 0> KE = CL/V;\n  real<lower = 0> sigma_sq = square(sigma);\n\n  vector[n_obs] dv_ppc;\n  vector[n_obs] log_lik;\n  vector[n_pred] cp;\n  vector[n_pred] dv_pred;\n\n  vector[n_obs] ires = log(dv) - log(ipred);\n  vector[n_obs] iwres = ires/sigma;\n  \n  for(i in 1:n_obs){\n    dv_ppc[i] = lognormal_rng(log(ipred[i]), sigma);\n    log_lik[i] = lognormal_lpdf(dv[i] | log(ipred[i]), sigma);\n  }\n\n  for(j in 1:n_pred){\n    if(time_since_dose_pred[j] <= 0){\n      cp[j] = 0;\n      dv_pred[j] = 0;\n    }else{\n      cp[j] = depot_1cmt(dose, CL, V, KA, time_since_dose_pred[j]);\n      dv_pred[j] = lognormal_rng(log(cp[j]), sigma);\n    }\n  }\n}\n\n\nNow we prepare the data for Stan and fit it:\n\n\nCode\nstan_data_fit <- list(n_obs = nrow(observed_data_torsten),\n                      dose = 200,\n                      time = observed_data_torsten$time,\n                      time_of_first_dose = 0,\n                      dv = observed_data_torsten$dv,\n                      scale_cl = 10,\n                      scale_v = 10,\n                      scale_ka = 1,\n                      scale_sigma = 0.5,\n                      n_pred = length(times_to_simulate),\n                      time_pred = times_to_simulate)\n\n\nfit_single_stan <- model_fit_stan$sample(data = stan_data_fit,\n                                         chains = 4,\n                                         # parallel_chains = 4,\n                                         iter_warmup = 1000,\n                                         iter_sampling = 1000,\n                                         adapt_delta = 0.95,\n                                         refresh = 500,\n                                         max_treedepth = 15,\n                                         seed = 8675309,\n                                         init = function() \n                                           list(CL = rlnorm(1, log(8), 0.3),\n                                                V = rlnorm(1, log(40), 0.3),\n                                                KA = rlnorm(1, log(0.8), 0.3),\n                                                sigma = rlnorm(1, log(0.3), 0.3)))\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.5 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.5 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 0.5 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 0.5 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.5 seconds.\nTotal execution time: 2.9 seconds.\n\n\n\n\nI’ve now written a model that uses Torsten’s built-in function for a one-compartment PK model. Let’s look at the model.\n\n\nCode\nmodel_fit_torsten <- cmdstan_model(\n  \"Torsten/Fit/depot_1cmt_lognormal_single.stan\")   \n\nmodel_fit_torsten$print()\n\n\n// First Order Absorption (oral/subcutaneous)\n// One-compartment PK Model\n// Single subject\n// lognormal error - DV = CP*exp(eps)\n// Closed form solution using a Torsten function\n\ndata{\n  \n  int n_total;\n  int n_obs;\n  array[n_obs] int i_obs;\n  array[n_total] real amt;\n  array[n_total] int cmt;\n  array[n_total] int evid;\n  array[n_total] real rate;\n  array[n_total] real ii;\n  array[n_total] int addl;\n  array[n_total] int ss;\n  array[n_total] real time;\n  vector<lower = 0>[n_total] dv;\n  \n  real<lower = 0> scale_cl;     // Prior Scale parameter for CL\n  real<lower = 0> scale_v;      // Prior Scale parameter for V\n  real<lower = 0> scale_ka;     // Prior Scale parameter for KA\n  \n  real<lower = 0> scale_sigma;  // Prior Scale parameter for lognormal error\n  \n  // These are data variables needed to make predictions at unobserved \n  // timepoints\n  int n_pred;               // Number of new times at which to make a prediction\n  array[n_pred] real amt_pred;\n  array[n_pred] int cmt_pred;\n  array[n_pred] int evid_pred;\n  array[n_pred] real rate_pred;\n  array[n_pred] real ii_pred;\n  array[n_pred] int addl_pred;\n  array[n_pred] int ss_pred;\n  array[n_pred] real time_pred;\n\n}\ntransformed data{\n  \n  vector<lower = 0>[n_obs] dv_obs = dv[i_obs];\n  \n}\nparameters{  \n  \n  real<lower = 0> CL;\n  real<lower = 0> V;\n  real<lower = CL/V> KA;\n  \n  real<lower = 0> sigma;\n  \n}\ntransformed parameters{\n  \n  vector[n_obs] ipred;\n   \n  { \n    vector[n_total] dv_ipred;\n    matrix[n_total, 2] x_ipred = pmx_solve_onecpt(time,\n                                                  amt,\n                                                  rate,\n                                                  ii,\n                                                  evid,\n                                                  cmt,\n                                                  addl,\n                                                  ss,\n                                                  {CL, V, KA})';\n                                                  \n    dv_ipred = x_ipred[, 2] ./ V;\n    ipred = dv_ipred[i_obs];\n  }\n  \n}\nmodel{ \n\n  // Priors\n  CL ~ cauchy(0, scale_cl);\n  V ~ cauchy(0, scale_v);\n  KA ~ normal(0, scale_ka) T[CL/V, ];\n  \n  sigma ~ normal(0, scale_sigma);\n  \n  // Likelihood\n  dv_obs ~ lognormal(log(ipred), sigma);\n\n}\ngenerated quantities{\n\n  real<lower = 0> KE = CL/V;\n  real<lower = 0> sigma_sq = square(sigma);\n\n  vector[n_obs] dv_ppc;\n  vector[n_obs] log_lik;\n  vector[n_pred] cp;\n  vector[n_pred] dv_pred;\n\n  vector[n_obs] ires = log(dv_obs) - log(ipred);\n  vector[n_obs] iwres = ires/sigma;\n  \n  for(i in 1:n_obs){\n    dv_ppc[i] = lognormal_rng(log(ipred[i]), sigma);\n    log_lik[i] = lognormal_lpdf(dv[i] | log(ipred[i]), sigma);\n  }\n\n  {\n    \n    matrix[n_pred, 2] x_cp;\n    array[3] real theta_params = {CL, V, KA}; \n    \n    x_cp = pmx_solve_onecpt(time_pred,\n                            amt_pred,\n                            rate_pred,\n                            ii_pred,\n                            evid_pred,\n                            cmt_pred,\n                            addl_pred,\n                            ss_pred,\n                            theta_params)';\n                               \n    cp = x_cp[, 2] ./ V;\n    \n  }\n\n  for(i in 1:n_pred){\n    if(cp[i] == 0){\n      dv_pred[i] = 0;\n    }else{\n      dv_pred[i] = lognormal_rng(log(cp[i]), sigma);  \n    }\n    \n  }\n}\n\n     \n\n\nNow we prepare the data for the Stan model with Torsten functions and fit it:\n\n\nCode\nnonmem_data_single_fit <- nonmem_data_single %>% \n  inner_join(observed_data_torsten, by = \"time\") %>% \n  bind_rows(nonmem_data_single %>% \n              filter(evid == 1)) %>% \n  arrange(time) %>% \n  mutate(dv = if_else(is.na(dv), 5555555, dv))\n\ni_obs <- nonmem_data_single_fit %>%\n  mutate(row_num = 1:n()) %>%\n  filter(evid == 0) %>%\n  select(row_num) %>%\n  deframe()\n\nn_obs <- length(i_obs)\n\ntorsten_data_fit <- list(n_total = nrow(nonmem_data_single_fit),\n                         n_obs = n_obs,\n                         i_obs = i_obs,\n                         amt = nonmem_data_single_fit$amt,\n                         cmt = nonmem_data_single_fit$cmt,\n                         evid = nonmem_data_single_fit$evid,\n                         rate = nonmem_data_single_fit$rate,\n                         ii = nonmem_data_single_fit$ii,\n                         addl = nonmem_data_single_fit$addl,\n                         ss = nonmem_data_single_fit$ss,\n                         time = nonmem_data_single_fit$time,\n                         dv = nonmem_data_single_fit$dv,\n                         scale_cl = 10,\n                         scale_v = 10,\n                         scale_ka = 1,\n                         scale_sigma = 0.5,\n                         n_pred = nrow(nonmem_data_single),\n                         amt_pred = nonmem_data_single$amt,\n                         cmt_pred = nonmem_data_single$cmt,\n                         evid_pred = nonmem_data_single$evid,\n                         rate_pred = nonmem_data_single$rate,\n                         ii_pred = nonmem_data_single$ii,\n                         addl_pred = nonmem_data_single$addl,\n                         ss_pred = nonmem_data_single$ss,\n                         time_pred = nonmem_data_single$time)\n\nfit_single_torsten <- model_fit_torsten$sample(data = torsten_data_fit,\n                                         chains = 4,\n                                         # parallel_chains = 4,\n                                         iter_warmup = 1000,\n                                         iter_sampling = 1000,\n                                         adapt_delta = 0.95,\n                                         refresh = 500,\n                                         max_treedepth = 15,\n                                         seed = 8675309,\n                                         init = function() \n                                           list(CL = rlnorm(1, log(8), 0.3),\n                                                V = rlnorm(1, log(40), 0.3),\n                                                KA = rlnorm(1, log(0.8), 0.3),\n                                                sigma = rlnorm(1, log(0.3), 0.3)))\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 2.8 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 3.8 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 3.3 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 4.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 3.5 seconds.\nTotal execution time: 14.6 seconds.\n\n\n\n\n\n\n\n3.2.4 Post-Processing and What is Happening\nIn the post-processing section, we will go through some of the MCMC sampler checking that we should do here, but we will skip it for brevity and go through it more thoroughly later.\nWe want to look at summaries of the posterior (posterior mean, median, quantiles, and standard deviation), posterior densities for our parameters, and 2D joint posterior densities:\n\n\nCode\nsummarize_draws(fit_single_torsten$draws(),\n                mean, median, sd, mcse_mean,\n                ~quantile2(.x, probs = c(0.025, 0.975)), rhat,\n                ess_bulk, ess_tail) %>%\n  mutate(rse = sd/mean*100,\n         across(where(is.numeric), round, 3)) %>%\n  select(variable, mean, sd, rse, q2.5, median, q97.5, rhat,\n         starts_with(\"ess\")) %>%\n  knitr::kable(col.names = c(\"Variable\", \"Mean\", \"Std. Dev.\", \"RSE\", \"2.5%\",\n                             \"Median\", \"97.5%\", \"$\\\\hat{R}$\", \"ESS Bulk\",\n                             \"ESS Tail\")) %>%\n  kableExtra::column_spec(column = 1:10, width = \"30em\") %>%\n  kableExtra::scroll_box(width = \"800px\", height = \"200px\")\n\n\n\n\n \n  \n    Variable \n    Mean \n    Std. Dev. \n    RSE \n    2.5% \n    Median \n    97.5% \n    $\\hat{R}$ \n    ESS Bulk \n    ESS Tail \n  \n \n\n  \n    lp__ \n    -2.137 \n    1.798 \n    -84.121 \n    -6.661 \n    -1.756 \n    0.246 \n    1.008 \n    794.635 \n    626.653 \n  \n  \n    CL \n    4.355 \n    1.068 \n    24.532 \n    1.930 \n    4.359 \n    6.456 \n    1.007 \n    980.970 \n    440.209 \n  \n  \n    V \n    55.489 \n    22.499 \n    40.546 \n    25.248 \n    51.614 \n    110.525 \n    1.006 \n    896.816 \n    774.931 \n  \n  \n    KA \n    0.501 \n    0.296 \n    59.088 \n    0.179 \n    0.422 \n    1.307 \n    1.004 \n    1020.112 \n    1324.258 \n  \n  \n    sigma \n    0.371 \n    0.148 \n    39.986 \n    0.176 \n    0.339 \n    0.742 \n    1.003 \n    1084.815 \n    1698.531 \n  \n  \n    ipred[1] \n    0.754 \n    0.207 \n    27.408 \n    0.469 \n    0.719 \n    1.248 \n    1.001 \n    3461.336 \n    2169.805 \n  \n  \n    ipred[2] \n    1.303 \n    0.305 \n    23.432 \n    0.846 \n    1.261 \n    2.018 \n    1.001 \n    3386.615 \n    1826.139 \n  \n  \n    ipred[3] \n    1.992 \n    0.399 \n    20.052 \n    1.340 \n    1.955 \n    2.903 \n    1.001 \n    2202.822 \n    1550.524 \n  \n  \n    ipred[4] \n    2.490 \n    0.505 \n    20.283 \n    1.542 \n    2.475 \n    3.576 \n    1.001 \n    1359.165 \n    1227.544 \n  \n  \n    ipred[5] \n    1.725 \n    0.386 \n    22.363 \n    1.046 \n    1.693 \n    2.548 \n    1.002 \n    1528.417 \n    1836.612 \n  \n  \n    ipred[6] \n    0.666 \n    0.273 \n    41.025 \n    0.303 \n    0.609 \n    1.385 \n    1.006 \n    1125.416 \n    521.467 \n  \n  \n    KE \n    0.090 \n    0.036 \n    40.192 \n    0.023 \n    0.087 \n    0.167 \n    1.008 \n    662.649 \n    369.039 \n  \n  \n    sigma_sq \n    0.159 \n    0.146 \n    91.314 \n    0.031 \n    0.115 \n    0.551 \n    1.003 \n    1084.813 \n    1698.531 \n  \n  \n    dv_ppc[1] \n    0.820 \n    0.524 \n    63.912 \n    0.303 \n    0.714 \n    1.948 \n    1.001 \n    4063.930 \n    3293.956 \n  \n  \n    dv_ppc[2] \n    1.422 \n    0.832 \n    58.515 \n    0.518 \n    1.260 \n    3.273 \n    1.002 \n    3765.599 \n    3200.819 \n  \n  \n    dv_ppc[3] \n    2.169 \n    1.212 \n    55.865 \n    0.799 \n    1.952 \n    4.747 \n    1.000 \n    3722.851 \n    2800.842 \n  \n  \n    dv_ppc[4] \n    2.729 \n    1.402 \n    51.371 \n    0.970 \n    2.489 \n    6.261 \n    1.001 \n    2989.344 \n    3079.407 \n  \n  \n    dv_ppc[5] \n    1.869 \n    1.068 \n    57.139 \n    0.655 \n    1.689 \n    4.260 \n    1.001 \n    3148.926 \n    3017.194 \n  \n  \n    dv_ppc[6] \n    0.718 \n    0.474 \n    65.960 \n    0.227 \n    0.615 \n    1.887 \n    1.002 \n    2113.095 \n    1669.165 \n  \n  \n    log_lik[1] \n    -1376.388 \n    1010.713 \n    -73.432 \n    -4067.306 \n    -1112.035 \n    -235.805 \n    1.003 \n    1084.735 \n    1707.823 \n  \n  \n    log_lik[2] \n    -4.392 \n    3.738 \n    -85.112 \n    -14.704 \n    -3.391 \n    -0.242 \n    1.003 \n    1289.555 \n    2157.140 \n  \n  \n    log_lik[3] \n    -0.621 \n    0.406 \n    -65.335 \n    -1.534 \n    -0.590 \n    0.073 \n    1.002 \n    1528.992 \n    1968.758 \n  \n  \n    log_lik[4] \n    -0.910 \n    0.419 \n    -46.061 \n    -1.839 \n    -0.872 \n    -0.197 \n    1.001 \n    1677.842 \n    2479.918 \n  \n  \n    log_lik[5] \n    -2.688 \n    1.400 \n    -52.078 \n    -6.329 \n    -2.310 \n    -1.112 \n    1.003 \n    1682.743 \n    2385.087 \n  \n  \n    log_lik[6] \n    -5.129 \n    4.289 \n    -83.623 \n    -16.118 \n    -3.965 \n    -0.677 \n    1.005 \n    1085.823 \n    1085.114 \n  \n  \n    cp[1] \n    0.000 \n    0.000 \n    NaN \n    0.000 \n    0.000 \n    0.000 \n    NA \n    NA \n    NA \n  \n  \n    cp[2] \n    0.407 \n    0.123 \n    30.280 \n    0.246 \n    0.384 \n    0.709 \n    1.001 \n    3310.127 \n    2035.722 \n  \n  \n    cp[3] \n    0.754 \n    0.207 \n    27.408 \n    0.469 \n    0.719 \n    1.248 \n    1.001 \n    3461.336 \n    2169.805 \n  \n  \n    cp[4] \n    1.050 \n    0.264 \n    25.162 \n    0.674 \n    1.008 \n    1.676 \n    1.001 \n    3507.971 \n    2083.616 \n  \n  \n    cp[5] \n    1.303 \n    0.305 \n    23.432 \n    0.846 \n    1.261 \n    2.018 \n    1.001 \n    3386.615 \n    1826.139 \n  \n  \n    cp[6] \n    1.518 \n    0.336 \n    22.127 \n    1.008 \n    1.477 \n    2.291 \n    1.001 \n    3120.881 \n    1913.441 \n  \n  \n    cp[7] \n    1.702 \n    0.360 \n    21.171 \n    1.142 \n    1.662 \n    2.530 \n    1.001 \n    2804.595 \n    1958.569 \n  \n  \n    cp[8] \n    1.859 \n    0.381 \n    20.498 \n    1.257 \n    1.822 \n    2.723 \n    1.001 \n    2493.777 \n    1976.229 \n  \n  \n    cp[9] \n    1.992 \n    0.399 \n    20.052 \n    1.340 \n    1.955 \n    2.903 \n    1.001 \n    2202.822 \n    1550.524 \n  \n  \n    cp[10] \n    2.105 \n    0.416 \n    19.783 \n    1.403 \n    2.071 \n    3.056 \n    1.001 \n    1979.723 \n    1408.983 \n  \n  \n    cp[11] \n    2.199 \n    0.432 \n    19.652 \n    1.455 \n    2.168 \n    3.166 \n    1.001 \n    1808.311 \n    1262.649 \n  \n  \n    cp[12] \n    2.277 \n    0.447 \n    19.626 \n    1.487 \n    2.250 \n    3.269 \n    1.001 \n    1678.486 \n    1192.524 \n  \n  \n    cp[13] \n    2.342 \n    0.461 \n    19.677 \n    1.506 \n    2.319 \n    3.356 \n    1.001 \n    1580.388 \n    1225.115 \n  \n  \n    cp[14] \n    2.394 \n    0.474 \n    19.783 \n    1.524 \n    2.372 \n    3.438 \n    1.001 \n    1505.590 \n    1266.716 \n  \n  \n    cp[15] \n    2.435 \n    0.485 \n    19.928 \n    1.533 \n    2.418 \n    3.501 \n    1.001 \n    1444.723 \n    1240.637 \n  \n  \n    cp[16] \n    2.467 \n    0.496 \n    20.098 \n    1.536 \n    2.454 \n    3.531 \n    1.001 \n    1397.028 \n    1178.269 \n  \n  \n    cp[17] \n    2.490 \n    0.505 \n    20.283 \n    1.542 \n    2.475 \n    3.576 \n    1.001 \n    1359.165 \n    1227.544 \n  \n  \n    cp[18] \n    2.506 \n    0.513 \n    20.474 \n    1.542 \n    2.493 \n    3.611 \n    1.001 \n    1330.739 \n    1196.487 \n  \n  \n    cp[19] \n    2.515 \n    0.520 \n    20.667 \n    1.536 \n    2.502 \n    3.637 \n    1.001 \n    1307.819 \n    1232.699 \n  \n  \n    cp[20] \n    2.519 \n    0.525 \n    20.856 \n    1.527 \n    2.506 \n    3.628 \n    1.001 \n    1292.332 \n    1274.479 \n  \n  \n    cp[21] \n    2.517 \n    0.530 \n    21.038 \n    1.520 \n    2.503 \n    3.633 \n    1.001 \n    1283.851 \n    1266.188 \n  \n  \n    cp[22] \n    2.511 \n    0.533 \n    21.211 \n    1.506 \n    2.501 \n    3.624 \n    1.001 \n    1277.842 \n    1258.189 \n  \n  \n    cp[23] \n    2.500 \n    0.534 \n    21.373 \n    1.491 \n    2.492 \n    3.613 \n    1.001 \n    1276.192 \n    1362.440 \n  \n  \n    cp[24] \n    2.486 \n    0.535 \n    21.522 \n    1.477 \n    2.481 \n    3.607 \n    1.001 \n    1276.340 \n    1402.978 \n  \n  \n    cp[25] \n    2.469 \n    0.535 \n    21.660 \n    1.466 \n    2.463 \n    3.586 \n    1.001 \n    1278.436 \n    1451.027 \n  \n  \n    cp[26] \n    2.450 \n    0.534 \n    21.784 \n    1.454 \n    2.444 \n    3.552 \n    1.001 \n    1283.296 \n    1442.900 \n  \n  \n    cp[27] \n    2.428 \n    0.532 \n    21.895 \n    1.443 \n    2.423 \n    3.513 \n    1.001 \n    1288.360 \n    1453.902 \n  \n  \n    cp[28] \n    2.403 \n    0.529 \n    21.993 \n    1.426 \n    2.397 \n    3.493 \n    1.001 \n    1290.774 \n    1537.526 \n  \n  \n    cp[29] \n    2.377 \n    0.525 \n    22.078 \n    1.412 \n    2.369 \n    3.453 \n    1.001 \n    1295.857 \n    1615.410 \n  \n  \n    cp[30] \n    2.350 \n    0.521 \n    22.152 \n    1.397 \n    2.340 \n    3.414 \n    1.001 \n    1301.703 \n    1594.963 \n  \n  \n    cp[31] \n    2.321 \n    0.516 \n    22.214 \n    1.380 \n    2.308 \n    3.371 \n    1.001 \n    1308.465 \n    1581.257 \n  \n  \n    cp[32] \n    2.291 \n    0.510 \n    22.265 \n    1.366 \n    2.274 \n    3.328 \n    1.001 \n    1315.960 \n    1571.063 \n  \n  \n    cp[33] \n    2.260 \n    0.504 \n    22.306 \n    1.349 \n    2.243 \n    3.286 \n    1.001 \n    1323.902 \n    1587.716 \n  \n  \n    cp[34] \n    2.228 \n    0.498 \n    22.338 \n    1.331 \n    2.209 \n    3.253 \n    1.001 \n    1333.695 \n    1608.290 \n  \n  \n    cp[35] \n    2.196 \n    0.491 \n    22.362 \n    1.310 \n    2.175 \n    3.198 \n    1.001 \n    1344.288 \n    1641.653 \n  \n  \n    cp[36] \n    2.163 \n    0.484 \n    22.378 \n    1.297 \n    2.140 \n    3.136 \n    1.001 \n    1354.873 \n    1658.188 \n  \n  \n    cp[37] \n    2.129 \n    0.477 \n    22.387 \n    1.278 \n    2.104 \n    3.082 \n    1.001 \n    1367.972 \n    1694.652 \n  \n  \n    cp[38] \n    2.095 \n    0.469 \n    22.392 \n    1.260 \n    2.069 \n    3.027 \n    1.001 \n    1380.645 \n    1710.939 \n  \n  \n    cp[39] \n    2.061 \n    0.462 \n    22.391 \n    1.244 \n    2.034 \n    2.981 \n    1.001 \n    1396.104 \n    1698.673 \n  \n  \n    cp[40] \n    2.027 \n    0.454 \n    22.387 \n    1.227 \n    2.001 \n    2.932 \n    1.001 \n    1409.412 \n    1713.934 \n  \n  \n    cp[41] \n    1.993 \n    0.446 \n    22.380 \n    1.206 \n    1.965 \n    2.885 \n    1.001 \n    1424.609 \n    1736.742 \n  \n  \n    cp[42] \n    1.959 \n    0.438 \n    22.372 \n    1.188 \n    1.933 \n    2.839 \n    1.001 \n    1440.164 \n    1751.570 \n  \n  \n    cp[43] \n    1.925 \n    0.430 \n    22.364 \n    1.167 \n    1.897 \n    2.799 \n    1.002 \n    1456.632 \n    1737.198 \n  \n  \n    cp[44] \n    1.891 \n    0.423 \n    22.356 \n    1.143 \n    1.861 \n    2.756 \n    1.002 \n    1470.886 \n    1777.634 \n  \n  \n    cp[45] \n    1.857 \n    0.415 \n    22.350 \n    1.124 \n    1.828 \n    2.714 \n    1.002 \n    1482.206 \n    1802.901 \n  \n  \n    cp[46] \n    1.824 \n    0.408 \n    22.346 \n    1.101 \n    1.795 \n    2.669 \n    1.002 \n    1493.578 \n    1751.658 \n  \n  \n    cp[47] \n    1.791 \n    0.400 \n    22.347 \n    1.081 \n    1.760 \n    2.615 \n    1.002 \n    1505.000 \n    1759.474 \n  \n  \n    cp[48] \n    1.758 \n    0.393 \n    22.352 \n    1.062 \n    1.725 \n    2.577 \n    1.002 \n    1516.146 \n    1784.567 \n  \n  \n    cp[49] \n    1.725 \n    0.386 \n    22.363 \n    1.046 \n    1.693 \n    2.548 \n    1.002 \n    1528.417 \n    1836.612 \n  \n  \n    cp[50] \n    1.693 \n    0.379 \n    22.382 \n    1.026 \n    1.660 \n    2.500 \n    1.002 \n    1540.168 \n    1776.184 \n  \n  \n    cp[51] \n    1.661 \n    0.372 \n    22.408 \n    1.006 \n    1.627 \n    2.477 \n    1.002 \n    1551.693 \n    1767.806 \n  \n  \n    cp[52] \n    1.629 \n    0.366 \n    22.444 \n    0.985 \n    1.598 \n    2.427 \n    1.002 \n    1562.940 \n    1793.044 \n  \n  \n    cp[53] \n    1.598 \n    0.359 \n    22.490 \n    0.966 \n    1.565 \n    2.389 \n    1.002 \n    1573.531 \n    1768.172 \n  \n  \n    cp[54] \n    1.568 \n    0.353 \n    22.547 \n    0.952 \n    1.536 \n    2.343 \n    1.003 \n    1583.789 \n    1768.061 \n  \n  \n    cp[55] \n    1.537 \n    0.348 \n    22.616 \n    0.928 \n    1.506 \n    2.316 \n    1.003 \n    1593.857 \n    1819.014 \n  \n  \n    cp[56] \n    1.508 \n    0.342 \n    22.697 \n    0.910 \n    1.478 \n    2.277 \n    1.003 \n    1604.120 \n    1836.960 \n  \n  \n    cp[57] \n    1.478 \n    0.337 \n    22.793 \n    0.892 \n    1.445 \n    2.240 \n    1.003 \n    1611.359 \n    1828.081 \n  \n  \n    cp[58] \n    1.449 \n    0.332 \n    22.903 \n    0.874 \n    1.418 \n    2.211 \n    1.003 \n    1618.564 \n    1845.886 \n  \n  \n    cp[59] \n    1.421 \n    0.327 \n    23.028 \n    0.858 \n    1.389 \n    2.183 \n    1.003 \n    1624.614 \n    1819.721 \n  \n  \n    cp[60] \n    1.393 \n    0.323 \n    23.170 \n    0.842 \n    1.361 \n    2.151 \n    1.003 \n    1629.936 \n    1777.485 \n  \n  \n    cp[61] \n    1.366 \n    0.319 \n    23.328 \n    0.823 \n    1.334 \n    2.116 \n    1.003 \n    1636.732 \n    1778.602 \n  \n  \n    cp[62] \n    1.339 \n    0.315 \n    23.503 \n    0.806 \n    1.307 \n    2.079 \n    1.004 \n    1640.799 \n    1604.642 \n  \n  \n    cp[63] \n    1.312 \n    0.311 \n    23.696 \n    0.790 \n    1.282 \n    2.053 \n    1.004 \n    1643.837 \n    1643.536 \n  \n  \n    cp[64] \n    1.286 \n    0.308 \n    23.907 \n    0.774 \n    1.254 \n    2.020 \n    1.004 \n    1645.462 \n    1628.331 \n  \n  \n    cp[65] \n    1.261 \n    0.304 \n    24.136 \n    0.756 \n    1.229 \n    1.983 \n    1.004 \n    1648.545 \n    1685.239 \n  \n  \n    cp[66] \n    1.236 \n    0.301 \n    24.385 \n    0.739 \n    1.203 \n    1.952 \n    1.004 \n    1648.489 \n    1576.650 \n  \n  \n    cp[67] \n    1.211 \n    0.299 \n    24.652 \n    0.722 \n    1.177 \n    1.929 \n    1.004 \n    1647.030 \n    1486.980 \n  \n  \n    cp[68] \n    1.187 \n    0.296 \n    24.939 \n    0.705 \n    1.154 \n    1.899 \n    1.004 \n    1645.640 \n    1420.064 \n  \n  \n    cp[69] \n    1.163 \n    0.294 \n    25.246 \n    0.687 \n    1.130 \n    1.865 \n    1.004 \n    1642.638 \n    1399.048 \n  \n  \n    cp[70] \n    1.140 \n    0.292 \n    25.572 \n    0.667 \n    1.107 \n    1.847 \n    1.004 \n    1638.386 \n    1285.087 \n  \n  \n    cp[71] \n    1.117 \n    0.290 \n    25.917 \n    0.654 \n    1.082 \n    1.819 \n    1.004 \n    1623.986 \n    1208.491 \n  \n  \n    cp[72] \n    1.095 \n    0.288 \n    26.282 \n    0.638 \n    1.059 \n    1.785 \n    1.005 \n    1595.945 \n    1213.554 \n  \n  \n    cp[73] \n    1.073 \n    0.286 \n    26.666 \n    0.620 \n    1.038 \n    1.752 \n    1.005 \n    1569.831 \n    1097.293 \n  \n  \n    cp[74] \n    1.052 \n    0.285 \n    27.070 \n    0.602 \n    1.015 \n    1.731 \n    1.005 \n    1544.207 \n    1024.219 \n  \n  \n    cp[75] \n    1.031 \n    0.283 \n    27.492 \n    0.585 \n    0.994 \n    1.698 \n    1.005 \n    1524.407 \n    988.078 \n  \n  \n    cp[76] \n    1.010 \n    0.282 \n    27.934 \n    0.569 \n    0.974 \n    1.676 \n    1.005 \n    1497.156 \n    942.142 \n  \n  \n    cp[77] \n    0.990 \n    0.281 \n    28.394 \n    0.554 \n    0.953 \n    1.666 \n    1.005 \n    1475.116 \n    962.647 \n  \n  \n    cp[78] \n    0.971 \n    0.280 \n    28.873 \n    0.539 \n    0.933 \n    1.650 \n    1.005 \n    1454.016 \n    922.148 \n  \n  \n    cp[79] \n    0.951 \n    0.279 \n    29.370 \n    0.526 \n    0.913 \n    1.631 \n    1.005 \n    1432.645 \n    858.267 \n  \n  \n    cp[80] \n    0.932 \n    0.279 \n    29.884 \n    0.512 \n    0.893 \n    1.618 \n    1.005 \n    1409.768 \n    820.635 \n  \n  \n    cp[81] \n    0.914 \n    0.278 \n    30.416 \n    0.498 \n    0.874 \n    1.597 \n    1.005 \n    1387.063 \n    786.689 \n  \n  \n    cp[82] \n    0.896 \n    0.277 \n    30.965 \n    0.485 \n    0.856 \n    1.584 \n    1.006 \n    1365.755 \n    754.001 \n  \n  \n    cp[83] \n    0.878 \n    0.277 \n    31.531 \n    0.471 \n    0.838 \n    1.568 \n    1.005 \n    1346.028 \n    659.163 \n  \n  \n    cp[84] \n    0.861 \n    0.276 \n    32.114 \n    0.456 \n    0.820 \n    1.551 \n    1.006 \n    1323.575 \n    637.459 \n  \n  \n    cp[85] \n    0.844 \n    0.276 \n    32.712 \n    0.440 \n    0.803 \n    1.523 \n    1.006 \n    1304.414 \n    638.258 \n  \n  \n    cp[86] \n    0.827 \n    0.276 \n    33.326 \n    0.429 \n    0.784 \n    1.509 \n    1.007 \n    1287.176 \n    579.739 \n  \n  \n    cp[87] \n    0.811 \n    0.275 \n    33.956 \n    0.414 \n    0.766 \n    1.497 \n    1.006 \n    1270.700 \n    573.326 \n  \n  \n    cp[88] \n    0.795 \n    0.275 \n    34.601 \n    0.402 \n    0.749 \n    1.480 \n    1.007 \n    1252.582 \n    569.384 \n  \n  \n    cp[89] \n    0.779 \n    0.275 \n    35.260 \n    0.391 \n    0.731 \n    1.462 \n    1.007 \n    1237.987 \n    579.076 \n  \n  \n    cp[90] \n    0.764 \n    0.275 \n    35.934 \n    0.378 \n    0.715 \n    1.445 \n    1.007 \n    1221.935 \n    578.209 \n  \n  \n    cp[91] \n    0.749 \n    0.274 \n    36.622 \n    0.367 \n    0.698 \n    1.442 \n    1.007 \n    1207.200 \n    561.965 \n  \n  \n    cp[92] \n    0.735 \n    0.274 \n    37.323 \n    0.357 \n    0.683 \n    1.437 \n    1.007 \n    1192.217 \n    557.030 \n  \n  \n    cp[93] \n    0.720 \n    0.274 \n    38.038 \n    0.346 \n    0.668 \n    1.423 \n    1.007 \n    1177.631 \n    545.579 \n  \n  \n    cp[94] \n    0.706 \n    0.274 \n    38.766 \n    0.335 \n    0.652 \n    1.410 \n    1.007 \n    1165.204 \n    544.430 \n  \n  \n    cp[95] \n    0.693 \n    0.274 \n    39.506 \n    0.324 \n    0.637 \n    1.406 \n    1.006 \n    1151.520 \n    524.779 \n  \n  \n    cp[96] \n    0.679 \n    0.274 \n    40.259 \n    0.313 \n    0.623 \n    1.399 \n    1.006 \n    1139.034 \n    522.412 \n  \n  \n    cp[97] \n    0.666 \n    0.273 \n    41.025 \n    0.303 \n    0.609 \n    1.385 \n    1.006 \n    1125.416 \n    521.467 \n  \n  \n    dv_pred[1] \n    0.000 \n    0.000 \n    NaN \n    0.000 \n    0.000 \n    0.000 \n    NA \n    NA \n    NA \n  \n  \n    dv_pred[2] \n    0.447 \n    0.266 \n    59.625 \n    0.159 \n    0.389 \n    1.085 \n    1.001 \n    3818.070 \n    2957.421 \n  \n  \n    dv_pred[3] \n    0.804 \n    0.450 \n    55.960 \n    0.292 \n    0.720 \n    1.759 \n    1.002 \n    3843.261 \n    3291.723 \n  \n  \n    dv_pred[4] \n    1.135 \n    0.699 \n    61.579 \n    0.393 \n    1.015 \n    2.640 \n    1.000 \n    3923.071 \n    3046.939 \n  \n  \n    dv_pred[5] \n    1.413 \n    0.801 \n    56.687 \n    0.489 \n    1.258 \n    3.272 \n    1.000 \n    3927.959 \n    2966.367 \n  \n  \n    dv_pred[6] \n    1.665 \n    1.100 \n    66.080 \n    0.624 \n    1.498 \n    3.648 \n    1.000 \n    3941.923 \n    3108.647 \n  \n  \n    dv_pred[7] \n    1.858 \n    1.235 \n    66.450 \n    0.720 \n    1.662 \n    4.104 \n    1.001 \n    4184.315 \n    3265.967 \n  \n  \n    dv_pred[8] \n    2.016 \n    1.129 \n    55.995 \n    0.734 \n    1.802 \n    4.624 \n    1.001 \n    3870.380 \n    2754.671 \n  \n  \n    dv_pred[9] \n    2.152 \n    1.230 \n    57.157 \n    0.754 \n    1.951 \n    4.587 \n    1.001 \n    3639.027 \n    2924.317 \n  \n  \n    dv_pred[10] \n    2.315 \n    1.960 \n    84.658 \n    0.819 \n    2.080 \n    4.983 \n    1.000 \n    3872.531 \n    3057.195 \n  \n  \n    dv_pred[11] \n    2.406 \n    1.619 \n    67.306 \n    0.866 \n    2.161 \n    5.262 \n    1.000 \n    3624.962 \n    2850.676 \n  \n  \n    dv_pred[12] \n    2.443 \n    1.262 \n    51.682 \n    0.874 \n    2.244 \n    5.293 \n    1.000 \n    3141.947 \n    2718.986 \n  \n  \n    dv_pred[13] \n    2.532 \n    1.465 \n    57.850 \n    0.899 \n    2.318 \n    5.533 \n    1.001 \n    3364.679 \n    2789.529 \n  \n  \n    dv_pred[14] \n    2.603 \n    1.440 \n    55.327 \n    0.927 \n    2.381 \n    5.679 \n    1.001 \n    3435.816 \n    3177.955 \n  \n  \n    dv_pred[15] \n    2.678 \n    1.644 \n    61.384 \n    0.940 \n    2.425 \n    5.831 \n    1.001 \n    2767.301 \n    2814.226 \n  \n  \n    dv_pred[16] \n    2.668 \n    1.457 \n    54.582 \n    0.933 \n    2.445 \n    5.660 \n    1.000 \n    2908.415 \n    2344.151 \n  \n  \n    dv_pred[17] \n    2.689 \n    1.372 \n    51.007 \n    0.961 \n    2.459 \n    6.012 \n    1.000 \n    2959.315 \n    2660.712 \n  \n  \n    dv_pred[18] \n    2.748 \n    1.618 \n    58.879 \n    0.960 \n    2.478 \n    5.981 \n    1.002 \n    2959.956 \n    2526.857 \n  \n  \n    dv_pred[19] \n    2.712 \n    1.381 \n    50.912 \n    0.964 \n    2.469 \n    6.082 \n    1.001 \n    2561.149 \n    2693.410 \n  \n  \n    dv_pred[20] \n    2.760 \n    1.486 \n    53.850 \n    0.973 \n    2.508 \n    5.980 \n    1.000 \n    3038.825 \n    3001.176 \n  \n  \n    dv_pred[21] \n    2.716 \n    1.423 \n    52.374 \n    0.990 \n    2.471 \n    5.859 \n    1.000 \n    3005.574 \n    2681.790 \n  \n  \n    dv_pred[22] \n    2.725 \n    1.392 \n    51.082 \n    0.948 \n    2.485 \n    5.887 \n    1.001 \n    2759.230 \n    2728.163 \n  \n  \n    dv_pred[23] \n    2.688 \n    1.329 \n    49.441 \n    0.939 \n    2.481 \n    5.703 \n    1.001 \n    2908.465 \n    3197.180 \n  \n  \n    dv_pred[24] \n    2.710 \n    1.438 \n    53.054 \n    0.930 \n    2.479 \n    5.849 \n    1.000 \n    2653.094 \n    2828.734 \n  \n  \n    dv_pred[25] \n    2.695 \n    1.552 \n    57.601 \n    0.901 \n    2.468 \n    5.762 \n    1.001 \n    3026.305 \n    2909.133 \n  \n  \n    dv_pred[26] \n    2.676 \n    1.447 \n    54.080 \n    0.908 \n    2.434 \n    5.769 \n    1.000 \n    2895.777 \n    2734.100 \n  \n  \n    dv_pred[27] \n    2.626 \n    1.365 \n    51.983 \n    0.925 \n    2.397 \n    5.762 \n    1.001 \n    2890.257 \n    2802.849 \n  \n  \n    dv_pred[28] \n    2.577 \n    1.368 \n    53.088 \n    0.936 \n    2.344 \n    5.662 \n    1.001 \n    2797.384 \n    2698.817 \n  \n  \n    dv_pred[29] \n    2.574 \n    1.431 \n    55.607 \n    0.851 \n    2.331 \n    5.516 \n    1.000 \n    2870.321 \n    2939.255 \n  \n  \n    dv_pred[30] \n    2.519 \n    1.287 \n    51.087 \n    0.809 \n    2.317 \n    5.444 \n    1.001 \n    2657.040 \n    2702.597 \n  \n  \n    dv_pred[31] \n    2.547 \n    1.631 \n    64.032 \n    0.870 \n    2.285 \n    5.755 \n    1.001 \n    2524.845 \n    2924.445 \n  \n  \n    dv_pred[32] \n    2.497 \n    1.305 \n    52.270 \n    0.843 \n    2.268 \n    5.437 \n    1.002 \n    2269.055 \n    2652.609 \n  \n  \n    dv_pred[33] \n    2.430 \n    1.278 \n    52.598 \n    0.783 \n    2.212 \n    5.297 \n    1.002 \n    2806.445 \n    2925.347 \n  \n  \n    dv_pred[34] \n    2.432 \n    1.470 \n    60.451 \n    0.808 \n    2.236 \n    5.162 \n    1.002 \n    2789.945 \n    2688.141 \n  \n  \n    dv_pred[35] \n    2.391 \n    2.861 \n    119.671 \n    0.778 \n    2.155 \n    5.236 \n    1.000 \n    2483.533 \n    2763.568 \n  \n  \n    dv_pred[36] \n    2.325 \n    1.168 \n    50.239 \n    0.795 \n    2.143 \n    5.192 \n    1.001 \n    2775.385 \n    2890.691 \n  \n  \n    dv_pred[37] \n    2.307 \n    1.166 \n    50.533 \n    0.748 \n    2.102 \n    4.904 \n    1.001 \n    2751.990 \n    2824.307 \n  \n  \n    dv_pred[38] \n    2.264 \n    1.166 \n    51.498 \n    0.743 \n    2.059 \n    4.990 \n    1.000 \n    2776.428 \n    2570.533 \n  \n  \n    dv_pred[39] \n    2.239 \n    1.219 \n    54.425 \n    0.725 \n    2.041 \n    5.031 \n    1.001 \n    2669.571 \n    2681.213 \n  \n  \n    dv_pred[40] \n    2.172 \n    1.040 \n    47.879 \n    0.742 \n    1.983 \n    4.706 \n    1.002 \n    2574.518 \n    2963.137 \n  \n  \n    dv_pred[41] \n    2.157 \n    1.368 \n    63.422 \n    0.723 \n    1.966 \n    4.655 \n    1.002 \n    2678.576 \n    2785.044 \n  \n  \n    dv_pred[42] \n    2.139 \n    1.155 \n    54.012 \n    0.716 \n    1.940 \n    4.806 \n    1.001 \n    2875.284 \n    2896.157 \n  \n  \n    dv_pred[43] \n    2.097 \n    1.181 \n    56.328 \n    0.704 \n    1.897 \n    4.638 \n    1.000 \n    2581.940 \n    2493.340 \n  \n  \n    dv_pred[44] \n    2.046 \n    1.064 \n    52.020 \n    0.718 \n    1.861 \n    4.500 \n    1.000 \n    2451.147 \n    3136.163 \n  \n  \n    dv_pred[45] \n    2.019 \n    1.053 \n    52.170 \n    0.708 \n    1.828 \n    4.444 \n    1.000 \n    3238.931 \n    3281.057 \n  \n  \n    dv_pred[46] \n    1.949 \n    0.969 \n    49.713 \n    0.669 \n    1.795 \n    4.373 \n    1.001 \n    2896.901 \n    2754.905 \n  \n  \n    dv_pred[47] \n    1.927 \n    1.032 \n    53.575 \n    0.649 \n    1.745 \n    4.134 \n    1.002 \n    2863.391 \n    2765.566 \n  \n  \n    dv_pred[48] \n    1.913 \n    1.002 \n    52.398 \n    0.690 \n    1.726 \n    4.194 \n    1.001 \n    2635.603 \n    2999.175 \n  \n  \n    dv_pred[49] \n    1.886 \n    1.175 \n    62.274 \n    0.658 \n    1.697 \n    4.143 \n    1.000 \n    3218.694 \n    2735.446 \n  \n  \n    dv_pred[50] \n    1.836 \n    1.115 \n    60.714 \n    0.642 \n    1.651 \n    4.131 \n    1.002 \n    3100.467 \n    3110.115 \n  \n  \n    dv_pred[51] \n    1.801 \n    0.966 \n    53.655 \n    0.637 \n    1.633 \n    3.945 \n    1.001 \n    2979.938 \n    2768.123 \n  \n  \n    dv_pred[52] \n    1.748 \n    0.939 \n    53.733 \n    0.577 \n    1.589 \n    4.008 \n    1.000 \n    2551.083 \n    2835.196 \n  \n  \n    dv_pred[53] \n    1.764 \n    1.103 \n    62.507 \n    0.601 \n    1.577 \n    3.935 \n    1.002 \n    3189.534 \n    3284.027 \n  \n  \n    dv_pred[54] \n    1.699 \n    1.004 \n    59.119 \n    0.599 \n    1.536 \n    3.783 \n    1.002 \n    3036.695 \n    3297.585 \n  \n  \n    dv_pred[55] \n    1.668 \n    0.864 \n    51.833 \n    0.587 \n    1.514 \n    3.693 \n    1.001 \n    2899.314 \n    3047.590 \n  \n  \n    dv_pred[56] \n    1.641 \n    0.844 \n    51.446 \n    0.586 \n    1.475 \n    3.639 \n    1.000 \n    3112.393 \n    2624.276 \n  \n  \n    dv_pred[57] \n    1.611 \n    0.882 \n    54.741 \n    0.585 \n    1.463 \n    3.605 \n    1.000 \n    3139.052 \n    3347.912 \n  \n  \n    dv_pred[58] \n    1.572 \n    0.827 \n    52.637 \n    0.538 \n    1.415 \n    3.515 \n    1.000 \n    2895.760 \n    3023.591 \n  \n  \n    dv_pred[59] \n    1.547 \n    0.899 \n    58.113 \n    0.529 \n    1.402 \n    3.478 \n    1.001 \n    2849.552 \n    2984.601 \n  \n  \n    dv_pred[60] \n    1.518 \n    0.850 \n    55.979 \n    0.556 \n    1.356 \n    3.523 \n    1.002 \n    2945.537 \n    2982.626 \n  \n  \n    dv_pred[61] \n    1.478 \n    0.825 \n    55.859 \n    0.529 \n    1.330 \n    3.346 \n    1.001 \n    3464.706 \n    3008.463 \n  \n  \n    dv_pred[62] \n    1.474 \n    0.851 \n    57.749 \n    0.518 \n    1.315 \n    3.293 \n    1.002 \n    2399.661 \n    3133.712 \n  \n  \n    dv_pred[63] \n    1.418 \n    0.807 \n    56.941 \n    0.514 \n    1.275 \n    3.252 \n    1.001 \n    2867.998 \n    2951.121 \n  \n  \n    dv_pred[64] \n    1.388 \n    0.801 \n    57.697 \n    0.493 \n    1.253 \n    3.212 \n    1.001 \n    2946.870 \n    2750.863 \n  \n  \n    dv_pred[65] \n    1.371 \n    0.713 \n    51.970 \n    0.484 \n    1.247 \n    3.098 \n    1.001 \n    3029.518 \n    2766.086 \n  \n  \n    dv_pred[66] \n    1.369 \n    0.966 \n    70.506 \n    0.473 \n    1.218 \n    3.195 \n    1.002 \n    3153.732 \n    2654.869 \n  \n  \n    dv_pred[67] \n    1.310 \n    0.777 \n    59.357 \n    0.447 \n    1.163 \n    2.969 \n    1.002 \n    3008.766 \n    2682.113 \n  \n  \n    dv_pred[68] \n    1.314 \n    0.763 \n    58.070 \n    0.434 \n    1.175 \n    3.176 \n    1.001 \n    2801.894 \n    2580.734 \n  \n  \n    dv_pred[69] \n    1.248 \n    0.758 \n    60.711 \n    0.438 \n    1.131 \n    2.799 \n    1.001 \n    3022.266 \n    2021.036 \n  \n  \n    dv_pred[70] \n    1.236 \n    0.697 \n    56.347 \n    0.432 \n    1.106 \n    2.882 \n    1.000 \n    3047.656 \n    2688.365 \n  \n  \n    dv_pred[71] \n    1.199 \n    0.703 \n    58.614 \n    0.419 \n    1.075 \n    2.675 \n    1.001 \n    3070.487 \n    2440.910 \n  \n  \n    dv_pred[72] \n    1.168 \n    0.661 \n    56.571 \n    0.397 \n    1.054 \n    2.715 \n    1.001 \n    2964.047 \n    2496.541 \n  \n  \n    dv_pred[73] \n    1.175 \n    0.727 \n    61.888 \n    0.403 \n    1.037 \n    2.756 \n    1.001 \n    2759.395 \n    2378.120 \n  \n  \n    dv_pred[74] \n    1.147 \n    0.636 \n    55.400 \n    0.380 \n    1.024 \n    2.669 \n    1.002 \n    2609.560 \n    1751.163 \n  \n  \n    dv_pred[75] \n    1.147 \n    1.181 \n    102.950 \n    0.352 \n    1.001 \n    2.733 \n    1.002 \n    2838.048 \n    2265.052 \n  \n  \n    dv_pred[76] \n    1.094 \n    0.607 \n    55.514 \n    0.386 \n    0.976 \n    2.431 \n    1.000 \n    2898.406 \n    2367.640 \n  \n  \n    dv_pred[77] \n    1.073 \n    0.605 \n    56.428 \n    0.353 \n    0.952 \n    2.598 \n    1.004 \n    2517.548 \n    1849.124 \n  \n  \n    dv_pred[78] \n    1.045 \n    0.615 \n    58.835 \n    0.348 \n    0.930 \n    2.419 \n    1.002 \n    3078.508 \n    2414.908 \n  \n  \n    dv_pred[79] \n    1.030 \n    0.642 \n    62.325 \n    0.343 \n    0.908 \n    2.457 \n    1.002 \n    2842.938 \n    2372.292 \n  \n  \n    dv_pred[80] \n    1.016 \n    0.663 \n    65.204 \n    0.338 \n    0.892 \n    2.491 \n    1.001 \n    2522.017 \n    2042.289 \n  \n  \n    dv_pred[81] \n    0.988 \n    0.731 \n    73.932 \n    0.325 \n    0.864 \n    2.486 \n    1.002 \n    2343.766 \n    1934.742 \n  \n  \n    dv_pred[82] \n    0.984 \n    0.770 \n    78.300 \n    0.327 \n    0.852 \n    2.402 \n    1.002 \n    2478.733 \n    1501.345 \n  \n  \n    dv_pred[83] \n    0.964 \n    0.669 \n    69.449 \n    0.317 \n    0.838 \n    2.341 \n    1.001 \n    2987.248 \n    2264.714 \n  \n  \n    dv_pred[84] \n    0.921 \n    0.540 \n    58.685 \n    0.301 \n    0.815 \n    2.187 \n    1.003 \n    2557.602 \n    2063.966 \n  \n  \n    dv_pred[85] \n    0.917 \n    0.586 \n    63.869 \n    0.298 \n    0.797 \n    2.309 \n    1.003 \n    2567.302 \n    2507.972 \n  \n  \n    dv_pred[86] \n    0.900 \n    0.581 \n    64.528 \n    0.305 \n    0.784 \n    2.180 \n    1.003 \n    2752.548 \n    1796.800 \n  \n  \n    dv_pred[87] \n    0.884 \n    0.876 \n    99.088 \n    0.285 \n    0.754 \n    2.198 \n    1.003 \n    2414.141 \n    1951.958 \n  \n  \n    dv_pred[88] \n    0.861 \n    0.590 \n    68.583 \n    0.284 \n    0.744 \n    2.103 \n    1.002 \n    2202.549 \n    1522.078 \n  \n  \n    dv_pred[89] \n    0.844 \n    0.515 \n    61.053 \n    0.287 \n    0.728 \n    2.112 \n    1.001 \n    2527.825 \n    1930.587 \n  \n  \n    dv_pred[90] \n    0.822 \n    0.535 \n    65.106 \n    0.268 \n    0.716 \n    2.079 \n    1.003 \n    1804.574 \n    1470.038 \n  \n  \n    dv_pred[91] \n    0.809 \n    0.543 \n    67.127 \n    0.245 \n    0.702 \n    2.011 \n    1.002 \n    2761.356 \n    1656.639 \n  \n  \n    dv_pred[92] \n    0.810 \n    0.713 \n    88.048 \n    0.249 \n    0.684 \n    2.079 \n    1.002 \n    2117.533 \n    1880.745 \n  \n  \n    dv_pred[93] \n    0.784 \n    0.501 \n    63.936 \n    0.240 \n    0.664 \n    2.043 \n    1.002 \n    2115.718 \n    1632.866 \n  \n  \n    dv_pred[94] \n    0.776 \n    0.574 \n    73.969 \n    0.238 \n    0.660 \n    2.057 \n    1.003 \n    1907.815 \n    1368.044 \n  \n  \n    dv_pred[95] \n    0.763 \n    0.719 \n    94.317 \n    0.235 \n    0.646 \n    1.989 \n    1.003 \n    1914.341 \n    1561.765 \n  \n  \n    dv_pred[96] \n    0.746 \n    0.544 \n    73.010 \n    0.218 \n    0.635 \n    2.004 \n    1.004 \n    1606.362 \n    1149.605 \n  \n  \n    dv_pred[97] \n    0.725 \n    0.518 \n    71.512 \n    0.205 \n    0.607 \n    2.011 \n    1.003 \n    1842.228 \n    1037.075 \n  \n  \n    ires[1] \n    -0.434 \n    0.240 \n    -55.298 \n    -0.969 \n    -0.418 \n    0.010 \n    1.001 \n    3461.309 \n    2169.805 \n  \n  \n    ires[2] \n    0.271 \n    0.212 \n    78.353 \n    -0.190 \n    0.280 \n    0.679 \n    1.001 \n    3386.622 \n    1826.139 \n  \n  \n    ires[3] \n    0.051 \n    0.190 \n    369.214 \n    -0.343 \n    0.052 \n    0.430 \n    1.002 \n    2202.833 \n    1550.524 \n  \n  \n    ires[4] \n    0.184 \n    0.205 \n    111.721 \n    -0.199 \n    0.169 \n    0.642 \n    1.001 \n    1359.157 \n    1227.544 \n  \n  \n    ires[5] \n    -0.114 \n    0.225 \n    -198.597 \n    -0.528 \n    -0.120 \n    0.362 \n    1.002 \n    1528.414 \n    1836.612 \n  \n  \n    ires[6] \n    -0.011 \n    0.367 \n    -3341.167 \n    -0.812 \n    0.010 \n    0.708 \n    1.005 \n    1125.416 \n    521.467 \n  \n  \n    iwres[1] \n    -1.303 \n    0.712 \n    -54.614 \n    -2.798 \n    -1.275 \n    0.020 \n    1.001 \n    2163.697 \n    2827.847 \n  \n  \n    iwres[2] \n    0.861 \n    0.646 \n    75.064 \n    -0.358 \n    0.841 \n    2.161 \n    1.001 \n    2238.880 \n    1976.552 \n  \n  \n    iwres[3] \n    0.157 \n    0.477 \n    304.638 \n    -0.777 \n    0.159 \n    1.070 \n    1.000 \n    3015.700 \n    2458.927 \n  \n  \n    iwres[4] \n    0.538 \n    0.536 \n    99.688 \n    -0.454 \n    0.512 \n    1.629 \n    1.000 \n    1996.959 \n    2733.015 \n  \n  \n    iwres[5] \n    -0.378 \n    0.647 \n    -171.053 \n    -1.669 \n    -0.352 \n    0.815 \n    1.002 \n    1617.519 \n    2248.742 \n  \n  \n    iwres[6] \n    0.004 \n    0.946 \n    21979.166 \n    -1.849 \n    0.032 \n    1.849 \n    1.004 \n    1529.114 \n    1270.987 \n  \n\n\n\n\n\nCode\nmcmc_pairs(fit_single_torsten$draws(c(\"CL\", \"V\", \"KA\", \"sigma\")),\n           diag_fun = \"dens\")\n\n\n\n\n\n\n\n\n\nWe have also created a predicted curve for each draw from the posterior ( cp in the code). Here, 5 draws are highlighted, and you can see the curve corresponding to each of these draws, along with a few others:\n\n\nCode\ndraws_single <- fit_single_torsten$draws(format = \"draws_df\")\n\ndraws_to_highlight <- seq(1, 9, by = 2)\ncolors_to_highlight <- c(\"red\", \"blue\", \"green\", \"purple\", \"orange\")\n\ndraws_single %>%\n  filter(.draw <= 100) %>%\n  select(starts_with(\".\"), CL, V, KA, KE, sigma, starts_with(c(\"cp\", \"dv\"))) %>%\n  as_tibble() %>%\n  mutate(across(where(is.double), round, 3)) %>%\n  DT::datatable(rownames = FALSE, filter = \"top\",\n                options = list(scrollX = TRUE,\n                               columnDefs = list(list(className = 'dt-center',\n                                                      targets = \"_all\")))) %>%\n  DT::formatStyle(\".draw\", target = \"row\",\n                  backgroundColor = DT::styleEqual(draws_to_highlight,\n                                                   colors_to_highlight))\n\n\n\n\n\n\n\nCode\npreds_single <- draws_single %>%\n  spread_draws(cp[i], dv_pred[i]) %>%\n  mutate(time = torsten_data_fit$time_pred[i]) %>%\n  ungroup() %>%\n  arrange(.draw, time)\n\n\n\npreds_single %>%\n  mutate(sample_draws = .draw %in% draws_to_highlight,\n         color = case_when(.draw == draws_to_highlight[1] ~\n                             colors_to_highlight[1],\n                           .draw == draws_to_highlight[2] ~\n                             colors_to_highlight[2],\n                           .draw == draws_to_highlight[3] ~\n                             colors_to_highlight[3],\n                           .draw == draws_to_highlight[4] ~\n                             colors_to_highlight[4],\n                           .draw == draws_to_highlight[5] ~\n                             colors_to_highlight[5],\n                           TRUE ~ \"black\")) %>%\n  # filter(.draw %in% c(draws_to_highlight, sample(11:max(.draw), 100))) %>%\n  filter(.draw <= 100) %>%\n  arrange(desc(.draw)) %>%\n  ggplot(aes(x = time, y = cp, group = .draw)) +\n  geom_line(aes(size = sample_draws, alpha = sample_draws, color = color),\n            show.legend = FALSE) +\n  scale_color_manual(name = NULL,\n                     breaks = c(\"red\", \"blue\", \"green\", \"purple\", \"orange\",\n                                \"black\"),\n                     values = c(\"red\", \"blue\", \"green\", \"purple\", \"orange\",\n                                \"black\")) +\n  scale_size_manual(name = NULL,\n                    breaks = c(FALSE, TRUE),\n                    values = c(1, 1.5)) +\n  scale_alpha_manual(name = NULL,\n                     breaks = c(FALSE, TRUE),\n                     values = c(0.10, 1))  +\n  theme_bw(20) +\n  scale_x_continuous(name = \"Time (h)\") +\n  scale_y_continuous(name = \"Drug Concentration (ug/mL)\")\n\n\n\n\n\nThis collection of predicted concentration curves, one for each sample from the posterior distribution, gives us a distribution for the “true” concentration at each time point. From this distribution we can plot our mean prediction (essentially an IPRED curve) and 95% credible interval (the Bayesian version of a confidence interval) for that mean:\n\n\nCode\n(mean_and_ci <- preds_single %>%\n  group_by(time) %>%\n  mean_qi(cp, .width = 0.95) %>%\n  ggplot(aes(x = time, y = cp)) +\n  geom_line(size = 1.5) +\n  geom_ribbon(aes(ymin = .lower, ymax = .upper),\n              fill = \"yellow\", alpha = 0.25) +\n  theme_bw(20) +\n  scale_x_continuous(name = \"Time (h)\") +\n  scale_y_continuous(name = \"Drug Concentration (ug/mL)\") +\n   coord_cartesian(ylim = c(0, 4.5)))\n\n\n\n\n\n\n\n\n\nTo do some model checking and to make future predictions, we can also get a mean prediction and 95% prediction interval from our replicates of the concentration (one replicate, dv, for each draw from the posterior):\n\n\nCode\n(mean_and_pi <- preds_single %>%\n  group_by(time) %>%\n  mean_qi(dv_pred, .width = 0.95) %>%\n  ggplot(aes(x = time, y = dv_pred)) +\n  geom_line(size = 1.5) +\n  geom_ribbon(aes(ymin = .lower, ymax = .upper),\n              fill = \"yellow\", alpha = 0.25) +\n  theme_bw(20) +\n  scale_x_continuous(name = \"Time (h)\") +\n  scale_y_continuous(name = \"Drug Concentration (ug/mL)\") +\n   coord_cartesian(ylim = c(0, 6)))\n\n\n\n\n\n\n\n\n\nWhat’s actually happening is that we get the posterior density of the prediction for a given time3\n\n Show/Hide DV Curves \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s look at the posterior density for the time points that were actually observed:\n\n\nCode\nmean_and_pi +\n  stat_halfeye(data = preds_single %>%\n                 filter(time %in% times_to_observe),\n               aes(x = time, y = dv_pred, group = time),\n               scale = 2, interval_size = 2, .width = 0.95,\n               point_interval = mean_qi, normalize = \"xy\") +\n  geom_point(data = observed_data_torsten,\n             mapping = aes(x = time, y = dv), color = \"red\", size = 3) +\n  coord_cartesian(ylim = c(0, 6))\n\n\n\n\n\n\n\n\n\n\n Show/Hide Intervals"
  },
  {
    "objectID": "poppk.html#two-compartment-model-with-iv-infusion",
    "href": "poppk.html#two-compartment-model-with-iv-infusion",
    "title": "3  A Bayesian Approach to PK/PD using Stan and Torsten",
    "section": "3.3 Two-Compartment Model with IV Infusion",
    "text": "3.3 Two-Compartment Model with IV Infusion\nWe will use a simple, easily understood, and commonly used model to show many of the elements of the workflow of PopPK modeling in Stan with Torsten. We will talk about\n\nSimulation\nMethods for selection of priors\nHandling BLOQ values\nPrediction for observed subjects and potential future patients\nCovariate effects\nWithin-Chain parallelization to speed up the MCMC sampling\n\n\n3.3.1 PK Model\nIn this example I have simulated data from a two-compartment model with IV infusion with proportional-plus-additive error. The data-generating model is \\[C_{ij} = f(\\mathbf{\\theta}_i, t_{ij})*\\left( 1 + \\epsilon_{p_{ij}} \\right) + \\epsilon_{a_{ij}}\\] where \\(\\mathbf{\\theta}_i = \\left[CL_i, \\, V_{c_i}, \\, Q_i, \\,V_{p_i}\\right]^\\top\\) is a vector containing the individual parameters for individual \\(i\\)4, and \\(f(\\cdot, \\cdot)\\) is the two-compartment model mean function seen here. The corresponding system of ODEs is:\n\\[\\begin{align}\n\\frac{dA_d}{dt} &= -K_a*A_d \\notag \\\\\n\\frac{dA_c}{dt} &= rate_{in} + K_a*A_d - \\left(\\frac{CL}{V_c} + \\frac{Q}{V_c}\\right)A_C + \\frac{Q}{V_p}A_p  \\notag \\\\\n\\frac{dA_p}{dt} &= \\frac{Q}{V_c}A_c - \\frac{Q}{V_p}A_p \\\\\n\\end{align} \\tag{3.1}\\]\nand then \\(C = \\frac{A_c}{V_c}\\)5.\nThe true parameters used to simulate the data are as follows:\n\n\n\n\nTable 3.1: True Parameter Values\n\n\n\n\n\n\n\n\nParameter\nValue\nUnits\nDescription\n\n\n\n\nTVCL\n0.20\n\\(\\frac{L}{d}\\)\nPopulation value for clearance\n\n\nTVVC\n3.00\n\\(L\\)\nPopulation value for central compartment volume\n\n\nTVQ\n1.40\n\\(\\frac{L}{d}\\)\nPopulation value for intercompartmental clearance\n\n\nTVVP\n4.00\n\\(L\\)\nPopulation value for peripheral compartment volume\n\n\n\\(\\omega_{CL}\\)\n0.30\n-\nStandard deviation for IIV in \\(CL\\)\n\n\n\\(\\omega_{V_c}\\)\n0.25\n-\nStandard deviation for IIV in \\(V_c\\)\n\n\n\\(\\omega_{Q}\\)\n0.20\n-\nStandard deviation for IIV in \\(Q\\)\n\n\n\\(\\omega_{V_p}\\)\n0.15\n-\nStandard deviation for IIV in \\(V_p\\)\n\n\n\\(\\rho_{CL,V_c}\\)\n0.10\n-\nCorrelation between \\(\\eta_{CL}\\) and \\(\\eta_{V_c}\\)\n\n\n\\(\\rho_{CL,Q}\\)\n0.00\n-\nCorrelation between \\(\\eta_{CL}\\) and \\(\\eta_{Q}\\)\n\n\n\\(\\rho_{CL,V_p}\\)\n0.10\n-\nCorrelation between \\(\\eta_{CL}\\) and \\(\\eta_{V_p}\\)\n\n\n\\(\\rho_{V_c,Q}\\)\n-0.10\n-\nCorrelation between \\(\\eta_{V_c}\\) and \\(\\eta_{Q}\\)\n\n\n\\(\\rho_{V_c,V_p}\\)\n0.20\n-\nCorrelation between \\(\\eta_{V_c}\\) and \\(\\eta_{V_p}\\)\n\n\n\\(\\rho_{Q,V_p}\\)\n0.15\n-\nCorrelation between \\(\\eta_{Q}\\) and \\(\\eta_{V_p}\\)\n\n\n\\(\\sigma_p\\)\n0.20\n-\nStandard deviation for proportional residual error\n\n\n\\(\\sigma_a\\)\n0.03\n\\(\\frac{\\mu g}{mL}\\)\nStandard deviation for additive residual error\n\n\n\\(\\rho_{p,a}\\)\n0.00\n-\nCorrelation between \\(\\epsilon_p\\) and \\(\\epsilon_a\\)\n\n\n\n\n\n\n\n\n3.3.2 Statistical Model\nI’ll write down two models that are very similar with the only differences being the prior distributions on the population parameters.\n\nHalf-Cauchy PriorsLognormal Priors\n\n\nIn the interest of thoroughness and completeness, I’ll write down the full statistical model. For this presentation, I’ll just fit the data to the data-generating model. Letting \\[\\begin{align}\n\\Omega &=\n\\begin{pmatrix}\n\\omega^2_{CL} & \\omega_{CL, V_c} & \\omega_{CL, Q} & \\omega_{CL, V_p} \\\\\n\\omega_{CL, V_c} & \\omega^2_{V_c} & \\omega_{V_c, Q} & \\omega_{V_c, V_p} \\\\\n\\omega_{CL, Q} & \\omega_{V_c, Q} & \\omega^2_{Q} & \\omega_{Q, V_p} \\\\\n\\omega_{CL, V_p} & \\omega_{V_c, V_p} & \\omega_{Q, V_p} & \\omega^2_{V_p} \\\\\n\\end{pmatrix} \\\\\n&= \\begin{pmatrix}\n\\omega_{CL} & 0 & 0 & 0 \\\\\n0 & \\omega_{V_c} & 0 & 0 \\\\\n0 & 0 & \\omega_{Q} & 0 \\\\\n0 & 0 & 0 & \\omega_{V_p} \\\\\n\\end{pmatrix} \\mathbf{R_{\\Omega}}\n\\begin{pmatrix}\n\\omega_{CL} & 0 & 0 & 0 \\\\\n0 & \\omega_{V_c} & 0 & 0 \\\\\n0 & 0 & \\omega_{Q} & 0 \\\\\n0 & 0 & 0 & \\omega_{V_p} \\\\\n\\end{pmatrix} \\\\\n&= \\begin{pmatrix}\n\\omega_{CL} & 0 & 0 & 0 \\\\\n0 & \\omega_{V_c} & 0 & 0 \\\\\n0 & 0 & \\omega_{Q} & 0 \\\\\n0 & 0 & 0 & \\omega_{V_p} \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & \\rho_{CL, V_c} & \\rho_{CL, Q} & \\rho_{CL, V_p} \\\\\n\\rho_{CL, V_c} & 1 & \\rho_{V_c, Q} & \\rho_{V_c, V_p} \\\\\n\\rho_{CL, Q} & \\rho_{V_c, Q} & 1 & \\rho_{Q, V_p} \\\\\n\\rho_{CL, V_p} & \\rho_{V_c, V_p} & \\rho_{Q, V_p} & 1 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n\\omega_{CL} & 0 & 0 & 0 \\\\\n0 & \\omega_{V_c} & 0 & 0 \\\\\n0 & 0 & \\omega_{Q} & 0 \\\\\n0 & 0 & 0 & \\omega_{V_p} \\\\\n\\end{pmatrix}\n\\end{align}\\] I’ll model the correlation matrix \\(R_{\\Omega}\\) and standard deviations (\\(\\omega_p\\)) of the random effects rather than the covariance matrix \\(\\Omega\\) that is typically done in NONMEM. The full statistical model is \\[\\begin{align}\nC_{ij} \\mid \\mathbf{TV}, \\; \\mathbf{\\eta}_i, \\; \\mathbf{\\Omega}, \\; \\mathbf{\\Sigma}\n&\\sim Normal\\left( f(\\mathbf{\\theta}_i, t_{ij}), \\; \\sigma_{ij} \\right)\nI(C_{ij} > 0) \\notag \\\\\n\\mathbf{\\eta}_i \\; | \\; \\Omega &\\sim Normal\\left(\n\\begin{pmatrix}\n0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\\n\\end{pmatrix}\n, \\; \\Omega\\right) \\notag \\\\\nTVCL &\\sim Half-Cauchy\\left(0, scale_{TVCL}\\right) \\notag \\\\\nTVVC &\\sim Half-Cauchy\\left(0, scale_{TVVC}\\right) \\notag \\\\\nTVQ &\\sim Half-Cauchy\\left(0, scale_{TVQ}\\right) \\notag \\\\\nTVVP &\\sim Half-Cauchy\\left(0, scale_{TVVP}\\right) \\notag \\\\\n\\omega_{CL} &\\sim Half-Normal(0, scale_{\\omega_{CL}}) \\notag \\\\\n\\omega_{V_c} &\\sim Half-Normal(0, scale_{\\omega_{V_c}}) \\notag \\\\\n\\omega_{Q} &\\sim Half-Normal(0, scale_{\\omega_{Q}}) \\notag \\\\\n\\omega_{V_p} &\\sim Half-Normal(0, scale_{\\omega_{V_p}}) \\notag \\\\\nR_{\\Omega} &\\sim LKJ(df_{R_{\\Omega}}) \\notag \\\\\n\\sigma_p &\\sim Half-Normal(0, scale_{\\sigma_p}) \\\\\n\\sigma_a &\\sim Half-Normal(0, scale_{\\sigma_a}) \\\\\nR_{\\Sigma} &\\sim LKJ(df_{R_{\\Sigma}}) \\notag \\\\\nCL_i &= TVCL \\times e^{\\eta_{CL_i}} \\notag \\\\\nV_{c_i} &= TVVC \\times e^{\\eta_{V_{c_i}}} \\notag \\\\\nQ_i &= TVQ \\times e^{\\eta_{Q_i}} \\notag \\\\\nV_{p_i} &= TVVP \\times e^{\\eta_{V_{p_i}}} \\notag \\\\\n\\end{align}\\] where \\[\\begin{align}\n\\mathbf{TV} &=\n\\begin{pmatrix}\nTVCL \\\\ TVVC \\\\ TVQ \\\\ TVVP \\\\\n\\end{pmatrix}, \\;\n\\mathbf{\\theta}_i =\n\\begin{pmatrix}\nCL_i \\\\ V_{c_i} \\\\ Q_i \\\\ V_{p_i} \\\\\n\\end{pmatrix}, \\;\n\\mathbf{\\eta}_i =\n\\begin{pmatrix}\n\\eta_{CL_i} \\\\ \\eta_{V_{c_i}} \\\\ \\eta_{Q_i} \\\\ \\eta_{V_{p_i}} \\\\\n\\end{pmatrix}, \\;\n\\mathbf{\\Sigma} =\n\\begin{pmatrix}\n\\sigma^2_{p} & \\rho_{p,a}\\sigma_{p}\\sigma_{a} \\\\\n\\rho_{p,a}\\sigma_{p}\\sigma_{a} & \\sigma^2_{z} \\\\\n\\end{pmatrix} \\\\\n\\sigma_{ij} &= \\sqrt{f(\\mathbf{\\theta}_i, t_{ij})^2\\sigma^2_p + \\sigma^2_a + 2f(\\mathbf{\\theta}_i, t_{ij})\\rho_{p,a}\\sigma_{p}\\sigma_{a}}\n\\end{align}\\] Note: The indicator for \\(C_{ij} | \\ldots\\) indicates that we are truncating the distribution of the observed concentrations to be greater than 0.\n\n\nIn the interest of thoroughness and completeness, I’ll write down the full statistical model. For this presentation, I’ll just fit the data to the data-generating model. Letting \\[\\begin{align}\n\\Omega &=\n\\begin{pmatrix}\n\\omega^2_{CL} & \\omega_{CL, V_c} & \\omega_{CL, Q} & \\omega_{CL, V_p} \\\\\n\\omega_{CL, V_c} & \\omega^2_{V_c} & \\omega_{V_c, Q} & \\omega_{V_c, V_p} \\\\\n\\omega_{CL, Q} & \\omega_{V_c, Q} & \\omega^2_{Q} & \\omega_{Q, V_p} \\\\\n\\omega_{CL, V_p} & \\omega_{V_c, V_p} & \\omega_{Q, V_p} & \\omega^2_{V_p} \\\\\n\\end{pmatrix} \\\\\n&= \\begin{pmatrix}\n\\omega_{CL} & 0 & 0 & 0 \\\\\n0 & \\omega_{V_c} & 0 & 0 \\\\\n0 & 0 & \\omega_{Q} & 0 \\\\\n0 & 0 & 0 & \\omega_{V_p} \\\\\n\\end{pmatrix} \\mathbf{R_{\\Omega}}\n\\begin{pmatrix}\n\\omega_{CL} & 0 & 0 & 0 \\\\\n0 & \\omega_{V_c} & 0 & 0 \\\\\n0 & 0 & \\omega_{Q} & 0 \\\\\n0 & 0 & 0 & \\omega_{V_p} \\\\\n\\end{pmatrix} \\\\\n&= \\begin{pmatrix}\n\\omega_{CL} & 0 & 0 & 0 \\\\\n0 & \\omega_{V_c} & 0 & 0 \\\\\n0 & 0 & \\omega_{Q} & 0 \\\\\n0 & 0 & 0 & \\omega_{V_p} \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & \\rho_{CL, V_c} & \\rho_{CL, Q} & \\rho_{CL, V_p} \\\\\n\\rho_{CL, V_c} & 1 & \\rho_{V_c, Q} & \\rho_{V_c, V_p} \\\\\n\\rho_{CL, Q} & \\rho_{V_c, Q} & 1 & \\rho_{Q, V_p} \\\\\n\\rho_{CL, V_p} & \\rho_{V_c, V_p} & \\rho_{Q, V_p} & 1 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n\\omega_{CL} & 0 & 0 & 0 \\\\\n0 & \\omega_{V_c} & 0 & 0 \\\\\n0 & 0 & \\omega_{Q} & 0 \\\\\n0 & 0 & 0 & \\omega_{V_p} \\\\\n\\end{pmatrix}\n\\end{align}\\] I’ll model the correlation matrix \\(R_{\\Omega}\\) and standard deviations (\\(\\omega_p\\)) of the random effects rather than the covariance matrix \\(\\Omega\\) that is typically done in NONMEM. The full statistical model is \\[\\begin{align}\nC_{ij} \\mid \\mathbf{TV}, \\; \\mathbf{\\eta}_i, \\; \\mathbf{\\Omega}, \\; \\mathbf{\\Sigma}\n&\\sim Normal\\left( f(\\mathbf{\\theta}_i, t_{ij}), \\; \\sigma_{ij} \\right)\nI(C_{ij} > 0) \\notag \\\\\n\\mathbf{\\eta}_i \\; | \\; \\Omega &\\sim Normal\\left(\n\\begin{pmatrix}\n0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\\n\\end{pmatrix}\n, \\; \\Omega\\right) \\notag \\\\\nTVCL &\\sim Lognormal\\left(log\\left(location_{TVCL}\\right), scale_{TVCL}\\right) \\notag \\\\\nTVVC &\\sim Lognormal\\left(log\\left(location_{TVVC}\\right), scale_{TVVC}\\right) \\notag \\\\\nTVQ &\\sim Lognormal\\left(log\\left(location_{TVQ}\\right), scale_{TVQ}\\right) \\notag \\\\\nTVVP &\\sim Lognormal\\left(log\\left(location_{TVVP}\\right), scale_{TVVP}\\right) \\notag \\\\\n\\omega_{CL} &\\sim Half-Normal(0, scale_{\\omega_{CL}}) \\notag \\\\\n\\omega_{V_c} &\\sim Half-Normal(0, scale_{\\omega_{V_c}}) \\notag \\\\\n\\omega_{Q} &\\sim Half-Normal(0, scale_{\\omega_{Q}}) \\notag \\\\\n\\omega_{V_p} &\\sim Half-Normal(0, scale_{\\omega_{V_p}}) \\notag \\\\\nR_{\\Omega} &\\sim LKJ(df_{R_{\\Omega}}) \\notag \\\\\n\\sigma_p &\\sim Half-Normal(0, scale_{\\sigma_p}) \\\\\n\\sigma_a &\\sim Half-Normal(0, scale_{\\sigma_a}) \\\\\nR_{\\Sigma} &\\sim LKJ(df_{R_{\\Sigma}}) \\notag \\\\\nCL_i &= TVCL \\times e^{\\eta_{CL_i}} \\notag \\\\\nV_{c_i} &= TVVC \\times e^{\\eta_{V_{c_i}}} \\notag \\\\\nQ_i &= TVQ \\times e^{\\eta_{Q_i}} \\notag \\\\\nV_{p_i} &= TVVP \\times e^{\\eta_{V_{p_i}}} \\notag \\\\\n\\end{align}\\] where \\[\\begin{align}\n\\mathbf{TV} &=\n\\begin{pmatrix}\nTVCL \\\\ TVVC \\\\ TVQ \\\\ TVVP \\\\\n\\end{pmatrix}, \\;\n\\mathbf{\\theta}_i =\n\\begin{pmatrix}\nCL_i \\\\ V_{c_i} \\\\ Q_i \\\\ V_{p_i} \\\\\n\\end{pmatrix}, \\;\n\\mathbf{\\eta}_i =\n\\begin{pmatrix}\n\\eta_{CL_i} \\\\ \\eta_{V_{c_i}} \\\\ \\eta_{Q_i} \\\\ \\eta_{V_{p_i}} \\\\\n\\end{pmatrix}, \\;\n\\mathbf{\\Sigma} =\n\\begin{pmatrix}\n\\sigma^2_{p} & \\rho_{p,a}\\sigma_{p}\\sigma_{a} \\\\\n\\rho_{p,a}\\sigma_{p}\\sigma_{a} & \\sigma^2_{z} \\\\\n\\end{pmatrix} \\\\\n\\sigma_{ij} &= \\sqrt{f(\\mathbf{\\theta}_i, t_{ij})^2\\sigma^2_p + \\sigma^2_a + 2f(\\mathbf{\\theta}_i, t_{ij})\\rho_{p,a}\\sigma_{p}\\sigma_{a}}\n\\end{align}\\] Note: The indicator for \\(C_{ij} | \\ldots\\) indicates that we are truncating the distribution of the observed concentrations to be greater than 0.\n\n\n\nNote that for both of these models, I’ve used the non-centered parameterization (see here for more information on why the non-centered parameterization is often better from an MCMC algorithm standpoint), which is what we commonly use in the pharmacometrics world. You may also see the centered parameterization6. Also note7 and8.\n\n\n3.3.3 Data\nAs mentioned previously, we can simulate data directly in Stan with Torsten. For this example, we will simulate 24 individuals total, 3 subjects at each of 5 mg Q4W, 10 mg Q4W, 20 mg Q4W, 50 mg Q4W, 100 mg Q4W, 200 mg Q4W, 400 mg Q4W, and 800 mg Q4W for 24 weeks (6 cycles), where each dose is a 1-hour infusion. We take observations at nominal times 1, 3, 5, 24, 72, 168, and 336 hours after the first and second doses, and then trough measurements just before the last 4 doses.\nHere is the Stan code with Torsten functions used to simulate the data:\n\n\nCode\nmodel_simulate <- cmdstan_model(\"Torsten/Simulate/iv_2cmt_ppa.stan\")\n\nmodel_simulate$print()\n\n\n// IV Infusion\n// Two-compartment PK Model\n// IIV on CL, VC, Q, VP\n// proportional plus additive error - DV = CP(1 + eps_p) + eps_a\n// Closed form solution using Torsten\n// Observations are generated from a normal that is truncated below at 0\n// Since we have a normal distribution on the error, but the DV must be > 0, it\n//   generates values from a normal that is truncated below at 0\n\nfunctions{\n  \n  real normal_lb_rng(real mu, real sigma, real lb){\n    \n    real p_lb = normal_cdf(lb | mu, sigma);\n    real u = uniform_rng(p_lb, 1);\n    real y = mu + sigma * inv_Phi(u);\n    return y;\n\n  }\n  \n}\ndata{\n  \n  int n_subjects;\n  int n_total;                  \n  \n  array[n_total] real amt;\n  array[n_total] int cmt;\n  array[n_total] int evid;\n  array[n_total] real rate;\n  array[n_total] real ii;\n  array[n_total] int addl;\n  array[n_total] int ss;\n  array[n_total] real time;\n  \n  array[n_subjects] int subj_start;\n  array[n_subjects] int subj_end;\n  \n  real<lower = 0> TVCL;\n  real<lower = 0> TVVC;\n  real<lower = 0> TVQ;\n  real<lower = 0> TVVP;\n  \n  real<lower = 0> omega_cl;\n  real<lower = 0> omega_vc;\n  real<lower = 0> omega_q;\n  real<lower = 0> omega_vp;\n  \n  corr_matrix[4] R;  // Correlation matrix before transforming to Omega.\n                     // Can in theory change this to having inputs for\n                     // cor_cl_vc, cor_cl_q, ... and then construct the \n                     // correlation matrix in transformed data, but it's easy\n                     // enough to do in R\n  \n  real<lower = 0> sigma_p;\n  real<lower = 0> sigma_a;\n  real<lower = -1, upper = 1> cor_p_a;\n  \n}\ntransformed data{\n  \n  int n_random = 4;\n  int n_cmt = 3;\n\n  vector[n_random] omega = [omega_cl, omega_vc, omega_q, omega_vp]';\n  \n  matrix[n_random, n_random] L = cholesky_decompose(R);\n\n  vector[2] sigma = [sigma_p, sigma_a]';\n  matrix[2, 2] R_Sigma = rep_matrix(1, 2, 2);\n  R_Sigma[1, 2] = cor_p_a;\n  R_Sigma[2, 1] = cor_p_a;\n  \n  matrix[2, 2] Sigma = quad_form_diag(R_Sigma, sigma);\n  \n}\nmodel{\n  \n}\ngenerated quantities{\n  \n  vector[n_total] cp; // concentration with no residual error\n  vector[n_total] dv; // concentration with residual error\n  \n  vector[n_subjects] eta_cl;\n  vector[n_subjects] eta_vc;\n  vector[n_subjects] eta_q;\n  vector[n_subjects] eta_vp;\n  \n  vector[n_subjects] CL;\n  vector[n_subjects] VC;\n  vector[n_subjects] Q;\n  vector[n_subjects] VP;\n  \n  {\n    \n    vector[n_random] typical_values = to_vector({TVCL, TVVC, TVQ, TVVP});\n    \n    matrix[n_random, n_subjects] eta;   \n    matrix[n_subjects, n_random] theta; \n  \n    matrix[n_total, n_cmt] x_cp;\n    \n    for(i in 1:n_subjects){\n      eta[, i] = multi_normal_cholesky_rng(rep_vector(0, n_random),\n                                           diag_pre_multiply(omega, L));\n    }\n    theta = (rep_matrix(typical_values, n_subjects) .* exp(eta))';\n    \n    eta_cl = row(eta, 1)';\n    eta_vc = row(eta, 2)';\n    eta_q = row(eta, 3)';\n    eta_vp = row(eta, 4)';\n\n    CL = col(theta, 1);\n    VC = col(theta, 2);\n    Q = col(theta, 3);\n    VP = col(theta, 4);\n    \n    for(j in 1:n_subjects){\n      \n      array[n_random + 1] real theta_params = {CL[j], Q[j], VC[j], VP[j], 0};\n      \n\n      x_cp[subj_start[j]:subj_end[j],] =\n        pmx_solve_twocpt(time[subj_start[j]:subj_end[j]],\n                         amt[subj_start[j]:subj_end[j]],\n                         rate[subj_start[j]:subj_end[j]],\n                         ii[subj_start[j]:subj_end[j]],\n                         evid[subj_start[j]:subj_end[j]],\n                         cmt[subj_start[j]:subj_end[j]],\n                         addl[subj_start[j]:subj_end[j]],\n                         ss[subj_start[j]:subj_end[j]],\n                         theta_params)';\n\n      cp[subj_start[j]:subj_end[j]] = x_cp[subj_start[j]:subj_end[j], 2] ./ VC[j];\n    \n    }\n\n    for(i in 1:n_total){\n      if(cp[i] == 0){\n        dv[i] = 0;\n      }else{\n        real cp_tmp = cp[i];\n        real sigma_tmp = sqrt(square(cp_tmp) * Sigma[1, 1] + Sigma[2, 2] + \n                              2*cp_tmp*Sigma[2, 1]);\n        dv[i] = normal_lb_rng(cp_tmp, sigma_tmp, 0.0);\n        \n      }\n    }\n  }\n}\n\n\n\nSimulating a Dataset on a GridSimulating a Realistic Dataset with Jittered Time\n\n\nFor testing and illustration purposes, we often simulate a simple dataset where every subject is dosed and observed on a grid of nominal times9:\n\n\nCode\ncheck_valid_cov_mat <- function(x){\n\n  if(!is.matrix(x)) stop(\"'Matrix' is not a matrix.\")\n  if(!is.numeric(x)) stop(\"Matrix is not numeric.\")\n  if(!(nrow(x) == ncol(x))) stop(\"Matrix is not square.\")\n  # if(!(sum(x == t(x)) == (nrow(x)^2)))\n  #   stop(\"Matrix is not symmetric\")\n  if(!(isTRUE(all.equal(x, t(x)))))\n    stop(\"Matrix is not symmetric.\")\n\n  eigenvalues <- eigen(x, only.values = TRUE)$values\n  eigenvalues[abs(eigenvalues) < 1e-8] <- 0\n  if(any(eigenvalues < 0)){\n    stop(\"Matrix is not positive semi-definite.\")\n  }\n\n  return(TRUE)\n}\n\ncheck_valid_cor_mat <- function(x){\n\n  if(any(diag(x) != 1)) stop(\"Diagonal of matrix is not all 1s.\")\n  check_valid_cov_mat(x)\n\n  return(TRUE)\n}\n\ncreate_dosing_data <- function(n_subjects_per_dose = 6,\n                               dose_amounts = c(100, 200, 400),\n                               addl = 5, ii = 28, cmt = 2, tinf = 1,\n                               sd_min_dose = 0){\n\n  dosing_data <- expand.ev(ID = 1:n_subjects_per_dose, addl = addl, ii = ii,\n                           cmt = cmt, amt = dose_amounts, tinf = tinf,\n                           evid = 1) %>%\n    realize_addl() %>%\n    as_tibble() %>%\n    rename_all(toupper) %>%\n    rowwise() %>%\n    mutate(TIMENOM = TIME,\n           TIME = if_else(TIMENOM == 0, TIMENOM,\n                          TIMENOM + rnorm(1, 0, sd_min_dose/(60*24)))) %>%\n    ungroup() %>%\n    select(ID, TIMENOM, TIME, everything())\n\n  return(dosing_data)\n\n}\n\ncreate_nonmem_data <- function(dosing_data,\n                               sd_min_obs = 3,\n                               times_to_simulate = seq(0, 96, 1),\n                               times_obs = seq(0, 96, 1)){\n\n  data_set_obs <- tibble(TIMENOM = times_obs)\n  data_set_all <- tibble(TIMENOM = times_to_simulate)\n\n  data_set_times_obs <- create_times_nm(dosing_data, data_set_obs,\n                                        sd_min_obs) %>%\n    select(ID, TIMENOM, TIME, NONMEM)\n\n  data_set_times_pred <- bind_rows(replicate(max(dosing_data$ID),\n                                             data_set_all,\n                                             simplify = FALSE)) %>%\n    mutate(ID = rep(1:max(dosing_data$ID), each = nrow(data_set_all)),\n           TIME = TIMENOM,\n           NONMEM = FALSE) %>%\n    select(ID, TIMENOM, TIME, NONMEM)\n\n  data_set_times_all <- bind_rows(data_set_times_obs, data_set_times_pred) %>%\n    arrange(ID, TIME) %>%\n    select(ID, TIMENOM, TIME, NONMEM) %>%\n    mutate(CMT = 2,\n           EVID = 0,\n           AMT = NA_real_,\n           MDV = 0,\n           RATE = 0,\n           II = 0,\n           ADDL = 0)\n\n  nonmem_data_set <- bind_rows(data_set_times_all,\n                               dosing_data %>%\n                                 mutate(NONMEM = TRUE)) %>%\n    arrange(ID, TIME, EVID) %>%\n    filter(!(CMT == 2 & EVID == 0 & TIME == 0))\n\n  return(nonmem_data_set)\n\n}\n\ndosing_data <- expand.ev(ID = 1:3, addl = 5, ii = 28,\n                         cmt = 2, amt = c(5, 10, 20, 50, 100,\n                                          200, 400, 800),\n                         tinf = 1/24, evid = 1) %>%\n  realize_addl() %>%\n  as_tibble() %>%\n  rename_all(toupper) %>%\n  select(ID, TIME, everything())\n\nTVCL <- 0.2 # L/d\nTVVC <- 3   # L\nTVQ <- 1.4  # L/d\nTVVP <- 4   # L\n\nomega_cl <- 0.30\nomega_vc <- 0.25\nomega_q <- 0.2\nomega_vp <- 0.15\n\nR <- matrix(1, nrow = 4, ncol = 4)\nR[1, 2] <- R[2, 1] <- cor_cl_vc <- 0.1\nR[1, 3] <- R[3, 1] <- cor_cl_q <- 0\nR[1, 4] <- R[4, 1] <- cor_cl_vp <- 0.1\nR[2, 3] <- R[3, 2] <- cor_vc_q <- -0.1\nR[2, 4] <- R[4, 2] <- cor_vc_vp <- 0.2\nR[3, 4] <- R[4, 3] <- cor_q_vp <- 0.15\n\ntimes_to_simulate <- seq(12/24, max(dosing_data$TIME) + 28, by = 12/24)\ntimes_obs <- c(c(1, 3, 5, 24, 72, 168, 336)/24, 28 +\n                 c(0, 1, 3, 5, 24, 72, 168, 336)/24,\n               seq(56, max(dosing_data$TIME) + 28, by = 28))\ntimes_all <- sort(unique(c(times_to_simulate, times_obs)))\n\ntimes_new <- tibble(time = times_all)\n\nnonmem_data_simulate <- bind_rows(replicate(max(dosing_data$ID), times_new,\n                                            simplify = FALSE)) %>%\n  mutate(ID = rep(1:max(dosing_data$ID), each = nrow(times_new)),\n         amt = 0,\n         evid = 0,\n         rate = 0,\n         addl = 0,\n         ii = 0,\n         cmt = 2,\n         mdv = 0,\n         ss = 0,\n         nonmem = if_else(time %in% times_obs, TRUE, FALSE)) %>%\n  select(ID, time, everything()) %>%\n  bind_rows(dosing_data %>%\n              rename_all(tolower) %>%\n              rename(ID = \"id\") %>%\n              mutate(ss = 0,\n                     nonmem = TRUE)) %>%\n  arrange(ID, time)\n\nn_subjects <- nonmem_data_simulate %>%  # number of individuals to simulate\n  distinct(ID) %>%\n  count() %>%\n  deframe()\n\nn_total <- nrow(nonmem_data_simulate) # total number of time points at which to predict\n\nsubj_start <- nonmem_data_simulate %>%\n  mutate(row_num = 1:n()) %>%\n  group_by(ID) %>%\n  slice_head(n = 1) %>%\n  ungroup() %>%\n  select(row_num) %>%\n  deframe()\n\nsubj_end <- c(subj_start[-1] - 1, n_total)\n\nstan_data_simulate <- list(n_subjects = n_subjects,\n                           n_total = n_total,\n                           amt = nonmem_data_simulate$amt,\n                           cmt = nonmem_data_simulate$cmt,\n                           evid = nonmem_data_simulate$evid,\n                           rate = nonmem_data_simulate$rate,\n                           ii = nonmem_data_simulate$ii,\n                           addl = nonmem_data_simulate$addl,\n                           ss = nonmem_data_simulate$ss,\n                           time = nonmem_data_simulate$time,\n                           subj_start = subj_start,\n                           subj_end = subj_end,\n                           TVCL = TVCL,\n                           TVVC = TVVC,\n                           TVQ = TVQ,\n                           TVVP = TVVP,\n                           omega_cl = omega_cl,\n                           omega_vc = omega_vc,\n                           omega_q = omega_q,\n                           omega_vp = omega_vp,\n                           R = R,\n                           sigma_p = 0.2,\n                           sigma_a = 0.05,\n                           cor_p_a = 0)\n\nsimulated_data_grid <- model_simulate$sample(data = stan_data_simulate,\n                                        fixed_param = TRUE,\n                                        seed = 112358,\n                                        iter_warmup = 0,\n                                        iter_sampling = 1,\n                                        chains = 1,\n                                        parallel_chains = 1)\n\ndata_grid <- simulated_data_grid$draws(c(\"cp\", \"dv\"), format = \"draws_df\") %>%\n  spread_draws(cp[i], dv[i]) %>%\n  ungroup() %>%\n  mutate(time = nonmem_data_simulate$time[i],\n         ID = factor(nonmem_data_simulate$ID[i])) %>%\n  select(ID, time, cp, dv)\n\nnonmem_data_observed_grid <- nonmem_data_simulate %>%\n  mutate(DV = data_grid$dv,\n         mdv = if_else(evid == 1, 1, 0),\n         ID = factor(ID)) %>%\n  filter(nonmem == TRUE) %>%\n  select(-nonmem, -tinf) %>%\n  rename_all(toupper) %>%\n  mutate(DV = if_else(EVID == 1, NA_real_, DV))\n\n\n\n\nCode\nnonmem_data_observed_grid %>%\n  mutate(across(where(is.double), round, 3)) %>%\n  DT::datatable(rownames = FALSE, filter = \"top\",\n                options = list(scrollX = TRUE,\n                               columnDefs = list(list(className = 'dt-center',\n                                                      targets = \"_all\"))))\n\n\n\n\n\n\n\n\n\nIn reality, while doses and observations are scheduled at a nominal time, they tend to be off by a few minutes, and sometimes the observations are not made at all. So to make our dataset a bit more realistic, I have added a bit of jitter around the nominal time so that the subjects are taking their dose or getting their measurements in the neighborhood of the nominal time, rather than at the exact nominal time. I’ve also randomly removed an observation or two from some of the subjects10.\n\n\nCode\n# Check if the NONMEM times are ok (ordered by time within an individual).\n# Nominal time is automatically ok. This checks that the true measurement\n# time is ordered within an individual\n# Check if the NONMEM times are ok (ordered by time within an individual).\n# Nominal time is automatically ok. This checks that the true measurement\n# time is ordered within an individual\ncheck_times_nm <- function(data_set_times_nm){\n\n  minimum <- data_set_times_nm %>%\n    group_by(ID) %>%\n    mutate(across(TIME, ~.x - lag(.x))) %>%\n    ungroup() %>%\n    na.omit() %>%\n    summarize(min_time = min(TIME)) %>%\n    deframe() %>%\n    min\n\n  not_ok <- (minimum < 0)\n  return(not_ok)\n\n}\n\n# Simulate times to be slightly different from nominal time for something a bit\n# more realistic. It'll check that the times are ordered within individuals so\n# the time vector can be plugged into NONMEM and Stan. Trough measurements\n# should come shortly before a dose, and measurements at the end of infusion\n# should come slightly after the end of infusion\ncreate_times_nm <- function(dosing_data, data_set_tmp, sd_min){\n\n  not_ok <- TRUE\n  while(not_ok){\n\n    data_set_times_nm <- bind_rows(replicate(max(dosing_data$ID), data_set_tmp,\n                                             simplify = FALSE)) %>%\n      mutate(ID = rep(1:max(dosing_data$ID), each = nrow(data_set_tmp))) %>%\n      full_join(dosing_data %>%\n                  select(ID, TIMENOM, TIME, AMT, EVID, TINF),\n                by = c(\"TIMENOM\", \"ID\")) %>%\n      arrange(ID, TIMENOM) %>%\n      mutate(end_inf_timenom = TIMENOM + TINF,\n             end_inf = (TIMENOM %in% end_inf_timenom),\n             AMT = if_else(is.na(AMT), 0, AMT),\n             tmp_g = cumsum(c(FALSE, as.logical(diff(AMT)))),\n             num_doses_taken = cumsum(as.logical(AMT > 0))) %>%\n      group_by(ID) %>%\n      mutate(tmp_a = c(0, diff(TIMENOM)) * !AMT) %>%\n      group_by(tmp_g) %>%\n      mutate(NTSLD = cumsum(tmp_a)) %>%\n      ungroup() %>%\n      group_by(ID, num_doses_taken) %>%\n      mutate(time_prev_dose = unique(TIME[!is.na(TIME)])) %>%\n      ungroup() %>%\n      rowwise() %>%\n      mutate(TIME = case_when((TIMENOM == 0 && EVID == 1) ~ 0, # First time is time 0\n                              (TIMENOM > 0 && EVID == 1) ~       # trough measurement. Make sure\n                                time_prev_dose - abs(rnorm(1, 0, # it's before the new dose\n                                                           sd_min/(60*24))),\n                              (TIMENOM > 0 && end_inf == TRUE && is.na(EVID)) ~ # End of infusion.\n                                time_prev_dose + NTSLD +                        # Make sure it's after\n                                abs(rnorm(1, 0, sd_min/(60*24))),               # end of infusion\n                              (TIMENOM > 0 && end_inf == FALSE && is.na(EVID)) ~ # Everything else\n                                time_prev_dose + NTSLD +\n                                rnorm(1, 0, sd_min/(60*24)),\n                              TRUE ~ NA_real_)) %>%\n      ungroup() %>%\n      select(ID, TIMENOM, TIME) %>%\n      mutate(NONMEM = TRUE)\n\n    not_ok <- check_times_nm(data_set_times_nm)\n  }\n  return(data_set_times_nm)\n}\n\ncreate_cor_mat <- function(...){\n\n  args <- list(...)\n\n  for(i in 1:length(args)) {\n    assign(x = names(args)[i], value = args[[i]])\n  }\n\n  x <- matrix(1, ncol = 5, nrow = 5)\n  x[2, 1] <- x[1, 2] <- cor_cl_vc\n  x[3, 1] <- x[1, 3] <- cor_cl_q\n  x[4, 1] <- x[1, 4] <- cor_cl_vp\n  x[3, 2] <- x[2, 3] <- cor_vc_q\n  x[4, 2] <- x[2, 4] <- cor_vc_vp\n  x[4, 3] <- x[3, 4] <- cor_q_vp\n\n  return(x)\n}\n\ncheck_valid_cov_mat <- function(x){\n\n  if(!is.matrix(x)) stop(\"'Matrix' is not a matrix.\")\n  if(!is.numeric(x)) stop(\"Matrix is not numeric.\")\n  if(!(nrow(x) == ncol(x))) stop(\"Matrix is not square.\")\n  # if(!(sum(x == t(x)) == (nrow(x)^2)))\n  #   stop(\"Matrix is not symmetric\")\n  if(!(isTRUE(all.equal(x, t(x)))))\n    stop(\"Matrix is not symmetric.\")\n\n  eigenvalues <- eigen(x, only.values = TRUE)$values\n  eigenvalues[abs(eigenvalues) < 1e-8] <- 0\n  if(any(eigenvalues < 0)){\n    stop(\"Matrix is not positive semi-definite.\")\n  }\n\n  return(TRUE)\n}\n\ncheck_valid_cor_mat <- function(x){\n\n  if(any(diag(x) != 1)) stop(\"Diagonal of matrix is not all 1s.\")\n  check_valid_cov_mat(x)\n\n  return(TRUE)\n}\n\ncreate_dosing_data <- function(n_subjects_per_dose = 6,\n                               dose_amounts = c(100, 200, 400),\n                               addl = 5, ii = 28, cmt = 2, tinf = 1/24,\n                               sd_min_dose = 0){\n\n  dosing_data <- expand.ev(ID = 1:n_subjects_per_dose, addl = addl, ii = ii,\n                           cmt = cmt, amt = dose_amounts, tinf = tinf,\n                           evid = 1) %>%\n    realize_addl() %>%\n    as_tibble() %>%\n    rename_all(toupper) %>%\n    rowwise() %>%\n    mutate(TIMENOM = TIME,\n           TIME = if_else(TIMENOM == 0, TIMENOM,\n                          TIMENOM + rnorm(1, 0, sd_min_dose/(60*24)))) %>%\n    ungroup() %>%\n    select(ID, TIMENOM, TIME, everything())\n\n  return(dosing_data)\n\n}\n\ncreate_nonmem_data <- function(dosing_data,\n                               sd_min_obs = 3,\n                               times_to_simulate = seq(0, 96, 1),\n                               times_obs = seq(0, 96, 1)){\n\n  data_set_obs <- tibble(TIMENOM = times_obs)\n  data_set_all <- tibble(TIMENOM = times_to_simulate)\n\n  data_set_times_obs <- create_times_nm(dosing_data, data_set_obs,\n                                        sd_min_obs) %>%\n    select(ID, TIMENOM, TIME, NONMEM)\n\n  data_set_times_pred <- bind_rows(replicate(max(dosing_data$ID),\n                                             data_set_all,\n                                             simplify = FALSE)) %>%\n    mutate(ID = rep(1:max(dosing_data$ID), each = nrow(data_set_all)),\n           TIME = TIMENOM,\n           NONMEM = FALSE) %>%\n    select(ID, TIMENOM, TIME, NONMEM)\n\n  data_set_times_all <- bind_rows(data_set_times_obs, data_set_times_pred) %>%\n    arrange(ID, TIME) %>%\n    select(ID, TIMENOM, TIME, NONMEM) %>%\n    mutate(CMT = 2,\n           EVID = 0,\n           AMT = NA_real_,\n           MDV = 0,\n           RATE = 0,\n           II = 0,\n           ADDL = 0)\n\n  nonmem_data_set <- bind_rows(data_set_times_all,\n                               dosing_data %>%\n                                 mutate(NONMEM = TRUE)) %>%\n    arrange(ID, TIME, EVID) %>%\n    filter(!(CMT == 2 & EVID == 0 & TIME == 0))\n\n  return(nonmem_data_set)\n\n}\n\n\ndosing_data <- create_dosing_data(n_subjects_per_dose = 3,\n                                  dose_amounts = c(5, 10, 20, 50, 100,\n                                                   200, 400, 800), # mg\n                                  addl = 5, ii = 28, cmt = 2, tinf = 1/24,\n                                  sd_min_dose = 5)\n\nTVCL <- 0.2 # L/d\nTVVC <- 3   # L\nTVQ <- 1.4  # L/d\nTVVP <- 4   # L\n\nomega_cl <- 0.30\nomega_vc <- 0.25\nomega_q <- 0.2\nomega_vp <- 0.15\n\nR <- matrix(1, nrow = 4, ncol = 4)\nR[1, 2] <- R[2, 1] <- cor_cl_vc <- 0.1\nR[1, 3] <- R[3, 1] <- cor_cl_q <- 0\nR[1, 4] <- R[4, 1] <- cor_cl_vp <- 0.1\nR[2, 3] <- R[3, 2] <- cor_vc_q <- -0.1\nR[2, 4] <- R[4, 2] <- cor_vc_vp <- 0.2\nR[3, 4] <- R[4, 3] <- cor_q_vp <- 0.15\n\nsd_min_obs <- 5 # standard deviation in minutes for the observed true time\n                # around the nominal time\n\ntimes_to_simulate <- seq(0, max(dosing_data$TIMENOM) + 28, by = 12/24)\ntimes_obs <- c(c(1, 3, 5, 24, 72, 168, 336)/24, 28 +\n                 c(0, 1, 3, 5, 24, 72, 168, 336)/24,\n               seq(56, max(dosing_data$TIMENOM) + 28, by = 28))\n\nnonmem_data_simulate <- create_nonmem_data(\n  dosing_data,\n  sd_min_obs = sd_min_obs,\n  times_to_simulate = times_to_simulate,\n  times_obs = times_obs) %>%\n  rename_all(tolower) %>%\n  rename(ID = \"id\") %>%\n  mutate(ss = 0,\n         amt = if_else(is.na(amt), 0, amt))\n\nn_subjects <- nonmem_data_simulate %>%  # number of individuals to simulate\n  distinct(ID) %>%\n  count() %>%\n  deframe()\n\nn_total <- nrow(nonmem_data_simulate) # total number of time points at which to predict\n\nsubj_start <- nonmem_data_simulate %>%\n  mutate(row_num = 1:n()) %>%\n  group_by(ID) %>%\n  slice_head(n = 1) %>%\n  ungroup() %>%\n  select(row_num) %>%\n  deframe()\n\nsubj_end <- c(subj_start[-1] - 1, n_total)\n\nstan_data_simulate <- list(n_subjects = n_subjects,\n                           n_total = n_total,\n                           amt = nonmem_data_simulate$amt,\n                           cmt = nonmem_data_simulate$cmt,\n                           evid = nonmem_data_simulate$evid,\n                           rate = nonmem_data_simulate$rate,\n                           ii = nonmem_data_simulate$ii,\n                           addl = nonmem_data_simulate$addl,\n                           ss = nonmem_data_simulate$ss,\n                           time = nonmem_data_simulate$time,\n                           subj_start = subj_start,\n                           subj_end = subj_end,\n                           TVCL = TVCL,\n                           TVVC = TVVC,\n                           TVQ = TVQ,\n                           TVVP = TVVP,\n                           omega_cl = omega_cl,\n                           omega_vc = omega_vc,\n                           omega_q = omega_q,\n                           omega_vp = omega_vp,\n                           R = R,\n                           sigma_p = 0.2,\n                           sigma_a = 0.05,\n                           cor_p_a = 0)\n\nsimulated_data <- model_simulate$sample(data = stan_data_simulate,\n                                        fixed_param = TRUE,\n                                        seed = 112358,\n                                        iter_warmup = 0,\n                                        iter_sampling = 1,\n                                        chains = 1,\n                                        parallel_chains = 1)\n\ndata <- simulated_data$draws(c(\"cp\", \"dv\"), format = \"draws_df\") %>%\n  spread_draws(cp[i], dv[i]) %>%\n  ungroup() %>%\n  mutate(time = nonmem_data_simulate$time[i],\n         ID = factor(nonmem_data_simulate$ID[i])) %>%\n  select(ID, time, cp, dv)\n\nnonmem_data_observed_tmp <- nonmem_data_simulate %>%\n  mutate(DV = data$dv,\n         mdv = if_else(evid == 1, 1, 0),\n         ID = factor(ID)) %>%\n  filter(nonmem == TRUE) %>%\n  select(-nonmem, -tinf)\n\nnonmem_data_observed <- nonmem_data_observed_tmp %>%\n  filter(evid == 0) %>%\n  group_by(ID) %>%\n  nest() %>%\n  mutate(num_rows_to_remove = rbinom(1, size = 2, prob = 0.5)) %>%\n  ungroup() %>%\n  mutate(num_rows = map_dbl(data, nrow),\n         num_rows_to_keep = num_rows - num_rows_to_remove,\n         sample = map2(data, num_rows_to_keep, sample_n)) %>%\n  unnest(sample) %>%\n  select(ID, everything(), -data, -starts_with(\"num_rows\")) %>%\n  bind_rows(nonmem_data_observed_tmp %>%\n              filter(evid == 1)) %>%\n  arrange(ID, time) %>%\n  rename_all(toupper) %>%\n  mutate(DV = if_else(EVID == 1, NA_real_, DV))\n\n\n\n\n\n\n\n\nCode\nnonmem_data_observed %>%\n  mutate(across(where(is.double), round, 3)) %>%\n  DT::datatable(rownames = FALSE, filter = \"top\",\n                options = list(scrollX = TRUE,\n                               columnDefs = list(list(className = 'dt-center',\n                                                      targets = \"_all\"))))\n\n\n\n\n\n\n\n\n\n\nNow that we have simulated a simple dataset on a grid and another dataset that is a bit more realistic that has missed observations and dosing and sampling times that are slightly off from nominal time, we will continue the rest of this discussion using the more realistic dataset.\nWe can visualize the observed data in the NONMEM data set that we will be modeling (with dosing events marked with a magenta dashed line):\n\n\n\nCode\n(p1 <- ggplot(nonmem_data_observed %>%\n                mutate(ID = factor(ID)) %>%\n                group_by(ID) %>%\n                mutate(Dose = factor(max(AMT, na.rm = TRUE))) %>%\n                ungroup() %>%\n                filter(MDV == 0)) +\n  geom_line(mapping = aes(x = TIME, y = DV, group = ID, color = Dose)) +\n  geom_point(mapping = aes(x = TIME, y = DV, group = ID, color = Dose)) +\n  scale_color_discrete(name = \"Dose (mg)\") +\n  scale_y_log10(name = latex2exp::TeX(\"$Drug Conc. \\\\; (\\\\mu g/mL)$\"),\n                limits = c(NA, NA)) +\n  scale_x_continuous(name = \"Time (w)\",\n                     breaks = seq(0, 168, by = 28),\n                     labels = seq(0, 168/7, by = 4)) +\n  theme_bw(18) +\n  theme(axis.text = element_text(size = 14, face = \"bold\"),\n        axis.title = element_text(size = 18, face = \"bold\"),\n        axis.line = element_line(size = 2),\n        legend.position = \"bottom\") +\n  geom_vline(data = nonmem_data_observed %>%\n               mutate(ID = factor(ID)) %>%\n               filter(EVID == 1),\n             mapping = aes(xintercept = TIME, group = ID),\n             linetype = 2, color = \"magenta\", alpha = 0.5) +\n  facet_wrap(~ID, labeller = label_both, ncol = 3, scales = \"free_y\"))\n\n\n\n\n\n\n\n\n3.3.4 Eliciting Prior Information\nThe use of the prior distribution can range from being a nuisance that serves simply as a catalyst that allows us to express uncertainty via Bayes’ theorem to a means to stabilize the sampling algorithm to actually incorporating knowledge about the parameter(s) before collecting data. We will discuss a couple common methods for eliciting prior information and setting your prior distributions.\n\n3.3.4.1 Drug Class and/or Historical Information\nA simple method that I’ll show here is to look at other drugs in the same class. For this example I used “true” PK parameters that are similar to a monoclonal antibody (mAb) I’ve worked on in the past to simulate this data. Since this fictional drug is a mAb, I can look at this paper to see a review of published clearances and volumes of distribution for other mAbs:\n\n\n\nI will set my priors in such a way that there is plenty of prior mass over the values that we’ve previously seen in mAbs (in red) but not so wide that the sampler will go looking at non-sensical values like \\(CL = 300 \\frac{L}{d}\\)\n\\[\\begin{align}\nTVCL &\\sim Half-Cauchy\\;(0, \\; 2) \\notag \\\\\nTVVC &\\sim Half-Cauchy\\;(0, \\; 10)\n\\end{align}\\]\n\n\nCode\nmy_length <- 200\n\ndens_data <- tibble(parameter = rep(c(\"TVCL\", \"TVVC\"), each = my_length),\n                    value = c(seq(0, 10, length.out = my_length),\n                              seq(0, 40, length.out = my_length))) %>%\n  bind_rows(tibble(parameter = rep(c(\"TVCL\", \"TVVC\"), each = 2),\n                   value = c(0.09, 0.56, 3, 8))) %>%\n  arrange(parameter, value) %>%\n  mutate(scale = if_else(parameter == \"TVCL\", 2, 10),\n         dens = dcauchy(value, scale = scale)/pcauchy(0, scale = scale))\n\np_cl <- ggplot(data = dens_data %>%\n                 filter(parameter == \"TVCL\"),\n               aes(x = value, y = dens, group = parameter)) +\n  geom_line(size = 1.25) +\n  theme_bw(18) +\n  geom_area(mapping = aes(x = if_else(between(value, 0.09, 0.56), value, 0)),\n            fill = \"red\") +\n  scale_y_continuous(name = \"Density\",\n                     limits = c(0, 0.34),\n                     expand = c(0, 0)) +\n  scale_x_continuous(name = \"Clearance (L/d)\",\n                     expand = c(0, 0),\n                     breaks = seq(0, 10, by = 2.5),\n                     labels = c(\"0\", \"2.5\", \"5\", \"7.5\", \"10\"))\n\np_vc <- ggplot(data = dens_data %>%\n                 filter(parameter == \"TVVC\"),\n               aes(x = value, y = dens, group = parameter)) +\n  geom_line(size = 1.25) +\n  theme_bw(18) +\n  geom_area(mapping = aes(x = if_else(between(value, 3, 8), value, 0)),\n            fill = \"red\") +\n  scale_y_continuous(name = \"Density\",\n                     limits = c(0, 0.07),\n                     expand = c(0, 0)) +\n  scale_x_continuous(name = \"Volume (L)\",\n                     expand = c(0, 0))\np_cl | p_vc\n\n\n\n\n\nThis could also be done with lognormal prior distributions (which might be a little more familiar in the PK/PD world):\\[\\begin{align}\nTVCL &\\sim Lognormal\\;(log(0.5), \\; 1) \\notag \\\\\nTVVC &\\sim Lognormal\\;(log(6), \\; 1)\n\\end{align}\\]\n\n\nCode\ndens_data_2 <- tibble(parameter = rep(c(\"TVCL\", \"TVVC\"), each = my_length),\n                      value = c(seq(0, 5, length.out = my_length),\n                                seq(0, 40, length.out = my_length))) %>%\n  bind_rows(tibble(parameter = rep(c(\"TVCL\", \"TVVC\"), each = 2),\n                   value = c(0.09, 0.56, 3, 8))) %>%\n  arrange(parameter, value) %>%\n  mutate(location = if_else(parameter == \"TVCL\", log(0.5), log(6)),\n         scale = if_else(parameter == \"TVCL\", 1, 1),\n         dens = dlnorm(value, meanlog = location,\n                       sdlog = scale))\n\np_cl_2 <- ggplot(data = dens_data_2 %>%\n                   filter(parameter == \"TVCL\"),\n                 aes(x = value, y = dens, group = parameter)) +\n  geom_line(size = 1.25) +\n  theme_bw(18) +\n  geom_area(mapping = aes(x = if_else(between(value, 0.09, 0.56), value, 0)),\n            fill = \"red\") +\n  scale_y_continuous(name = \"Density\",\n                     limits = c(0, 1.35),\n                     expand = c(0, 0)) +\n  scale_x_continuous(name = \"Clearance (L/d)\",\n                     expand = c(0, 0),\n                     breaks = seq(0, 5, by = 1),\n                     labels = seq(0, 5, by = 1))\n\n\np_vc_2 <- ggplot(data = dens_data_2 %>%\n                   filter(parameter == \"TVVC\"),\n                 aes(x = value, y = dens, group = parameter)) +\n  geom_line(size = 1.25) +\n  theme_bw(18) +\n  geom_area(mapping = aes(x = if_else(between(value, 3, 8), value, 0)),\n            fill = \"red\") +\n  scale_y_continuous(name = \"Density\",\n                     limits = c(0, 0.15),\n                     expand = c(0, 0)) +\n  scale_x_continuous(name = \"Volume (L)\",\n                     expand = c(0, 0))\np_cl_2 | p_vc_2\n\n\n\n\n\nEither way, my goal here was to set a weakly informative prior by looking at historical estimates for drugs in the same class, then making sure there is plenty of prior mass over the range of previous estimates while also ruling out values that really wouldn’t make any sense. Then ideally the data will take it from there.\n\n\n3.3.4.2 Priors for Nested Models and Occam’s Razor\nOften times in pharmacometrics we extend our models because we believe they are failing to capture some aspect of the data. For example, extending a one compartment model to a two compartment model to account for two-phase elimination. Other examples in pharmacometrics abound such as delay compartments, nonlinear clearance, and exponential versus logistic tumor growth. These models are all nested in the sense that the less complex model is a special case of the more complex model with parameter values fixed to some constant. For example, a one compartment model can be thought of as a two compartment model where the \\(Q\\) parameter is fixed to zero, or in other words, a two compartment model with an infinitely informative prior with all of its mass on zero. At the other extreme is the extended model with completely uninformative priors on the additional parameters, which in practice can cause computational issues when data alone does not identify the additional parameters very well.\nFor nested models we can place priors on the extended parameters that give us the best of the both worlds. In a two compartment model for example, a Half-Cauchy prior on \\(Q\\) is a relaxation of the one compartment model that slightly favors the simpler explanation of \\(Q=0\\), but also allows for non-zero \\(Q\\) if the data provides enough evidence to support it. This idea of incorporating the principle of Occam’s razor in our inference via priors, relieves computational issues and is the principle behind modern complex non-parameteric models and tools like LASSO that can fit many parameters by regularizing to favor the simpler nested model.\n\n\n3.3.4.3 Prior Predictive Checks\nA wealth of prior information about the properties of our drugs exists in the form of published historical data, pre-clinical experiments, and basic physical constraints. However, for complex models we occasionally have parameters whose interpretation and relationship to data which we observe is unclear or highly complex. In these cases, prior predictive checks are a powerful tool for helping us elucidate these relationships and choose our priors. The principle is that while we may not always know reasonable values for a parameter in a complex model, we generally have a good understanding of reasonable values of our data. We briefly illustrate this using a two compartment model.\nA prior predictive check simply consists of taking draws from your prior and simulating data using those draws. To do this, all we have to do is fit our model without any data. This can be done easily in Stan/Torsten by fitting a model as we normally would except we place the likelihood in an if-statement that controls whether we wants to draw from the prior only or the full posterior which includes both the prior and the likelihood. This is done in the two compartment model below where we use log-normal priors on the parameters.\n\n\nCode\nmodel_ppc <- cmdstan_model(\"Torsten/Fit/iv_2cmt_ppa_lognormal_priors_ppc.stan\")\n\nmodel_ppc$print()\n\n\n// IV Infusion\n// Two-compartment PK Model\n// IIV on CL, VC, Q, VP (full covariance matrix)\n// proportional plus additive error - DV = CP(1 + eps_p) + eps_a\n// Closed form solution using Torsten\n// Implements threading for within-chain parallelization \n\nfunctions{\n\n  array[] int sequence(int start, int end) { \n    array[end - start + 1] int seq;\n    for (n in 1:num_elements(seq)) {\n      seq[n] = n + start - 1;\n    }\n    return seq; \n  } \n  \n  int num_between(int lb, int ub, array[] int y){\n    \n    int n = 0;\n    for(i in 1:num_elements(y)){\n      if(y[i] >= lb && y[i] <= ub)\n                    n = n + 1;\n    }\n    return n;\n    \n  }\n  \n  array[] int find_between(int lb, int ub, array[] int y) {\n    // vector[num_between(lb, ub, y)] result;\n    array[num_between(lb, ub, y)] int result;\n    int n = 1;\n    for (i in 1:num_elements(y)) {\n      if (y[i] >= lb && y[i] <= ub) {\n        result[n] = y[i];\n        n = n + 1;\n      }\n    }\n    return result;\n  }\n  \n  vector find_between_vec(int lb, int ub, array[] int idx, vector y) {\n    \n    vector[num_between(lb, ub, idx)] result;\n    int n = 1;\n    if(num_elements(idx) != num_elements(y)) reject(\"illegal input\");\n    for (i in 1:rows(y)) {\n      if (idx[i] >= lb && idx[i] <= ub) {\n        result[n] = y[i];\n        n = n + 1;\n      }\n    }\n    return result;\n  }\n  \n  real partial_sum_lpmf(array[] int seq_subj, int start, int end,\n                        vector dv_obs, array[] int dv_obs_id, array[] int i_obs,\n                        array[] real amt, array[] int cmt, array[] int evid, \n                        array[] real time, array[] real rate, array[] real ii, \n                        array[] int addl, array[] int ss,\n                        array[] int subj_start, array[] int subj_end, \n                        real TVCL, real TVVC, real TVQ, real TVVP, \n                        vector omega, matrix L, matrix Z, \n                        vector sigma, matrix L_Sigma,\n                        int n_random, int n_subjects, int n_total){\n                           \n    real ptarget = 0;\n    row_vector[n_random] typical_values = to_row_vector({TVCL, TVVC, TVQ, TVVP});\n    \n    matrix[n_subjects, n_random] eta = diag_pre_multiply(omega, L * Z)';\n\n    matrix[n_subjects, n_random] theta =\n                          (rep_matrix(typical_values, n_subjects) .* exp(eta));\n                          \n    matrix[2, 2] R_Sigma = multiply_lower_tri_self_transpose(L_Sigma);\n    matrix[2, 2] Sigma = quad_form_diag(R_Sigma, sigma);\n    \n                              \n    int N = end - start + 1;    // number of subjects in this slice  \n    vector[n_total] dv_ipred;   // = rep_vector(0, n_total);\n    matrix[n_total, 3] x_ipred; // = rep_matrix(0, n_total, 2);\n  \n\n    int n_obs_slice = num_between(subj_start[start], subj_end[end], i_obs);\n    array[n_obs_slice] int i_obs_slice = find_between(subj_start[start], \n                                                      subj_end[end], i_obs);\n                                                \n    vector[n_obs_slice] dv_obs_slice = find_between_vec(start, end, \n                                                        dv_obs_id, dv_obs);\n    \n    vector[n_obs_slice] ipred_slice;\n    \n    \n    for(n in 1:N){            // loop over subjects in this slice\n    \n      int nn = n + start - 1; // nn is the ID of the current subject\n      \n      row_vector[n_random] theta_nn = theta[nn]; // access the parameters for subject nn\n      real cl = theta_nn[1];\n      real vc = theta_nn[2];\n      real q = theta_nn[3];\n      real vp = theta_nn[4];\n      array[5] real theta_params = {cl, q, vc, vp, 0}; // The 0 is for KA. Skip the absorption\n      \n      x_ipred[subj_start[nn]:subj_end[nn], ] =\n        pmx_solve_twocpt(time[subj_start[nn]:subj_end[nn]],\n                         amt[subj_start[nn]:subj_end[nn]],\n                         rate[subj_start[nn]:subj_end[nn]],\n                         ii[subj_start[nn]:subj_end[nn]],\n                         evid[subj_start[nn]:subj_end[nn]],\n                         cmt[subj_start[nn]:subj_end[nn]],\n                         addl[subj_start[nn]:subj_end[nn]],\n                         ss[subj_start[nn]:subj_end[nn]],\n                         theta_params)';\n                      \n      dv_ipred[subj_start[nn]:subj_end[nn]] = \n        x_ipred[subj_start[nn]:subj_end[nn], 2] ./ vc; \n    \n    }\n  \n    \n    ipred_slice = dv_ipred[i_obs_slice];\n    \n    ptarget += normal_lpdf(dv_obs_slice | ipred_slice, \n                                        sqrt(square(ipred_slice) * Sigma[1, 1] +\n                                             Sigma[2, 2] + \n                                             2 * ipred_slice * Sigma[2, 1]));\n    return ptarget;\n                           \n  }\n  \n}\ndata{\n  \n  int n_subjects;\n  int n_total;\n  int n_obs;\n  array[n_obs] int i_obs;\n  array[n_total] int ID;\n  array[n_total] real amt;\n  array[n_total] int cmt;\n  array[n_total] int evid;\n  array[n_total] real rate;\n  array[n_total] real ii;\n  array[n_total] int addl;\n  array[n_total] int ss;\n  array[n_total] real time;\n  vector[n_total] dv;\n  array[n_subjects] int subj_start;\n  array[n_subjects] int subj_end;\n  \n  real<lower = 0> location_tvcl;   // Prior Location parameter for CL\n  real<lower = 0> location_tvvc;   // Prior Location parameter for VC\n  real<lower = 0> location_tvq;    // Prior Location parameter for Q\n  real<lower = 0> location_tvvp;   // Prior Location parameter for VP\n  \n  real<lower = 0> scale_tvcl;      // Prior Scale parameter for CL\n  real<lower = 0> scale_tvvc;      // Prior Scale parameter for VC\n  real<lower = 0> scale_tvq;       // Prior Scale parameter for Q\n  real<lower = 0> scale_tvvp;      // Prior Scale parameter for VP\n  \n  real<lower = 0> scale_omega_cl; // Prior scale parameter for omega_cl\n  real<lower = 0> scale_omega_vc; // Prior scale parameter for omega_vc\n  real<lower = 0> scale_omega_q;  // Prior scale parameter for omega_q\n  real<lower = 0> scale_omega_vp; // Prior scale parameter for omega_vp\n  \n  real<lower = 0> lkj_df_omega;   // Prior degrees of freedom for omega cor mat\n  \n  real<lower = 0> scale_sigma_p;  // Prior Scale parameter for proportional error\n  real<lower = 0> scale_sigma_a;  // Prior Scale parameter for additive error\n  real<lower = 0> lkj_df_sigma;   // Prior degrees of freedom for sigma cor mat\n  \n  // If set to 1, don't include the likelihood so we can do prior predictive checks\n  int <lower = 0, upper = 1> prior_only; \n \n}\ntransformed data{ \n  \n  int grainsize = 1;\n  \n  vector[n_obs] dv_obs = dv[i_obs];\n  array[n_obs] int dv_obs_id = ID[i_obs];\n  \n  int n_random = 4;                    // Number of random effects\n  \n  array[n_random] real scale_omega = {scale_omega_cl, scale_omega_vc, \n                                      scale_omega_q, scale_omega_vp}; \n  array[2] real scale_sigma = {scale_sigma_p, scale_sigma_a};\n  \n  array[n_subjects] int seq_subj = sequence(1, n_subjects); // reduce_sum over subjects\n  \n}\nparameters{ \n  \n  real<lower = 0> TVCL;       \n  real<lower = 0> TVVC; \n  real<lower = 0> TVQ;\n  real<lower = 0> TVVP; \n  \n  vector<lower = 0>[n_random] omega;\n  cholesky_factor_corr[n_random] L;\n  \n  vector<lower = 0>[2] sigma;\n  cholesky_factor_corr[2] L_Sigma;\n  \n  matrix[n_random, n_subjects] Z;\n  \n}\n\nmodel{ \n  \n  // Priors\n  TVCL ~ lognormal(log(location_tvcl), scale_tvcl);\n  TVVC ~ lognormal(log(location_tvvc), scale_tvvc);\n  TVQ ~ lognormal(log(location_tvq), scale_tvq);\n  TVVP ~ lognormal(log(location_tvvp), scale_tvvp);\n\n  omega ~ normal(0, scale_omega);\n  L ~ lkj_corr_cholesky(lkj_df_omega);\n  \n  sigma ~ normal(0, scale_sigma);\n  L_Sigma ~ lkj_corr_cholesky(lkj_df_sigma);\n  \n  to_vector(Z) ~ std_normal();\n  \n  // Likelihood\n  if(prior_only == 0) {\n    target += reduce_sum(partial_sum_lupmf, seq_subj, grainsize,\n                         dv_obs, dv_obs_id, i_obs,\n                         amt, cmt, evid, time, \n                         rate, ii, addl, ss, subj_start, subj_end, \n                         TVCL, TVVC, TVQ, TVVP, omega, L, Z,\n                         sigma, L_Sigma, \n                         n_random, n_subjects, n_total);\n  }\n  \n}\ngenerated quantities{\n  \n  real<lower = 0> sigma_p = sigma[1];\n  real<lower = 0> sigma_a = sigma[2];\n  \n  real<lower = 0> sigma_sq_p = square(sigma_p);\n  real<lower = 0> sigma_sq_a = square(sigma_a); \n\n  real<lower = 0> omega_cl = omega[1];\n  real<lower = 0> omega_vc = omega[2];\n  real<lower = 0> omega_q = omega[3];\n  real<lower = 0> omega_vp = omega[4];\n\n  real<lower = 0> omega_sq_cl = square(omega_cl);\n  real<lower = 0> omega_sq_vc = square(omega_vc);\n  real<lower = 0> omega_sq_q = square(omega_q);\n  real<lower = 0> omega_sq_vp = square(omega_vp);\n\n  real cor_cl_vc;\n  real cor_cl_q;\n  real cor_cl_vp;\n  real cor_vc_q;\n  real cor_vc_vp;\n  real cor_q_vp;\n  real omega_cl_vc;\n  real omega_cl_q;\n  real omega_cl_vp;\n  real omega_vc_q;\n  real omega_vc_vp;\n  real omega_q_vp;\n\n  real cor_p_a;\n  real sigma_p_a;\n\n  vector[n_subjects] eta_cl;\n  vector[n_subjects] eta_vc;\n  vector[n_subjects] eta_q;\n  vector[n_subjects] eta_vp;\n  vector[n_subjects] CL;\n  vector[n_subjects] VC;\n  vector[n_subjects] Q;\n  vector[n_subjects] VP;\n  \n  vector[n_obs] ipred;\n  vector[n_obs] pred;\n  vector[n_obs] dv_ppc;\n  vector[n_obs] log_lik;\n  vector[n_obs] res;\n  vector[n_obs] wres;\n  vector[n_obs] ires;\n  vector[n_obs] iwres;\n \n  {\n    row_vector[n_random] typical_values = to_row_vector({TVCL, TVVC, TVQ, TVVP});\n\n    matrix[n_random, n_random] R = multiply_lower_tri_self_transpose(L);\n    matrix[n_random, n_random] Omega = quad_form_diag(R, omega);\n\n    matrix[2, 2] R_Sigma = multiply_lower_tri_self_transpose(L_Sigma);\n    matrix[2, 2] Sigma = quad_form_diag(R_Sigma, sigma);\n\n    matrix[n_subjects, n_random] eta = diag_pre_multiply(omega, L * Z)';\n\n    matrix[n_subjects, n_random] theta =\n                          (rep_matrix(typical_values, n_subjects) .* exp(eta));\n\n    vector[n_total] dv_pred;\n    matrix[n_total, 3] x_pred;\n    vector[n_total] dv_ipred;\n    matrix[n_total, 3] x_ipred;\n    \n    array[5] real theta_params_tv = {TVCL, TVQ, TVVC, TVVP, 0}; // 0 is for KA to skip absorption\n    \n    eta_cl = col(eta, 1);\n    eta_vc = col(eta, 2);\n    eta_q = col(eta, 3);\n    eta_vp = col(eta, 4);\n    \n    CL = col(theta, 1);\n    VC = col(theta, 2);\n    Q = col(theta, 3);\n    VP = col(theta, 4);\n\n    cor_cl_vc = R[1, 2];\n    cor_cl_q = R[1, 3];\n    cor_cl_vp = R[1, 4];\n    cor_vc_q = R[2, 3];\n    cor_vc_vp = R[2, 4];\n    cor_q_vp = R[3, 4];\n    omega_cl_vc = Omega[1, 2];\n    omega_cl_q = Omega[1, 3];\n    omega_cl_vp = Omega[1, 4];\n    omega_vc_q = Omega[2, 3];\n    omega_vc_vp = Omega[2, 4];\n    omega_q_vp = Omega[3, 4];\n\n    cor_p_a = R_Sigma[2, 1];\n    sigma_p_a = Sigma[2, 1];\n\n    for(j in 1:n_subjects){\n\n      array[5] real theta_params = {CL[j], Q[j], VC[j], VP[j], 0};\n      \n      x_ipred[subj_start[j]:subj_end[j],] =\n        pmx_solve_twocpt(time[subj_start[j]:subj_end[j]],\n                         amt[subj_start[j]:subj_end[j]],\n                         rate[subj_start[j]:subj_end[j]],\n                         ii[subj_start[j]:subj_end[j]],\n                         evid[subj_start[j]:subj_end[j]],\n                         cmt[subj_start[j]:subj_end[j]],\n                         addl[subj_start[j]:subj_end[j]],\n                         ss[subj_start[j]:subj_end[j]],\n                         theta_params)';\n                      \n                      \n      dv_ipred[subj_start[j]:subj_end[j]] = \n        x_ipred[subj_start[j]:subj_end[j], 2] ./ VC[j];\n\n      x_pred[subj_start[j]:subj_end[j],] =\n        pmx_solve_twocpt(time[subj_start[j]:subj_end[j]],\n                         amt[subj_start[j]:subj_end[j]],\n                         rate[subj_start[j]:subj_end[j]],\n                         ii[subj_start[j]:subj_end[j]],\n                         evid[subj_start[j]:subj_end[j]],\n                         cmt[subj_start[j]:subj_end[j]],\n                         addl[subj_start[j]:subj_end[j]],\n                         ss[subj_start[j]:subj_end[j]],\n                         theta_params_tv)';\n\n      dv_pred[subj_start[j]:subj_end[j]] = \n        x_pred[subj_start[j]:subj_end[j], 2] ./ TVVC;\n    }\n\n    pred = dv_pred[i_obs];\n    ipred = dv_ipred[i_obs];\n\n  }\n\n  res = dv_obs - pred;                                                                             \n  ires = dv_obs - ipred;\n\n  for(i in 1:n_obs){\n    real ipred_tmp = ipred[i];\n    real sigma_tmp = sqrt(square(ipred_tmp) * sigma_sq_p + sigma_sq_a + \n                          2*ipred_tmp*sigma_p_a);\n    dv_ppc[i] = normal_rng(ipred_tmp, sigma_tmp);\n    log_lik[i] = normal_lpdf(dv_obs[i] | ipred_tmp, sigma_tmp);\n    wres[i] = res[i]/sigma_tmp;\n    iwres[i] = ires[i]/sigma_tmp;\n  }\n  \n}\n\n\nWe prepare the data as we normally would for Stan/Torsten except we include a new toggle prior_only and set it to 1. This data preparation step is mostly just book keeping to tell Stan and Torsten how many patients we’re fitting, how many observations, and the indices of each patient in the data frame we’re passing in.\nWe also specify the priors in this data preparation step. We start with the log-normal priors specified above along with vague log-normal priors on the second compartment volume and transfer rate.\nThe goal is to see whether for the prior values considered, the data generated from those priors, in this case PK concentration over time, results in sensible values and values that are able to capture your data. As an aside, priors can be thought to set the prior values considered in our search space where we will try to find all the combinations of parameters that reasonably fit our data. From this perspective it is easy to see why vague or no priors can lead to computational issues when data is not adequate, because you can be consdering an unbounded search space!\nIn general, you can start with vague priors and gradually tighten them so that your prior does not consider nonsensical values, e.g. values of concentration that are unreasonably high or low. Alternatively, you can start with highly informative priors centered around a value of the parameters where you know the solution makes sense, and you can gradually loosen them until the corresponding data generated from those priors considers all reasonable values of data you expect to see. We illustrate by starting somewhere in the middle, but the latter technique is sometimes preferred in practice as it is more computationally stable.\n\n\nCode\nnonmem_data <- read_csv(\"Data/iv_2cmt_no_bloq.csv\",\n                        na = \".\") %>%\n  rename_all(tolower) %>%\n  rename(ID = \"id\",\n         DV = \"dv\") %>%\n  mutate(DV = if_else(is.na(DV), 5555555, DV)) # This value can be anything except NA. It'll be indexed away\n\nn_subjects <- nonmem_data %>%  # number of individuals\n  distinct(ID) %>%\n  count() %>%\n  deframe()\n\nn_total <- nrow(nonmem_data)   # total number of records\n\ni_obs <- nonmem_data %>%       # index for observation records\n  mutate(row_num = 1:n()) %>%\n  filter(evid == 0) %>%\n  select(row_num) %>%\n  deframe()\n\nn_obs <- length(i_obs)         # number of observation records\n\nsubj_start <- nonmem_data %>%\n  mutate(row_num = 1:n()) %>%\n  group_by(ID) %>%\n  slice_head(n = 1) %>%\n  ungroup() %>%\n  select(row_num) %>%\n  deframe()\n\nsubj_end <- c(subj_start[-1] - 1, n_total)\n\nstan_data <- list(n_subjects = n_subjects,\n                  n_total = n_total,\n                  n_obs = n_obs,\n                  i_obs = i_obs,\n                  ID = nonmem_data$ID,\n                  amt = nonmem_data$amt,\n                  cmt = nonmem_data$cmt,\n                  evid = nonmem_data$evid,\n                  rate = nonmem_data$rate,\n                  ii = nonmem_data$ii,\n                  addl = nonmem_data$addl,\n                  ss = nonmem_data$ss,\n                  time = nonmem_data$time,\n                  dv = nonmem_data$DV,\n                  subj_start = subj_start,\n                  subj_end = subj_end,\n                  location_tvcl = 0.5, scale_tvcl = 1,\n                  location_tvvc = 6, scale_tvvc = 1,\n                  location_tvq = 1, scale_tvq = 2,\n                  location_tvvp = 3, scale_tvvp = 2,\n                  scale_omega_cl = 0.4,\n                  scale_omega_vc = 0.4,\n                  scale_omega_q = 0.4,\n                  scale_omega_vp = 0.4,\n                  lkj_df_omega = 2,\n                  scale_sigma_p = 0.5,\n                  scale_sigma_a = 1,\n                  lkj_df_sigma = 2,\n                  prior_only = 1L)\n\n\nAfter setting up the input data and our priors, we now fit the model as we normally would:\n\n\nCode\nfit_ppc1 <- model_ppc$sample(data = stan_data,\n                      seed = 112358,\n                      chains = 4,\n                      parallel_chains = 4,\n                      iter_warmup = 500,\n                      iter_sampling = 1000,\n                      adapt_delta = 0.8,\n                      refresh = 500,\n                      max_treedepth = 10,\n                      init = function() list(TVCL = rlnorm(1, log(0.3), 0.3),\n                                             TVVC = rlnorm(1, log(6), 0.3),\n                                             TVQ = rlnorm(1, log(1), 0.3),\n                                             TVVP = rlnorm(1, log(3), 0.3),\n                                             omega = rlnorm(5, log(0.3), 0.3),\n                                             sigma = rlnorm(2, log(0.5), 0.3)))\n\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 1500 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 1500 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 1500 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 1500 [  0%]  (Warmup) \nChain 1 Iteration:  500 / 1500 [ 33%]  (Warmup) \nChain 1 Iteration:  501 / 1500 [ 33%]  (Sampling) \nChain 2 Iteration:  500 / 1500 [ 33%]  (Warmup) \nChain 2 Iteration:  501 / 1500 [ 33%]  (Sampling) \nChain 3 Iteration:  500 / 1500 [ 33%]  (Warmup) \nChain 3 Iteration:  501 / 1500 [ 33%]  (Sampling) \nChain 4 Iteration:  500 / 1500 [ 33%]  (Warmup) \nChain 4 Iteration:  501 / 1500 [ 33%]  (Sampling) \nChain 2 Iteration: 1000 / 1500 [ 66%]  (Sampling) \nChain 1 Iteration: 1000 / 1500 [ 66%]  (Sampling) \nChain 4 Iteration: 1000 / 1500 [ 66%]  (Sampling) \nChain 3 Iteration: 1000 / 1500 [ 66%]  (Sampling) \nChain 1 Iteration: 1500 / 1500 [100%]  (Sampling) \nChain 4 Iteration: 1500 / 1500 [100%]  (Sampling) \nChain 2 Iteration: 1500 / 1500 [100%]  (Sampling) \nChain 3 Iteration: 1500 / 1500 [100%]  (Sampling) \nChain 1 finished in 24.4 seconds.\nChain 2 finished in 24.5 seconds.\nChain 4 finished in 24.4 seconds.\nChain 3 finished in 24.5 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 24.5 seconds.\nTotal execution time: 25.1 seconds.\n\n\nWe now plot the concentration data generated from our use of priors. Specifically, we look at concentration values at the first few study time points using the dosing regimen that was given to the first three patients in our study. The concentration values are generated in Stan’s generated quantities block by solving the two compartment model at the various values drawn from our prior. No additional observational noise is added (although we could if we wanted to also tune the prior on \\(\\sigma\\)). We summarize the drawn concentration samples with medians along with 5% and 95% quantiles. Lastly, we overlay the actual data of our three patients who actually were assigned this dosing regimen over these prior predictive samples to give us additional perspective on whether the simulated concentration values generated from our prior are reasonable.\n\n\nCode\nipred_ppc <- fit_ppc1$summary(\"ipred\") %>%\n  bind_cols(filter(nonmem_data, evid == 0)) %>%\n  filter(ID == 1, time <= 28)\n\nnonmem_data_patient123 <-nonmem_data %>%\n  filter(evid == 0) %>%\n  mutate(i = row_number()) %>%\n  filter(ID %in% c(1, 2, 3), time <= 28)\n\nipred_ppc %>%\n  ggplot(aes(time, median)) +\n  geom_pointrange(aes(ymin = q5, ymax = q95)) +\n  geom_point(aes(time, DV), color = \"red\", data = nonmem_data_patient123) +\n  scale_y_log10() +\n  theme_bw(20) +\n  scale_x_continuous(name = \"Time (d)\") +\n  scale_y_log10(name = \"Drug Concentration (ug/mL)\")\n\n\n\n\n\nFrom the plot it seems like we are considering a reasonable range of solutions for early values of concentration with our prior. However, for the later value, just before the second dose, we are clearly considering values that are unreasonably low. We know that for a mAB with a long half-life we should not have such low concentration after 28 days. Furthermore none of our observed data comes close to being that low. To explore further which parameter values that we are considering with our prior correspond to these unreasonable values, we can plot a scatter plot of our prior samples versus the simulated value at this time point (called ipred[6] in our model).\n\n\nCode\nmcmc_pairs(fit_ppc1$draws(c(\"TVCL\", \"TVVC\", \"TVQ\", \"TVVP\", \"ipred[6]\")),\n           diag_fun = \"dens\",\n           transformations = \"log\")\n\n\n\n\n\n\n\n\n\nWe can see that the particularly low values of ipred[6] correspond closely to high TVCL, low TVVC, and somewhat to low TVVP. We can thus limit our priors in these areas to give less weight to these values. Note that the most extreme points of ipred[6] occur when both TVCL is large and TVVC is small. This could suggest that we want a prior on these parameters with a negative correlation, although we’ll keep the priors uncorrelated for now. For the sake of the example, we’re assuming we don’t have much intuition about the parameters but we know that the posterior of volume and clearance are often negatively correlated.\n\n\nCode\nstan_data2 <- stan_data\nstan_data2$location_tvcl <- 0.2\nstan_data2$scale_tvcl <- 0.5\nstan_data2$location_tvvc <- 5\nstan_data2$scale_tvvc <- 0.5\nstan_data2$scale_tvvp <- 1\n\n\n\n\nCode\nfit_ppc2 <- model_ppc$sample(data = stan_data2,\n                             seed = 112358,\n                             chains = 4,\n                             parallel_chains = 4,\n                             iter_warmup = 500,\n                             iter_sampling = 1000,\n                             adapt_delta = 0.8,\n                             refresh = 500,\n                             max_treedepth = 10,\n                             init = function() list(TVCL = rlnorm(1, log(0.3), 0.3),\n                                                    TVVC = rlnorm(1, log(6), 0.3),\n                                                    TVQ = rlnorm(1, log(1), 0.3),\n                                                    TVVP = rlnorm(1, log(3), 0.3),\n                                                    omega = rlnorm(5, log(0.3), 0.3),\n                                                    sigma = rlnorm(2, log(0.5), 0.3)))\n\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 1500 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 1500 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 1500 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 1500 [  0%]  (Warmup) \nChain 1 Iteration:  500 / 1500 [ 33%]  (Warmup) \nChain 1 Iteration:  501 / 1500 [ 33%]  (Sampling) \nChain 2 Iteration:  500 / 1500 [ 33%]  (Warmup) \nChain 2 Iteration:  501 / 1500 [ 33%]  (Sampling) \nChain 3 Iteration:  500 / 1500 [ 33%]  (Warmup) \nChain 3 Iteration:  501 / 1500 [ 33%]  (Sampling) \nChain 4 Iteration:  500 / 1500 [ 33%]  (Warmup) \nChain 4 Iteration:  501 / 1500 [ 33%]  (Sampling) \nChain 1 Iteration: 1000 / 1500 [ 66%]  (Sampling) \nChain 4 Iteration: 1000 / 1500 [ 66%]  (Sampling) \nChain 2 Iteration: 1000 / 1500 [ 66%]  (Sampling) \nChain 3 Iteration: 1000 / 1500 [ 66%]  (Sampling) \nChain 1 Iteration: 1500 / 1500 [100%]  (Sampling) \nChain 4 Iteration: 1500 / 1500 [100%]  (Sampling) \nChain 2 Iteration: 1500 / 1500 [100%]  (Sampling) \nChain 1 finished in 23.9 seconds.\nChain 3 Iteration: 1500 / 1500 [100%]  (Sampling) \nChain 4 finished in 23.9 seconds.\nChain 2 finished in 24.1 seconds.\nChain 3 finished in 24.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 24.0 seconds.\nTotal execution time: 24.6 seconds.\n\n\n\n\nCode\nipred_ppc <- fit_ppc2$summary(\"ipred\") %>%\n  bind_cols(filter(nonmem_data, evid == 0)) %>%\n  filter(ID == 1, time <= 28)\n\nipred_ppc %>%\n  ggplot(aes(time, median)) +\n  geom_pointrange(aes(ymin = q5, ymax = q95)) +\n  geom_point(aes(time, DV), color = \"red\", data = nonmem_data_patient123) +\n  scale_y_log10() +\n  theme_bw(20) +\n  scale_x_continuous(name = \"Time (d)\") +\n  scale_y_log10(name = \"Drug Concentration (ug/mL)\")\n\n\n\n\n\nNow with our remodel refit to give us our new prior predictive check with the adjusted priors we can see that for ipred[6] we are no longer considering unreasonably low values. Meanwhile at the other time point we consider a reasonable range of parameter values that lead to concentrations which encompass our observed data. Further tuning could be done to broaden or tighten the search space as desired.\nAs a final and related note to this section, we briefly mention the concept of placing priors directly on derived quantities. In our prior predictive check example we knew reasonable ranges for our concentration (data) values, but we didn’t know reasonable values for our parameters or how they relate to the resulting concentrations. We thus hand-tuned our priors so that the resulting prior parameter values considered corresponded to reasonable output concentration solutions. In Stan there is indeed an even more direct way to do this. We can place priors on parameter-derived quantities directly! For example, we can place a lognormal prior on ipred[6] directly in our Stan model block so that unreasonably low values are avoided. Since ipred[6] is of course a function of the parameters, this induces a prior on the parameters.\nStan allows a lot of creativity on priors on derived quantities, as long as it’s expressible in an equation. For example we can specify a lognormal in the resulting amount (instead of concentration) of drug at a particular time point and specify that we think it should be no less than 1 10,000th of the dose after an hour. We can also place priors on things like half life for which we know the expression of as a function of parameters (incidentally this will induce a prior on volume and clearance that has a negative correlation).\n\n\n3.3.4.4 Flat and Other “non-informative” Priors\nPeople often try to be as “objective” as possible, so they use either a flat prior or an extremely wide prior. For example, in the NONMEM Users Guide, they use a multivariate normal prior distribution on the (log-)population parameters:\n\\[\\begin{align}\n\\begin{pmatrix}\nlog(TVCL) \\\\ log(TVVC) \\\\ log(TVQ) \\\\ log(TVVP) \\\\ log(TVKA) \\\\\n\\end{pmatrix} \\sim Normal\\left(\n\\begin{pmatrix}\n2 \\\\ 2 \\\\ 2 \\\\ 2 \\\\ 2 \\\\\n\\end{pmatrix}\n, \\;\n\\begin{pmatrix}\n10,000 & 0 & 0 & 0 & 0 \\\\\n0 & 10,000 & 0 & 0 & 0 \\\\\n0 & 0 & 10,000 & 0 & 0 \\\\\n0 & 0 & 0 & 10,000 & 0 \\\\\n0 & 0 & 0 & 0 & 10,000 \\\\\n\\end{pmatrix}\n\\right) \\notag \\\\\n\\end{align}\\]\nand say “because variances are very large, this means that the prior information on THETAS is highly uninformative.” However, if we look at this distribution after transforming to the natural space (i.e. \\(TVVC = exp(log(TVVC))\\)), then we see that this distribution is not at all uninformative:\n\n\nCode\ndata_uninformative <- tibble(TVVC = c(seq(0, 15, by = 0.0001), seq(16, 100,\n                                                                   by = 1),\n                     seq(101, 10001, by = 10))) %>%\n  mutate(dens = dlnorm(TVVC, log(2),  sqrt(10000)))\n\np_non_informative <- ggplot(data_uninformative, aes(x = TVVC, y = dens)) +\n  geom_line() +\n  scale_x_continuous(limits = c(0, 100),\n                     trans = \"identity\") +\n  scale_y_continuous(name = \"Density\",\n                     trans = \"identity\") +\n  geom_hline(yintercept = 0) +\n  theme_bw(18) +\n  ggtitle('\"Uninformative\" Prior') +\n  theme(plot.title = element_text(hjust = 0.5))\n\nplotly::ggplotly(p_non_informative)\n\n\n\n\n\n\nIn fact, this “uninformative” distribution pushes almost half of the prior mass to 0 and then has almost half of the prior mass at values that are nonsensically large:\n\n\nCode\ntibble(dist = c(\"Half-Cauchy(0, 10)\", \"Lognormal(log(6), 1)\",\n                \"Lognormal(2, 10000)\", \"Lognormal(2, 100)\"),\n       p_between_3_and_8 = c((pcauchy(8, 0, 10) - pcauchy(3, 0, 10))/\n                               pcauchy(0, 0, 10),\n                             plnorm(8, log(6), 1) - plnorm(3, log(6), 1),\n                             plnorm(8, 2, sqrt(10000)) -\n                               plnorm(3, 2, sqrt(10000)),\n                             plnorm(8, 2, sqrt(100)) -\n                               plnorm(3, 2, sqrt(100))),\n       p_lt_0.05 = c((pcauchy(0.5, 0, 10) - pcauchy(0, 0, 10))/pcauchy(0, 0, 10),\n                    plnorm(0.5, log(6), 1),\n                    plnorm(0.5, 2, sqrt(10000)),\n                    plnorm(0.5, 2, sqrt(100))),\n       p_gt_100 = c(pcauchy(100, 0, 10, lower.tail = FALSE)/pcauchy(0, 0, 10),\n                    plnorm(100, log(6), 1, lower.tail = FALSE),\n                    plnorm(100, 2, sqrt(10000), lower.tail = FALSE),\n                    plnorm(100, 2, sqrt(100), lower.tail = FALSE))) %>%\n  mutate(across(where(is.numeric), round, 3)) %>%\n  knitr::kable(col.names = c(\"Distribution\",\n                             \"P(3 < TVVC < 8)\",\n                             \"P(TVVC < 0.5)\",\n                             \"P(TVVC > 100)\")) %>%\n  kableExtra::kable_styling(full_width = FALSE)\n\n\n\n\n \n  \n    Distribution \n    P(3 < TVVC < 8) \n    P(TVVC < 0.5) \n    P(TVVC > 100) \n  \n \n\n  \n    Half-Cauchy(0, 10) \n    0.244 \n    0.032 \n    0.063 \n  \n  \n    Lognormal(log(6), 1) \n    0.369 \n    0.006 \n    0.002 \n  \n  \n    Lognormal(2, 10000) \n    0.004 \n    0.489 \n    0.490 \n  \n  \n    Lognormal(2, 100) \n    0.039 \n    0.394 \n    0.397 \n  \n\n\n\n\n\nThe main idea here is that what is “noninformative” on one scale might not be noninformative after a transformation. Don’t just give a parameter a wide prior variance without thinking about the meaning of the prior values. Also, we almost always know something about the parameter, even if it’s just the scale of potential values it might take, e.g. absorption rate constants tend to be less than 5 - higher than that we basically have a step function (or we maybe could change the units of time).\n\n\n\n3.3.5 Fitting the Model in Stan with Torsten\nThe main things we want to do after collecting our data and eliciting good prior information are fitting a model to our data to obtain a posterior distribution and prediction/simulation from the posterior. We can do the fitting and prediction/simulation either separately or simultaneously. It doesn’t make much difference if the fitting and prediction for observed individuals is done separately or simultaneously, but my recommendation is to do those first and simulate unobserved individuals separately, since we might not know what we want to predict/simulate at the time that we do the fitting.\nTo do this fitting and predicting/simulating, I have it set up to have a few .stan files:\n\nA file that fits the model. This will fit the data and save the posterior distributions in a CmdStanMCMC object for the\n\npopulation parameters (\\(TVCL,\\;TVVC,\\;TVQ,\\;TVVP\\)),\nindividual parameters (\\(CL, \\;VC, \\;Q, \\;VP\\)),\nindividual \\(\\eta\\) values,\nvariability \\(\\left(\\omega_{CL}, \\;\\omega_{V_c}, \\;\\omega_{Q}, \\;\\omega_{V_p}\\right)\\), covariance and correlation parameters \\(\\left(\\omega_{P_1, P_2}, \\;\\rho_{P_1, P_2}\\right)\\), and uncertainty parameters \\(\\left(\\sigma_p, \\; \\sigma_a\\right)\\).\nPRED, IPRED, RES, WRES, IRES, IWRES\nreplications of the observed data for posterior predictive checking (PPC, similar to a VPC),\nlog_lik variable that is used for model comparison (Leave-One-Out Cross Validation).\n\nA file that makes predictions at unobserved locations for the observed individuals and saves them in a CmdStanGQ object. This is mainly just to predict at a denser grid to make nice smooth pictures of posterior predictions for the observed individuals and to make the PPC look nicer.\nA file that makes predictions for a new, unobserved individual or individuals and saves them in a CmdStanGQ object. This is useful for simulating a population or for doing clinical trial simulations taking into account all sources of variability and uncertainty.\nA file that does 1. and 2. simultaneously.\n\nYou can choose whether you want to do 1. and 2. sequentially or simultaneously.\n\n3.3.5.1 A Standard Way to Write the Model\nThe first model I will show is relatively clean and easy-to-read.\n\n\nCode\nmodel1 <- cmdstan_model(\"Torsten/Fit/iv_2cmt_ppa_no_threading.stan\")\n\nmodel1$print()\n\n\n// IV infusion\n// Two-compartment PK Model\n// IIV on CL, VC, Q, VP (full covariance matrix)\n// proportional plus additive error - DV = CP(1 + eps_p) + eps_a\n// Closed form solution using Torsten\n\ndata{\n  \n  int n_subjects;\n  int n_total;\n  int n_obs;\n  array[n_obs] int i_obs;\n  array[n_total] int ID;\n  array[n_total] real amt;\n  array[n_total] int cmt;\n  array[n_total] int evid;\n  array[n_total] real rate;\n  array[n_total] real ii;\n  array[n_total] int addl;\n  array[n_total] int ss;\n  array[n_total] real time;\n  vector[n_total] dv;\n  array[n_subjects] int subj_start;\n  array[n_subjects] int subj_end;\n\n  real<lower = 0> scale_tvcl;      // Prior Scale parameter for CL\n  real<lower = 0> scale_tvvc;      // Prior Scale parameter for VC\n  real<lower = 0> scale_tvq;       // Prior Scale parameter for Q\n  real<lower = 0> scale_tvvp;      // Prior Scale parameter for VP\n  \n  real<lower = 0> scale_omega_cl; // Prior scale parameter for omega_cl\n  real<lower = 0> scale_omega_vc; // Prior scale parameter for omega_vc\n  real<lower = 0> scale_omega_q;  // Prior scale parameter for omega_q\n  real<lower = 0> scale_omega_vp; // Prior scale parameter for omega_vp\n  \n  real<lower = 0> lkj_df_omega;   // Prior degrees of freedom for omega cor mat\n  \n  real<lower = 0> scale_sigma_p;  // Prior Scale parameter for proportional error\n  real<lower = 0> scale_sigma_a;  // Prior Scale parameter for additive error\n  real<lower = 0> lkj_df_sigma;   // Prior degrees of freedom for sigma cor mat\n \n}\ntransformed data{ \n  \n  vector[n_obs] dv_obs = dv[i_obs]; \n  array[n_obs] int dv_obs_id = ID[i_obs];\n  \n  int n_random = 4;     // Number of random effects\n  int n_cmt = 3;        // Number of compartments in the model (includes depot)\n  \n  array[n_random] real scale_omega = {scale_omega_cl, scale_omega_vc, \n                                      scale_omega_q, scale_omega_vp}; \n                                      \n  array[2] real scale_sigma = {scale_sigma_p, scale_sigma_a}; \n  \n}\nparameters{  \n  \n  real<lower = 0> TVCL;       \n  real<lower = 0> TVVC; \n  real<lower = 0> TVQ;\n  real<lower = 0> TVVP;\n  \n  vector<lower = 0>[n_random] omega;\n  cholesky_factor_corr[n_random] L;\n  \n  vector<lower = 0>[2] sigma;\n  cholesky_factor_corr[2] L_Sigma;\n  \n  matrix[n_random, n_subjects] Z;\n  \n}\ntransformed parameters{\n  \n  vector[n_subjects] eta_cl;\n  vector[n_subjects] eta_vc;\n  vector[n_subjects] eta_q;\n  vector[n_subjects] eta_vp;\n  \n  vector[n_subjects] CL;\n  vector[n_subjects] VC;\n  vector[n_subjects] Q;\n  vector[n_subjects] VP;\n\n  vector[n_obs] ipred;\n  \n  matrix[2, 2] R_Sigma;\n  matrix[2, 2] Sigma;\n\n  {\n    row_vector[n_random] typical_values = \n      to_row_vector({TVCL, TVVC, TVQ, TVVP});\n\n    matrix[n_subjects, n_random] eta = diag_pre_multiply(omega, L * Z)';\n    \n    matrix[n_subjects, n_random] theta =\n                          (rep_matrix(typical_values, n_subjects) .* exp(eta));\n                          \n    R_Sigma = multiply_lower_tri_self_transpose(L_Sigma);\n    Sigma = quad_form_diag(R_Sigma, sigma);\n\n    vector[n_total] dv_ipred;\n    matrix[n_total, n_cmt] x_ipred;\n\n    eta_cl = col(eta, 1);\n    eta_vc = col(eta, 2);\n    eta_q = col(eta, 3);\n    eta_vp = col(eta, 4);\n\n    CL = col(theta, 1);\n    VC = col(theta, 2);\n    Q = col(theta, 3);\n    VP = col(theta, 4);\n\n    for(j in 1:n_subjects){\n\n      array[n_random + 1] real theta_params = {CL[j], Q[j], VC[j], VP[j], 0}; // KA = 0. Skip absorption\n      \n      x_ipred[subj_start[j]:subj_end[j],] =\n        pmx_solve_twocpt(time[subj_start[j]:subj_end[j]],\n                         amt[subj_start[j]:subj_end[j]],\n                         rate[subj_start[j]:subj_end[j]],\n                         ii[subj_start[j]:subj_end[j]],\n                         evid[subj_start[j]:subj_end[j]],\n                         cmt[subj_start[j]:subj_end[j]],\n                         addl[subj_start[j]:subj_end[j]],\n                         ss[subj_start[j]:subj_end[j]],\n                         theta_params)';\n                      \n                      \n      dv_ipred[subj_start[j]:subj_end[j]] = \n        x_ipred[subj_start[j]:subj_end[j], 2] ./ VC[j];\n\n    }\n\n    ipred = dv_ipred[i_obs];\n\n  }\n  \n}\n\nmodel{ \n  \n  // Priors\n  TVCL ~ cauchy(0, scale_tvcl);\n  TVVC ~ cauchy(0, scale_tvvc);\n  TVQ ~ cauchy(0, scale_tvq);\n  TVVP ~ cauchy(0, scale_tvvp);\n\n  omega ~ normal(0, scale_omega);\n  L ~ lkj_corr_cholesky(lkj_df_omega);\n  \n  sigma ~ normal(0, scale_sigma);\n  L_Sigma ~ lkj_corr_cholesky(lkj_df_sigma);\n  \n  to_vector(Z) ~ std_normal();\n  \n  // Likelihood\n  for(i in 1:n_obs){\n    dv_obs[i] ~ normal(ipred[i], sqrt(square(ipred[i]) * Sigma[1, 1] + \n                                             Sigma[2, 2] + \n                                             2*ipred[i]*Sigma[2, 1]));\n  }\n\n}\ngenerated quantities{\n\n  real<lower = 0> sigma_p = sigma[1];\n  real<lower = 0> sigma_a = sigma[2];\n  \n  real<lower = 0> sigma_sq_p = square(sigma_p);\n  real<lower = 0> sigma_sq_a = square(sigma_a); \n\n  real<lower = 0> omega_cl = omega[1];\n  real<lower = 0> omega_vc = omega[2];\n  real<lower = 0> omega_q = omega[3];\n  real<lower = 0> omega_vp = omega[4];\n\n  real<lower = 0> omega_sq_cl = square(omega_cl);\n  real<lower = 0> omega_sq_vc = square(omega_vc);\n  real<lower = 0> omega_sq_q = square(omega_q);\n  real<lower = 0> omega_sq_vp = square(omega_vp);\n\n  real cor_cl_vc;\n  real cor_cl_q;\n  real cor_cl_vp;\n  real cor_vc_q;\n  real cor_vc_vp;\n  real cor_q_vp;\n  \n  real omega_cl_vc;\n  real omega_cl_q;\n  real omega_cl_vp;\n  real omega_vc_q;\n  real omega_vc_vp;\n  real omega_q_vp;\n\n  real cor_p_a;\n  real sigma_p_a;\n  \n  vector[n_obs] pred;\n  vector[n_obs] dv_ppc;\n  vector[n_obs] log_lik;\n  vector[n_obs] res;\n  vector[n_obs] wres;\n  vector[n_obs] ires;\n  vector[n_obs] iwres;\n\n\n  {\n\n    matrix[n_random, n_random] R = multiply_lower_tri_self_transpose(L);\n    matrix[n_random, n_random] Omega = quad_form_diag(R, omega);\n\n    vector[n_total] dv_pred;\n    matrix[n_total, n_cmt] x_pred;\n\n    array[n_random + 1] real theta_params_tv = {TVCL, TVQ, TVVC, TVVP, 0}; // KA = 0. Skip absorption\n\n    cor_cl_vc = R[1, 2];\n    cor_cl_q = R[1, 3];\n    cor_cl_vp = R[1, 4];\n    cor_vc_q = R[2, 3];\n    cor_vc_vp = R[2, 4];\n    cor_q_vp = R[3, 4];\n    \n    omega_cl_vc = Omega[1, 2];\n    omega_cl_q = Omega[1, 3];\n    omega_cl_vp = Omega[1, 4];\n    omega_vc_q = Omega[2, 3];\n    omega_vc_vp = Omega[2, 4];\n    omega_q_vp = Omega[3, 4];\n    \n    cor_p_a = R_Sigma[2, 1];\n    sigma_p_a = Sigma[2, 1];\n\n    for(j in 1:n_subjects){\n\n      x_pred[subj_start[j]:subj_end[j],] =\n        pmx_solve_twocpt(time[subj_start[j]:subj_end[j]],\n                         amt[subj_start[j]:subj_end[j]],\n                         rate[subj_start[j]:subj_end[j]],\n                         ii[subj_start[j]:subj_end[j]],\n                         evid[subj_start[j]:subj_end[j]],\n                         cmt[subj_start[j]:subj_end[j]],\n                         addl[subj_start[j]:subj_end[j]],\n                         ss[subj_start[j]:subj_end[j]],\n                         theta_params_tv)';\n\n      dv_pred[subj_start[j]:subj_end[j]] = \n        x_pred[subj_start[j]:subj_end[j], 2] ./ TVVC;\n    }\n\n    pred = dv_pred[i_obs];\n\n  }\n\n  res = dv_obs - pred;                                                                             \n  ires = dv_obs - ipred;\n\n  for(i in 1:n_obs){\n    real ipred_tmp = ipred[i];\n    real sigma_tmp = sqrt(square(ipred_tmp) * sigma_sq_p + sigma_sq_a + \n                          2*ipred_tmp*sigma_p_a);\n    dv_ppc[i] = normal_rng(ipred_tmp, sigma_tmp);\n    log_lik[i] = normal_lpdf(dv_obs[i] | ipred_tmp, sigma_tmp);\n    wres[i] = res[i]/sigma_tmp;\n    iwres[i] = ires[i]/sigma_tmp;\n  }\n\n}\n\n\nBefore fitting the data to the model, there is a small amount of pre-processing to prepare the data to be inserted into Stan but they are basic data-manipulations, and with Torsten, we can simply input the columns of the NONMEM data set:\n\n\nCode\nnonmem_data <- read_csv(\"Data/iv_2cmt_no_bloq.csv\",\n                        na = \".\") %>%\n  rename_all(tolower) %>%\n  rename(ID = \"id\",\n         DV = \"dv\") %>%\n  mutate(DV = if_else(is.na(DV), 5555555, DV)) # This value can be anything except NA. It'll be indexed away\n\nn_subjects <- nonmem_data %>%  # number of individuals\n  distinct(ID) %>%\n  count() %>%\n  deframe()\n\nn_total <- nrow(nonmem_data)   # total number of records\n\ni_obs <- nonmem_data %>%       # index for observation records\n  mutate(row_num = 1:n()) %>%\n  filter(evid == 0) %>%\n  select(row_num) %>%\n  deframe()\n\nn_obs <- length(i_obs)         # number of observation records\n\nsubj_start <- nonmem_data %>%\n  mutate(row_num = 1:n()) %>%\n  group_by(ID) %>%\n  slice_head(n = 1) %>%\n  ungroup() %>%\n  select(row_num) %>%\n  deframe()\n\nsubj_end <- c(subj_start[-1] - 1, n_total)\n\nstan_data <- list(n_subjects = n_subjects,\n                  n_total = n_total,\n                  n_obs = n_obs,\n                  i_obs = i_obs,\n                  ID = nonmem_data$ID,\n                  amt = nonmem_data$amt,\n                  cmt = nonmem_data$cmt,\n                  evid = nonmem_data$evid,\n                  rate = nonmem_data$rate,\n                  ii = nonmem_data$ii,\n                  addl = nonmem_data$addl,\n                  ss = nonmem_data$ss,\n                  time = nonmem_data$time,\n                  dv = nonmem_data$DV,\n                  subj_start = subj_start,\n                  subj_end = subj_end,\n                  scale_tvcl = 2,\n                  scale_tvvc = 10,\n                  scale_tvq = 2,\n                  scale_tvvp = 10,\n                  scale_omega_cl = 0.4,\n                  scale_omega_vc = 0.4,\n                  scale_omega_q = 0.4,\n                  scale_omega_vp = 0.4,\n                  lkj_df_omega = 2,\n                  scale_sigma_p = 0.5,\n                  scale_sigma_a = 1,\n                  lkj_df_sigma = 2)\n\n\nWe can then fit the data to this model:\n\n\nCode\nfit1 <- model1$sample(data = stan_data,\n                      seed = 112358,\n                      chains = 4,\n                      parallel_chains = 4,\n                      iter_warmup = 500,\n                      iter_sampling = 1000,\n                      adapt_delta = 0.8,\n                      refresh = 500,\n                      max_treedepth = 10,\n                      init = function() list(TVCL = rlnorm(1, log(0.3), 0.3),\n                                             TVVC = rlnorm(1, log(6), 0.3),\n                                             TVQ = rlnorm(1, log(1), 0.3),\n                                             TVVP = rlnorm(1, log(3), 0.3),\n                                             omega = rlnorm(5, log(0.3), 0.3),\n                                             sigma = rlnorm(2, log(0.5), 0.3)))\n\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 1500 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 1500 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 1500 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 1500 [  0%]  (Warmup) \nChain 1 Iteration:  500 / 1500 [ 33%]  (Warmup) \nChain 1 Iteration:  501 / 1500 [ 33%]  (Sampling) \nChain 3 Iteration:  500 / 1500 [ 33%]  (Warmup) \nChain 3 Iteration:  501 / 1500 [ 33%]  (Sampling) \nChain 2 Iteration:  500 / 1500 [ 33%]  (Warmup) \nChain 4 Iteration:  500 / 1500 [ 33%]  (Warmup) \nChain 2 Iteration:  501 / 1500 [ 33%]  (Sampling) \nChain 4 Iteration:  501 / 1500 [ 33%]  (Sampling) \nChain 1 Iteration: 1000 / 1500 [ 66%]  (Sampling) \nChain 3 Iteration: 1000 / 1500 [ 66%]  (Sampling) \nChain 2 Iteration: 1000 / 1500 [ 66%]  (Sampling) \nChain 4 Iteration: 1000 / 1500 [ 66%]  (Sampling) \nChain 1 Iteration: 1500 / 1500 [100%]  (Sampling) \nChain 1 finished in 910.5 seconds.\nChain 3 Iteration: 1500 / 1500 [100%]  (Sampling) \nChain 3 finished in 921.0 seconds.\nChain 2 Iteration: 1500 / 1500 [100%]  (Sampling) \nChain 2 finished in 952.5 seconds.\nChain 4 Iteration: 1500 / 1500 [100%]  (Sampling) \nChain 4 finished in 953.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 934.3 seconds.\nTotal execution time: 954.0 seconds.\n\n\n\n\n3.3.5.2 Markov Chain Monte Carlo Aside\nHistorically we perform MCMC in a completely sequential manner:\n\n\n\n\n\n\nThis is what we did in the single-individual example in Section 3.2.3. However, Stan (along with many other softwares) can automatically sample the chains in parallel (as we did with the example just above in Section Section 3.3.5.1):\n\n\n\n\n\n\nWithin each iteration, we calculate the posterior density (up to a constant): \\[ \\pi(\\mathbf{\\theta \\, | \\, \\mathbf{y}}) \\propto \\mathcal{L}(\\mathbf{\\theta \\, | \\, \\mathbf{y}})\\,p(\\mathbf{\\theta})\\] where \\(\\mathcal{L(\\cdot\\,|\\,\\cdot)}\\) is the likelihood and \\(p(\\cdot)\\) is the prior. For us, we generally have \\(n_{subj}\\) independent subjects, and so we can calculate the likelihood for each individual separately from the others11. Typically, the likelihood for each subject is calculated sequentially, but Stan has multiple methods for parallelization. The reduce_sum function implemented in most of the models later in this presentation12 uses multi-threading that allows the individual likelihoods to be calculated in parallel rather than sequentially:\n\n\n\n\n\n\nThis within-chain parallelization can lead to substantial speed-ups in computation time13.\n\n\n3.3.5.3 A Model with Within-Chain Parallelization with reduce_sum\nThis next model is a bit harder to read, but it is threaded so that it can make use of the available computing power to implement within-chain parallelization. In practice, this means calculating the (log-)likelihood for each individual in parallel and then bringing them back together.\n\nHalf-Cauchy PriorsLognormal Priors\n\n\n\n\nCode\nmodel2 <- cmdstan_model(\"Torsten/Fit/iv_2cmt_ppa.stan\",\n                        cpp_options = list(stan_threads = TRUE))\n\nmodel2$print()\n\n\n// IV Infusion\n// Two-compartment PK Model\n// IIV on CL, VC, Q, VP (full covariance matrix)\n// proportional plus additive error - DV = CP(1 + eps_p) + eps_a\n// Closed form solution using Torsten\n// Implements threading for within-chain parallelization \n\nfunctions{\n\n  array[] int sequence(int start, int end) { \n    array[end - start + 1] int seq;\n    for (n in 1:num_elements(seq)) {\n      seq[n] = n + start - 1;\n    }\n    return seq; \n  } \n  \n  int num_between(int lb, int ub, array[] int y){\n    \n    int n = 0;\n    for(i in 1:num_elements(y)){\n      if(y[i] >= lb && y[i] <= ub)\n                    n = n + 1;\n    }\n    return n;\n    \n  }\n  \n  array[] int find_between(int lb, int ub, array[] int y) {\n    // vector[num_between(lb, ub, y)] result;\n    array[num_between(lb, ub, y)] int result;\n    int n = 1;\n    for (i in 1:num_elements(y)) {\n      if (y[i] >= lb && y[i] <= ub) {\n        result[n] = y[i];\n        n = n + 1;\n      }\n    }\n    return result;\n  }\n  \n  vector find_between_vec(int lb, int ub, array[] int idx, vector y) {\n    \n    vector[num_between(lb, ub, idx)] result;\n    int n = 1;\n    if(num_elements(idx) != num_elements(y)) reject(\"illegal input\");\n    for (i in 1:rows(y)) {\n      if (idx[i] >= lb && idx[i] <= ub) {\n        result[n] = y[i];\n        n = n + 1;\n      }\n    }\n    return result;\n  }\n  \n  real partial_sum_lpmf(array[] int seq_subj, int start, int end,\n                        vector dv_obs, array[] int dv_obs_id, array[] int i_obs,\n                        array[] real amt, array[] int cmt, array[] int evid, \n                        array[] real time, array[] real rate, array[] real ii, \n                        array[] int addl, array[] int ss,\n                        array[] int subj_start, array[] int subj_end, \n                        real TVCL, real TVVC, real TVQ, real TVVP, \n                        vector omega, matrix L, matrix Z, \n                        vector sigma, matrix L_Sigma,\n                        int n_random, int n_subjects, int n_total){\n                           \n    real ptarget = 0;\n    row_vector[n_random] typical_values = to_row_vector({TVCL, TVVC, TVQ, TVVP});\n    \n    matrix[n_subjects, n_random] eta = diag_pre_multiply(omega, L * Z)';\n\n    matrix[n_subjects, n_random] theta =\n                          (rep_matrix(typical_values, n_subjects) .* exp(eta));\n                          \n    matrix[2, 2] R_Sigma = multiply_lower_tri_self_transpose(L_Sigma);\n    matrix[2, 2] Sigma = quad_form_diag(R_Sigma, sigma);\n    \n                              \n    int N = end - start + 1;    // number of subjects in this slice  \n    vector[n_total] dv_ipred;   \n    matrix[n_total, 3] x_ipred; \n  \n\n    int n_obs_slice = num_between(subj_start[start], subj_end[end], i_obs);\n    array[n_obs_slice] int i_obs_slice = find_between(subj_start[start], \n                                                      subj_end[end], i_obs);\n                                                \n    vector[n_obs_slice] dv_obs_slice = find_between_vec(start, end, \n                                                        dv_obs_id, dv_obs);\n    \n    vector[n_obs_slice] ipred_slice;\n    \n    \n    for(n in 1:N){            // loop over subjects in this slice\n    \n      int nn = n + start - 1; // nn is the ID of the current subject\n      \n      row_vector[n_random] theta_nn = theta[nn]; // access the parameters for subject nn\n      real cl = theta_nn[1];\n      real vc = theta_nn[2];\n      real q = theta_nn[3];\n      real vp = theta_nn[4];\n      array[5] real theta_params = {cl, q, vc, vp, 0}; // The 0 is for ka. Skip the absorption\n      \n      x_ipred[subj_start[nn]:subj_end[nn], ] =\n        pmx_solve_twocpt(time[subj_start[nn]:subj_end[nn]],\n                         amt[subj_start[nn]:subj_end[nn]],\n                         rate[subj_start[nn]:subj_end[nn]],\n                         ii[subj_start[nn]:subj_end[nn]],\n                         evid[subj_start[nn]:subj_end[nn]],\n                         cmt[subj_start[nn]:subj_end[nn]],\n                         addl[subj_start[nn]:subj_end[nn]],\n                         ss[subj_start[nn]:subj_end[nn]],\n                         theta_params)';\n                      \n      dv_ipred[subj_start[nn]:subj_end[nn]] = \n        x_ipred[subj_start[nn]:subj_end[nn], 2] ./ vc; \n    \n    }\n  \n    \n    ipred_slice = dv_ipred[i_obs_slice];\n    \n    ptarget += normal_lpdf(dv_obs_slice | ipred_slice, \n                                        sqrt(square(ipred_slice) * Sigma[1, 1] +\n                                             Sigma[2, 2] + \n                                             2 * ipred_slice * Sigma[2, 1]));\n    return ptarget;\n                           \n  }\n  \n}\ndata{\n  \n  int n_subjects;\n  int n_total;\n  int n_obs;\n  array[n_obs] int i_obs;\n  array[n_total] int ID;\n  array[n_total] real amt;\n  array[n_total] int cmt;\n  array[n_total] int evid;\n  array[n_total] real rate;\n  array[n_total] real ii;\n  array[n_total] int addl;\n  array[n_total] int ss;\n  array[n_total] real time;\n  vector[n_total] dv;\n  array[n_subjects] int subj_start;\n  array[n_subjects] int subj_end;\n  \n  real<lower = 0> scale_tvcl;      // Prior Scale parameter for CL\n  real<lower = 0> scale_tvvc;      // Prior Scale parameter for VC\n  real<lower = 0> scale_tvq;       // Prior Scale parameter for Q\n  real<lower = 0> scale_tvvp;      // Prior Scale parameter for VP\n  \n  real<lower = 0> scale_omega_cl; // Prior scale parameter for omega_cl\n  real<lower = 0> scale_omega_vc; // Prior scale parameter for omega_vc\n  real<lower = 0> scale_omega_q;  // Prior scale parameter for omega_q\n  real<lower = 0> scale_omega_vp; // Prior scale parameter for omega_vp\n  \n  real<lower = 0> lkj_df_omega;   // Prior degrees of freedom for omega cor mat\n  \n  real<lower = 0> scale_sigma_p;  // Prior Scale parameter for proportional error\n  real<lower = 0> scale_sigma_a;  // Prior Scale parameter for additive error\n  real<lower = 0> lkj_df_sigma;   // Prior degrees of freedom for sigma cor mat\n \n}\ntransformed data{ \n  \n  int grainsize = 1;\n  \n  vector[n_obs] dv_obs = dv[i_obs];\n  array[n_obs] int dv_obs_id = ID[i_obs];\n  \n  int n_random = 4;                    // Number of random effects\n  \n  array[n_random] real scale_omega = {scale_omega_cl, scale_omega_vc, \n                                      scale_omega_q, scale_omega_vp}; \n  array[2] real scale_sigma = {scale_sigma_p, scale_sigma_a};\n  \n  array[n_subjects] int seq_subj = sequence(1, n_subjects); // reduce_sum over subjects\n  \n}\nparameters{ \n  \n  real<lower = 0> TVCL;       \n  real<lower = 0> TVVC; \n  real<lower = 0> TVQ;\n  real<lower = 0> TVVP; \n  \n  vector<lower = 0>[n_random] omega;\n  cholesky_factor_corr[n_random] L;\n  \n  vector<lower = 0>[2] sigma;\n  cholesky_factor_corr[2] L_Sigma;\n  \n  matrix[n_random, n_subjects] Z;\n  \n}\n\nmodel{ \n  \n  // Priors\n  TVCL ~ cauchy(0, scale_tvcl);\n  TVVC ~ cauchy(0, scale_tvvc);\n  TVQ ~ cauchy(0, scale_tvq);\n  TVVP ~ cauchy(0, scale_tvvp);\n\n  omega ~ normal(0, scale_omega);\n  L ~ lkj_corr_cholesky(lkj_df_omega);\n  \n  sigma ~ normal(0, scale_sigma);\n  L_Sigma ~ lkj_corr_cholesky(lkj_df_sigma);\n  \n  to_vector(Z) ~ std_normal();\n  \n  // Likelihood\n  target += reduce_sum(partial_sum_lupmf, seq_subj, grainsize,\n                       dv_obs, dv_obs_id, i_obs,\n                       amt, cmt, evid, time, \n                       rate, ii, addl, ss, subj_start, subj_end, \n                       TVCL, TVVC, TVQ, TVVP, omega, L, Z,\n                       sigma, L_Sigma, \n                       n_random, n_subjects, n_total);\n}\ngenerated quantities{\n  \n  real<lower = 0> sigma_p = sigma[1];\n  real<lower = 0> sigma_a = sigma[2];\n  \n  real<lower = 0> sigma_sq_p = square(sigma_p);\n  real<lower = 0> sigma_sq_a = square(sigma_a); \n\n  real<lower = 0> omega_cl = omega[1];\n  real<lower = 0> omega_vc = omega[2];\n  real<lower = 0> omega_q = omega[3];\n  real<lower = 0> omega_vp = omega[4];\n\n  real<lower = 0> omega_sq_cl = square(omega_cl);\n  real<lower = 0> omega_sq_vc = square(omega_vc);\n  real<lower = 0> omega_sq_q = square(omega_q);\n  real<lower = 0> omega_sq_vp = square(omega_vp);\n\n  real cor_cl_vc;\n  real cor_cl_q;\n  real cor_cl_vp;\n  real cor_vc_q;\n  real cor_vc_vp;\n  real cor_q_vp;\n  real omega_cl_vc;\n  real omega_cl_q;\n  real omega_cl_vp;\n  real omega_vc_q;\n  real omega_vc_vp;\n  real omega_q_vp;\n\n  real cor_p_a;\n  real sigma_p_a;\n\n  vector[n_subjects] eta_cl;\n  vector[n_subjects] eta_vc;\n  vector[n_subjects] eta_q;\n  vector[n_subjects] eta_vp;\n  vector[n_subjects] CL;\n  vector[n_subjects] VC;\n  vector[n_subjects] Q;\n  vector[n_subjects] VP;\n  \n  vector[n_obs] ipred;\n  vector[n_obs] pred;\n  vector[n_obs] dv_ppc;\n  vector[n_obs] log_lik;\n  vector[n_obs] res;\n  vector[n_obs] wres;\n  vector[n_obs] ires;\n  vector[n_obs] iwres;\n \n  {\n    row_vector[n_random] typical_values = to_row_vector({TVCL, TVVC, TVQ, TVVP});\n\n    matrix[n_random, n_random] R = multiply_lower_tri_self_transpose(L);\n    matrix[n_random, n_random] Omega = quad_form_diag(R, omega);\n\n    matrix[2, 2] R_Sigma = multiply_lower_tri_self_transpose(L_Sigma);\n    matrix[2, 2] Sigma = quad_form_diag(R_Sigma, sigma);\n\n    matrix[n_subjects, n_random] eta = diag_pre_multiply(omega, L * Z)';\n\n    matrix[n_subjects, n_random] theta =\n                          (rep_matrix(typical_values, n_subjects) .* exp(eta));\n\n    vector[n_total] dv_pred;\n    matrix[n_total, 3] x_pred;\n    vector[n_total] dv_ipred;\n    matrix[n_total, 3] x_ipred;\n    \n    array[5] real theta_params_tv = {TVCL, TVQ, TVVC, TVVP, 0}; // 0 is for KA to skip absorption\n    \n    eta_cl = col(eta, 1);\n    eta_vc = col(eta, 2);\n    eta_q = col(eta, 3);\n    eta_vp = col(eta, 4);\n    \n    CL = col(theta, 1);\n    VC = col(theta, 2);\n    Q = col(theta, 3);\n    VP = col(theta, 4);\n\n    cor_cl_vc = R[1, 2];\n    cor_cl_q = R[1, 3];\n    cor_cl_vp = R[1, 4];\n    cor_vc_q = R[2, 3];\n    cor_vc_vp = R[2, 4];\n    cor_q_vp = R[3, 4];\n    omega_cl_vc = Omega[1, 2];\n    omega_cl_q = Omega[1, 3];\n    omega_cl_vp = Omega[1, 4];\n    omega_vc_q = Omega[2, 3];\n    omega_vc_vp = Omega[2, 4];\n    omega_q_vp = Omega[3, 4];\n\n    cor_p_a = R_Sigma[2, 1];\n    sigma_p_a = Sigma[2, 1];\n\n    for(j in 1:n_subjects){\n\n      array[5] real theta_params = {CL[j], Q[j], VC[j], VP[j], 0};\n      \n      x_ipred[subj_start[j]:subj_end[j],] =\n        pmx_solve_twocpt(time[subj_start[j]:subj_end[j]],\n                         amt[subj_start[j]:subj_end[j]],\n                         rate[subj_start[j]:subj_end[j]],\n                         ii[subj_start[j]:subj_end[j]],\n                         evid[subj_start[j]:subj_end[j]],\n                         cmt[subj_start[j]:subj_end[j]],\n                         addl[subj_start[j]:subj_end[j]],\n                         ss[subj_start[j]:subj_end[j]],\n                         theta_params)';\n                      \n                      \n      dv_ipred[subj_start[j]:subj_end[j]] = \n        x_ipred[subj_start[j]:subj_end[j], 2] ./ VC[j];\n\n      x_pred[subj_start[j]:subj_end[j],] =\n        pmx_solve_twocpt(time[subj_start[j]:subj_end[j]],\n                         amt[subj_start[j]:subj_end[j]],\n                         rate[subj_start[j]:subj_end[j]],\n                         ii[subj_start[j]:subj_end[j]],\n                         evid[subj_start[j]:subj_end[j]],\n                         cmt[subj_start[j]:subj_end[j]],\n                         addl[subj_start[j]:subj_end[j]],\n                         ss[subj_start[j]:subj_end[j]],\n                         theta_params_tv)';\n\n      dv_pred[subj_start[j]:subj_end[j]] = \n        x_pred[subj_start[j]:subj_end[j], 2] ./ TVVC;\n    }\n\n    pred = dv_pred[i_obs];\n    ipred = dv_ipred[i_obs];\n\n  }\n\n  res = dv_obs - pred;                                                                             \n  ires = dv_obs - ipred;\n\n  for(i in 1:n_obs){\n    real ipred_tmp = ipred[i];\n    real sigma_tmp = sqrt(square(ipred_tmp) * sigma_sq_p + sigma_sq_a + \n                          2*ipred_tmp*sigma_p_a);\n    dv_ppc[i] = normal_rng(ipred_tmp, sigma_tmp);\n    log_lik[i] = normal_lpdf(dv_obs[i] | ipred_tmp, sigma_tmp);\n    wres[i] = res[i]/sigma_tmp;\n    iwres[i] = ires[i]/sigma_tmp;\n  }\n  \n}\n\n\nWe can then fit the data to this model. A guide for choosing the chains, parallel_chains, and threads_per_chain arguments is to\n\nFigure out how many cores you have available (parallel::detectCores())\nChoose the number of chains you want to sample (we recommend 4)\nIf chains < parallel::detectCores(), then have parallel_chains = chains (almost all modern machines have at least 4 cores)\nthreads_per_chain should be anywhere between 1 and \\(\\frac{parallel::detectCores()}{parallel\\_chains}\\)\n\nFor example, if your machine has 32 cores, we recommend having 4 chains, 4 parallel_chains, and 8 threads_per_chain. This will make use of all the available cores. Using more threads_per_chain won’t be helpful in reducing execution time, since the available cores are already in use.\n\n\nCode\nfit <- model2$sample(data = stan_data,\n                     seed = 112358,\n                     chains = 4,\n                     parallel_chains = 4,\n                     threads_per_chain = 4,\n                     iter_warmup = 500,\n                     iter_sampling = 1000,\n                     adapt_delta = 0.8,\n                     refresh = 500,\n                     max_treedepth = 10,\n                     init = function() list(TVCL = rlnorm(1, log(0.3), 0.3),\n                                            TVVC = rlnorm(1, log(6), 0.3),\n                                            TVQ = rlnorm(1, log(1), 0.3),\n                                            TVVP = rlnorm(1, log(3), 0.3),\n                                            omega = rlnorm(5, log(0.3), 0.3),\n                                            sigma = rlnorm(2, log(0.5), 0.3)))\n\n\nRunning MCMC with 4 parallel chains, with 4 thread(s) per chain...\n\nChain 1 Iteration:    1 / 1500 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 1500 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 1500 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 1500 [  0%]  (Warmup) \nChain 3 Iteration:  500 / 1500 [ 33%]  (Warmup) \nChain 3 Iteration:  501 / 1500 [ 33%]  (Sampling) \nChain 1 Iteration:  500 / 1500 [ 33%]  (Warmup) \nChain 1 Iteration:  501 / 1500 [ 33%]  (Sampling) \nChain 2 Iteration:  500 / 1500 [ 33%]  (Warmup) \nChain 2 Iteration:  501 / 1500 [ 33%]  (Sampling) \nChain 4 Iteration:  500 / 1500 [ 33%]  (Warmup) \nChain 4 Iteration:  501 / 1500 [ 33%]  (Sampling) \nChain 1 Iteration: 1000 / 1500 [ 66%]  (Sampling) \nChain 3 Iteration: 1000 / 1500 [ 66%]  (Sampling) \nChain 2 Iteration: 1000 / 1500 [ 66%]  (Sampling) \nChain 4 Iteration: 1000 / 1500 [ 66%]  (Sampling) \nChain 1 Iteration: 1500 / 1500 [100%]  (Sampling) \nChain 1 finished in 4838.9 seconds.\nChain 2 Iteration: 1500 / 1500 [100%]  (Sampling) \nChain 2 finished in 5005.6 seconds.\nChain 3 Iteration: 1500 / 1500 [100%]  (Sampling) \nChain 3 finished in 5006.8 seconds.\nChain 4 Iteration: 1500 / 1500 [100%]  (Sampling) \nChain 4 finished in 5071.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 4980.6 seconds.\nTotal execution time: 5079.1 seconds.\n\n\n\n\n\n\nCode\nmodel3 <- cmdstan_model(\"Torsten/Fit/iv_2cmt_ppa_lognormal_priors.stan\",\n                        cpp_options = list(stan_threads = TRUE))\n\nmodel3$print()\n\n\n// IV Infusion\n// Two-compartment PK Model\n// IIV on CL, VC, Q, VP (full covariance matrix)\n// proportional plus additive error - DV = CP(1 + eps_p) + eps_a\n// Closed form solution using Torsten\n// Implements threading for within-chain parallelization \n\nfunctions{\n\n  array[] int sequence(int start, int end) { \n    array[end - start + 1] int seq;\n    for (n in 1:num_elements(seq)) {\n      seq[n] = n + start - 1;\n    }\n    return seq; \n  } \n  \n  int num_between(int lb, int ub, array[] int y){\n    \n    int n = 0;\n    for(i in 1:num_elements(y)){\n      if(y[i] >= lb && y[i] <= ub)\n                    n = n + 1;\n    }\n    return n;\n    \n  }\n  \n  array[] int find_between(int lb, int ub, array[] int y) {\n    // vector[num_between(lb, ub, y)] result;\n    array[num_between(lb, ub, y)] int result;\n    int n = 1;\n    for (i in 1:num_elements(y)) {\n      if (y[i] >= lb && y[i] <= ub) {\n        result[n] = y[i];\n        n = n + 1;\n      }\n    }\n    return result;\n  }\n  \n  vector find_between_vec(int lb, int ub, array[] int idx, vector y) {\n    \n    vector[num_between(lb, ub, idx)] result;\n    int n = 1;\n    if(num_elements(idx) != num_elements(y)) reject(\"illegal input\");\n    for (i in 1:rows(y)) {\n      if (idx[i] >= lb && idx[i] <= ub) {\n        result[n] = y[i];\n        n = n + 1;\n      }\n    }\n    return result;\n  }\n  \n  real partial_sum_lpmf(array[] int seq_subj, int start, int end,\n                        vector dv_obs, array[] int dv_obs_id, array[] int i_obs,\n                        array[] real amt, array[] int cmt, array[] int evid, \n                        array[] real time, array[] real rate, array[] real ii, \n                        array[] int addl, array[] int ss,\n                        array[] int subj_start, array[] int subj_end, \n                        real TVCL, real TVVC, real TVQ, real TVVP, \n                        vector omega, matrix L, matrix Z, \n                        vector sigma, matrix L_Sigma,\n                        int n_random, int n_subjects, int n_total){\n                           \n    real ptarget = 0;\n    row_vector[n_random] typical_values = to_row_vector({TVCL, TVVC, TVQ, TVVP});\n    \n    matrix[n_subjects, n_random] eta = diag_pre_multiply(omega, L * Z)';\n\n    matrix[n_subjects, n_random] theta =\n                          (rep_matrix(typical_values, n_subjects) .* exp(eta));\n                          \n    matrix[2, 2] R_Sigma = multiply_lower_tri_self_transpose(L_Sigma);\n    matrix[2, 2] Sigma = quad_form_diag(R_Sigma, sigma);\n    \n                              \n    int N = end - start + 1;    // number of subjects in this slice  \n    vector[n_total] dv_ipred;   // = rep_vector(0, n_total);\n    matrix[n_total, 3] x_ipred; // = rep_matrix(0, n_total, 2);\n  \n\n    int n_obs_slice = num_between(subj_start[start], subj_end[end], i_obs);\n    array[n_obs_slice] int i_obs_slice = find_between(subj_start[start], \n                                                      subj_end[end], i_obs);\n                                                \n    vector[n_obs_slice] dv_obs_slice = find_between_vec(start, end, \n                                                        dv_obs_id, dv_obs);\n    \n    vector[n_obs_slice] ipred_slice;\n    \n    \n    for(n in 1:N){            // loop over subjects in this slice\n    \n      int nn = n + start - 1; // nn is the ID of the current subject\n      \n      row_vector[n_random] theta_nn = theta[nn]; // access the parameters for subject nn\n      real cl = theta_nn[1];\n      real vc = theta_nn[2];\n      real q = theta_nn[3];\n      real vp = theta_nn[4];\n      array[5] real theta_params = {cl, q, vc, vp, 0}; // The 0 is for KA. Skip the absorption\n      \n      x_ipred[subj_start[nn]:subj_end[nn], ] =\n        pmx_solve_twocpt(time[subj_start[nn]:subj_end[nn]],\n                         amt[subj_start[nn]:subj_end[nn]],\n                         rate[subj_start[nn]:subj_end[nn]],\n                         ii[subj_start[nn]:subj_end[nn]],\n                         evid[subj_start[nn]:subj_end[nn]],\n                         cmt[subj_start[nn]:subj_end[nn]],\n                         addl[subj_start[nn]:subj_end[nn]],\n                         ss[subj_start[nn]:subj_end[nn]],\n                         theta_params)';\n                      \n      dv_ipred[subj_start[nn]:subj_end[nn]] = \n        x_ipred[subj_start[nn]:subj_end[nn], 2] ./ vc; \n    \n    }\n  \n    \n    ipred_slice = dv_ipred[i_obs_slice];\n    \n    ptarget += normal_lpdf(dv_obs_slice | ipred_slice, \n                                        sqrt(square(ipred_slice) * Sigma[1, 1] +\n                                             Sigma[2, 2] + \n                                             2 * ipred_slice * Sigma[2, 1]));\n    return ptarget;\n                           \n  }\n  \n}\ndata{\n  \n  int n_subjects;\n  int n_total;\n  int n_obs;\n  array[n_obs] int i_obs;\n  array[n_total] int ID;\n  array[n_total] real amt;\n  array[n_total] int cmt;\n  array[n_total] int evid;\n  array[n_total] real rate;\n  array[n_total] real ii;\n  array[n_total] int addl;\n  array[n_total] int ss;\n  array[n_total] real time;\n  vector[n_total] dv;\n  array[n_subjects] int subj_start;\n  array[n_subjects] int subj_end;\n  \n  real<lower = 0> location_tvcl;   // Prior Location parameter for CL\n  real<lower = 0> location_tvvc;   // Prior Location parameter for VC\n  real<lower = 0> location_tvq;    // Prior Location parameter for Q\n  real<lower = 0> location_tvvp;   // Prior Location parameter for VP\n  \n  real<lower = 0> scale_tvcl;      // Prior Scale parameter for CL\n  real<lower = 0> scale_tvvc;      // Prior Scale parameter for VC\n  real<lower = 0> scale_tvq;       // Prior Scale parameter for Q\n  real<lower = 0> scale_tvvp;      // Prior Scale parameter for VP\n  \n  real<lower = 0> scale_omega_cl; // Prior scale parameter for omega_cl\n  real<lower = 0> scale_omega_vc; // Prior scale parameter for omega_vc\n  real<lower = 0> scale_omega_q;  // Prior scale parameter for omega_q\n  real<lower = 0> scale_omega_vp; // Prior scale parameter for omega_vp\n  \n  real<lower = 0> lkj_df_omega;   // Prior degrees of freedom for omega cor mat\n  \n  real<lower = 0> scale_sigma_p;  // Prior Scale parameter for proportional error\n  real<lower = 0> scale_sigma_a;  // Prior Scale parameter for additive error\n  real<lower = 0> lkj_df_sigma;   // Prior degrees of freedom for sigma cor mat\n \n}\ntransformed data{ \n  \n  int grainsize = 1;\n  \n  vector[n_obs] dv_obs = dv[i_obs];\n  array[n_obs] int dv_obs_id = ID[i_obs];\n  \n  int n_random = 4;                    // Number of random effects\n  \n  array[n_random] real scale_omega = {scale_omega_cl, scale_omega_vc, \n                                      scale_omega_q, scale_omega_vp}; \n  array[2] real scale_sigma = {scale_sigma_p, scale_sigma_a};\n  \n  array[n_subjects] int seq_subj = sequence(1, n_subjects); // reduce_sum over subjects\n  \n}\nparameters{ \n  \n  real<lower = 0> TVCL;       \n  real<lower = 0> TVVC; \n  real<lower = 0> TVQ;\n  real<lower = 0> TVVP; \n  \n  vector<lower = 0>[n_random] omega;\n  cholesky_factor_corr[n_random] L;\n  \n  vector<lower = 0>[2] sigma;\n  cholesky_factor_corr[2] L_Sigma;\n  \n  matrix[n_random, n_subjects] Z;\n  \n}\n\nmodel{ \n  \n  // Priors\n  TVCL ~ lognormal(log(location_tvcl), scale_tvcl);\n  TVVC ~ lognormal(log(location_tvvc), scale_tvvc);\n  TVQ ~ lognormal(log(location_tvq), scale_tvq);\n  TVVP ~ lognormal(log(location_tvvp), scale_tvvp);\n\n  omega ~ normal(0, scale_omega);\n  L ~ lkj_corr_cholesky(lkj_df_omega);\n  \n  sigma ~ normal(0, scale_sigma);\n  L_Sigma ~ lkj_corr_cholesky(lkj_df_sigma);\n  \n  to_vector(Z) ~ std_normal();\n  \n  // Likelihood\n  target += reduce_sum(partial_sum_lupmf, seq_subj, grainsize,\n                       dv_obs, dv_obs_id, i_obs,\n                       amt, cmt, evid, time, \n                       rate, ii, addl, ss, subj_start, subj_end, \n                       TVCL, TVVC, TVQ, TVVP, omega, L, Z,\n                       sigma, L_Sigma, \n                       n_random, n_subjects, n_total);\n}\ngenerated quantities{\n  \n  real<lower = 0> sigma_p = sigma[1];\n  real<lower = 0> sigma_a = sigma[2];\n  \n  real<lower = 0> sigma_sq_p = square(sigma_p);\n  real<lower = 0> sigma_sq_a = square(sigma_a); \n\n  real<lower = 0> omega_cl = omega[1];\n  real<lower = 0> omega_vc = omega[2];\n  real<lower = 0> omega_q = omega[3];\n  real<lower = 0> omega_vp = omega[4];\n\n  real<lower = 0> omega_sq_cl = square(omega_cl);\n  real<lower = 0> omega_sq_vc = square(omega_vc);\n  real<lower = 0> omega_sq_q = square(omega_q);\n  real<lower = 0> omega_sq_vp = square(omega_vp);\n\n  real cor_cl_vc;\n  real cor_cl_q;\n  real cor_cl_vp;\n  real cor_vc_q;\n  real cor_vc_vp;\n  real cor_q_vp;\n  real omega_cl_vc;\n  real omega_cl_q;\n  real omega_cl_vp;\n  real omega_vc_q;\n  real omega_vc_vp;\n  real omega_q_vp;\n\n  real cor_p_a;\n  real sigma_p_a;\n\n  vector[n_subjects] eta_cl;\n  vector[n_subjects] eta_vc;\n  vector[n_subjects] eta_q;\n  vector[n_subjects] eta_vp;\n  vector[n_subjects] CL;\n  vector[n_subjects] VC;\n  vector[n_subjects] Q;\n  vector[n_subjects] VP;\n  \n  vector[n_obs] ipred;\n  vector[n_obs] pred;\n  vector[n_obs] dv_ppc;\n  vector[n_obs] log_lik;\n  vector[n_obs] res;\n  vector[n_obs] wres;\n  vector[n_obs] ires;\n  vector[n_obs] iwres;\n \n  {\n    row_vector[n_random] typical_values = to_row_vector({TVCL, TVVC, TVQ, TVVP});\n\n    matrix[n_random, n_random] R = multiply_lower_tri_self_transpose(L);\n    matrix[n_random, n_random] Omega = quad_form_diag(R, omega);\n\n    matrix[2, 2] R_Sigma = multiply_lower_tri_self_transpose(L_Sigma);\n    matrix[2, 2] Sigma = quad_form_diag(R_Sigma, sigma);\n\n    matrix[n_subjects, n_random] eta = diag_pre_multiply(omega, L * Z)';\n\n    matrix[n_subjects, n_random] theta =\n                          (rep_matrix(typical_values, n_subjects) .* exp(eta));\n\n    vector[n_total] dv_pred;\n    matrix[n_total, 3] x_pred;\n    vector[n_total] dv_ipred;\n    matrix[n_total, 3] x_ipred;\n    \n    array[5] real theta_params_tv = {TVCL, TVQ, TVVC, TVVP, 0}; // 0 is for KA to skip absorption\n    \n    eta_cl = col(eta, 1);\n    eta_vc = col(eta, 2);\n    eta_q = col(eta, 3);\n    eta_vp = col(eta, 4);\n    \n    CL = col(theta, 1);\n    VC = col(theta, 2);\n    Q = col(theta, 3);\n    VP = col(theta, 4);\n\n    cor_cl_vc = R[1, 2];\n    cor_cl_q = R[1, 3];\n    cor_cl_vp = R[1, 4];\n    cor_vc_q = R[2, 3];\n    cor_vc_vp = R[2, 4];\n    cor_q_vp = R[3, 4];\n    omega_cl_vc = Omega[1, 2];\n    omega_cl_q = Omega[1, 3];\n    omega_cl_vp = Omega[1, 4];\n    omega_vc_q = Omega[2, 3];\n    omega_vc_vp = Omega[2, 4];\n    omega_q_vp = Omega[3, 4];\n\n    cor_p_a = R_Sigma[2, 1];\n    sigma_p_a = Sigma[2, 1];\n\n    for(j in 1:n_subjects){\n\n      array[5] real theta_params = {CL[j], Q[j], VC[j], VP[j], 0};\n      \n      x_ipred[subj_start[j]:subj_end[j],] =\n        pmx_solve_twocpt(time[subj_start[j]:subj_end[j]],\n                         amt[subj_start[j]:subj_end[j]],\n                         rate[subj_start[j]:subj_end[j]],\n                         ii[subj_start[j]:subj_end[j]],\n                         evid[subj_start[j]:subj_end[j]],\n                         cmt[subj_start[j]:subj_end[j]],\n                         addl[subj_start[j]:subj_end[j]],\n                         ss[subj_start[j]:subj_end[j]],\n                         theta_params)';\n                      \n                      \n      dv_ipred[subj_start[j]:subj_end[j]] = \n        x_ipred[subj_start[j]:subj_end[j], 2] ./ VC[j];\n\n      x_pred[subj_start[j]:subj_end[j],] =\n        pmx_solve_twocpt(time[subj_start[j]:subj_end[j]],\n                         amt[subj_start[j]:subj_end[j]],\n                         rate[subj_start[j]:subj_end[j]],\n                         ii[subj_start[j]:subj_end[j]],\n                         evid[subj_start[j]:subj_end[j]],\n                         cmt[subj_start[j]:subj_end[j]],\n                         addl[subj_start[j]:subj_end[j]],\n                         ss[subj_start[j]:subj_end[j]],\n                         theta_params_tv)';\n\n      dv_pred[subj_start[j]:subj_end[j]] = \n        x_pred[subj_start[j]:subj_end[j], 2] ./ TVVC;\n    }\n\n    pred = dv_pred[i_obs];\n    ipred = dv_ipred[i_obs];\n\n  }\n\n  res = dv_obs - pred;                                                                             \n  ires = dv_obs - ipred;\n\n  for(i in 1:n_obs){\n    real ipred_tmp = ipred[i];\n    real sigma_tmp = sqrt(square(ipred_tmp) * sigma_sq_p + sigma_sq_a + \n                          2*ipred_tmp*sigma_p_a);\n    dv_ppc[i] = normal_rng(ipred_tmp, sigma_tmp);\n    log_lik[i] = normal_lpdf(dv_obs[i] | ipred_tmp, sigma_tmp);\n    wres[i] = res[i]/sigma_tmp;\n    iwres[i] = ires[i]/sigma_tmp;\n  }\n  \n}\n\n\nWe can then fit the data to this model. A guide for choosing the chains, parallel_chains, and threads_per_chain arguments is to\n\nFigure out how many cores you have available (parallel::detectCores())\nChoose the number of chains you want to sample (we recommend 4)\nIf chains < parallel::detectCores(), then have parallel_chains = chains (almost all modern machines have at least 4 cores)\nthreads_per_chain should be anywhere between 1 and \\(\\frac{parallel::detectCores()}{parallel\\_chains}\\)\n\nFor example, if your machine has 32 cores, we recommend having 4 chains, 4 parallel_chains, and 8 threads_per_chain. This will make use of all the available cores. Using more threads_per_chain won’t be helpful in reducing execution time, since the available cores are already in use.\n\n\nCode\nstan_data <- list(n_subjects = n_subjects,\n                  n_total = n_total,\n                  n_obs = n_obs,\n                  i_obs = i_obs,\n                  ID = nonmem_data$ID,\n                  amt = nonmem_data$amt,\n                  cmt = nonmem_data$cmt,\n                  evid = nonmem_data$evid,\n                  rate = nonmem_data$rate,\n                  ii = nonmem_data$ii,\n                  addl = nonmem_data$addl,\n                  ss = nonmem_data$ss,\n                  time = nonmem_data$time,\n                  dv = nonmem_data$DV,\n                  subj_start = subj_start,\n                  subj_end = subj_end,\n                  location_tvcl = 0.5,\n                  location_tvvc = 6,\n                  location_tvq = 2,\n                  location_tvvp = 6,\n                  scale_tvcl = 1,\n                  scale_tvvc = 1,\n                  scale_tvq = 1,\n                  scale_tvvp = 1,\n                  scale_omega_cl = 0.4,\n                  scale_omega_vc = 0.4,\n                  scale_omega_q = 0.4,\n                  scale_omega_vp = 0.4,\n                  lkj_df_omega = 2,\n                  scale_sigma_p = 0.5,\n                  scale_sigma_a = 1,\n                  lkj_df_sigma = 2)\n\nfit3 <- model3$sample(data = stan_data,\n                      seed = 112358,\n                      chains = 4,\n                      parallel_chains = 4,\n                      threads_per_chain = 4,\n                      iter_warmup = 500,\n                      iter_sampling = 1000,\n                      adapt_delta = 0.8,\n                      refresh = 500,\n                      max_treedepth = 10,\n                      init = function() list(TVCL = rlnorm(1, log(0.3), 0.3),\n                                             TVVC = rlnorm(1, log(6), 0.3),\n                                             TVQ = rlnorm(1, log(1), 0.3),\n                                             TVVP = rlnorm(1, log(3), 0.3),\n                                             omega = rlnorm(5, log(0.3), 0.3),\n                                             sigma = rlnorm(2, log(0.5), 0.3)))\n\n\nRunning MCMC with 4 parallel chains, with 4 thread(s) per chain...\n\nChain 1 Iteration:    1 / 1500 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 1500 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 1500 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 1500 [  0%]  (Warmup) \nChain 1 Iteration:  500 / 1500 [ 33%]  (Warmup) \nChain 1 Iteration:  501 / 1500 [ 33%]  (Sampling) \nChain 2 Iteration:  500 / 1500 [ 33%]  (Warmup) \nChain 2 Iteration:  501 / 1500 [ 33%]  (Sampling) \nChain 4 Iteration:  500 / 1500 [ 33%]  (Warmup) \nChain 4 Iteration:  501 / 1500 [ 33%]  (Sampling) \nChain 3 Iteration:  500 / 1500 [ 33%]  (Warmup) \nChain 3 Iteration:  501 / 1500 [ 33%]  (Sampling) \nChain 1 Iteration: 1000 / 1500 [ 66%]  (Sampling) \nChain 2 Iteration: 1000 / 1500 [ 66%]  (Sampling) \nChain 4 Iteration: 1000 / 1500 [ 66%]  (Sampling) \nChain 3 Iteration: 1000 / 1500 [ 66%]  (Sampling) \nChain 1 Iteration: 1500 / 1500 [100%]  (Sampling) \nChain 1 finished in 4408.8 seconds.\nChain 2 Iteration: 1500 / 1500 [100%]  (Sampling) \nChain 2 finished in 4473.2 seconds.\nChain 4 Iteration: 1500 / 1500 [100%]  (Sampling) \nChain 4 finished in 4564.1 seconds.\nChain 3 Iteration: 1500 / 1500 [100%]  (Sampling) \nChain 3 finished in 4621.9 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 4517.0 seconds.\nTotal execution time: 4622.9 seconds."
  },
  {
    "objectID": "poppk.html#two-compartment-model-with-iv-infusion-and-bloq-values",
    "href": "poppk.html#two-compartment-model-with-iv-infusion-and-bloq-values",
    "title": "3  A Bayesian Approach to PK/PD using Stan and Torsten",
    "section": "3.4 Two-Compartment Model with IV Infusion and BLOQ values",
    "text": "3.4 Two-Compartment Model with IV Infusion and BLOQ values\nOftentimes, we have observations that are below the limit of quantification (BLOQ), meaning that the assay has only been validated down to a certain value, (the lower limit of quantification, LLOQ). A paper by Stuart Beal goes through 7 methods of handling this BLOQ data in NONMEM. While in my experience M1 (dropping any BLOQ values completely) and M5 (replace BLOQ values with \\(LLOQ/2\\)) are the most common in the pharmacometrics world, M3 (treat the BLOQ values as left-censored data) and M4 (treat the BLOQ values as left-censored data and truncated below at 0) are more technically correct and tend to produce better results.\n\n3.4.1 Data\nWe will just adapt the previous data to create some BLOQ values and then fit it. To create these BLOQ, let’s assume an LLOQ of \\(0.5 \\frac{\\mu g}{mL}\\) for the first four cohorts and an LLOQ of \\(5 \\frac{\\mu g}{mL}\\) for the last 4 cohorts14.\n\n\nCode\nnonmem_data_bloq <- nonmem_data_observed %>%\n  mutate(LLOQ = if_else(ID %in% 1:12, 0.5, 5),\n         BLOQ = factor(if_else(DV < LLOQ, 1, 0)),\n         DV = if_else(BLOQ %in% c(1, NA), NA_real_, DV),\n         MDV = as.numeric(is.na(DV) | BLOQ == 1))\n\n\nnonmem_data_bloq %>%\n  mutate(across(where(is.double), round, 3)) %>%\n  rename_all(toupper) %>%\n  DT::datatable(rownames = FALSE, filter = \"top\",\n                options = list(scrollX = TRUE,\n                               columnDefs = list(list(className = 'dt-center',\n                                                      targets = \"_all\"))))\n\n\n\n\n\n\n\nWe can visualize the observed data in this NONMEM data set that we will be modeling (with dosing events indicated with vertical magenta lines and BLOQ values indicated by lime green dots at the LLOQ (dashed line):\n\n\n\nCode\n(p1 <- ggplot(nonmem_data_bloq %>%\n                mutate(ID = factor(ID)) %>%\n                group_by(ID) %>%\n                mutate(Dose = factor(max(AMT, na.rm = TRUE))) %>%\n                ungroup() %>%\n                filter(MDV == 0)) +\n  geom_line(mapping = aes(x = TIME, y = DV, group = ID, color = Dose)) +\n  geom_point(mapping = aes(x = TIME, y = DV, group = ID, color = Dose)) +\n  scale_color_discrete(name = \"Dose (mg)\") +\n  scale_y_log10(name = latex2exp::TeX(\"$Drug Conc. \\\\; (\\\\mu g/mL)$\"),\n                limits = c(NA, NA)) +\n  scale_x_continuous(name = \"Time (w)\",\n                     breaks = seq(0, 168, by = 28),\n                     labels = seq(0, 168/7, by = 4)) +\n  theme_bw(18) +\n  theme(axis.text = element_text(size = 14, face = \"bold\"),\n        axis.title = element_text(size = 18, face = \"bold\"),\n        axis.line = element_line(size = 2),\n        legend.position = \"bottom\") +\n  geom_hline(aes(yintercept = LLOQ, group = ID), size = 1.2, linetype = 2) +\n  geom_vline(data = nonmem_data_bloq %>%\n               mutate(ID = factor(ID)) %>%\n               filter(EVID == 1),\n             mapping = aes(xintercept = TIME, group = ID),\n             linetype = 2, color = \"magenta\", alpha = 0.5) +\n  geom_point(data = nonmem_data_bloq %>%\n                 mutate(ID = factor(ID),\n                        DV = LLOQ) %>%\n                 filter(BLOQ == 1, TIME > 0),\n               aes(x = TIME, y = DV, group = ID), inherit.aes = TRUE,\n               shape = 18, color = \"limegreen\", size = 3) +\n  facet_wrap(~ID, labeller = label_both, ncol = 3, scales = \"free_y\"))\n\n\n\n\n\n\nAnd we can look at a quick summary of the number of BLOQ values:\n\n\nCode\nnonmem_data_bloq %>%\n  filter(EVID == 0) %>%\n  group_by(ID) %>%\n  summarize(lloq = unique(LLOQ),\n            n_obs = n(),\n            n_bloq = sum(BLOQ == 1)) %>%\n  filter(n_bloq > 0) %>%\n  knitr::kable(col.names = c(\"ID\", \"LLOQ\", \"Num. Observations\",\n                             \"Num. BLOQ\")) %>%\n  kableExtra::kable_styling(full_width = FALSE)\n\n\n\n\n \n  \n    ID \n    LLOQ \n    Num. Observations \n    Num. BLOQ \n  \n \n\n  \n    1 \n    0.5 \n    20 \n    3 \n  \n  \n    2 \n    0.5 \n    18 \n    4 \n  \n  \n    3 \n    0.5 \n    19 \n    4 \n  \n  \n    14 \n    5.0 \n    19 \n    1 \n  \n\n\n\n\n\n\n\n3.4.2 Treat the BLOQ values as Left-Censored Data (M3)\nInstead of tossing out the BLOQ data (M1) or assigning them some arbitrary value (M5-M7), we should keep them in the data set and treat them as left-censored data. This means that the likelihood contribution for observation \\(c_{ij}\\) is calculated differently for observed values than for BLOQ values: \\[\\begin{align}\n\\mbox{observed data} &- f\\left(c_{ij} \\, | \\, \\theta_i, \\sigma, t_{ij} \\right) \\notag \\\\\n\\mbox{BLOQ data} &- F\\left(LLOQ \\, | \\, \\theta_i, \\sigma, t_{ij} \\right) \\notag \\\\\n\\end{align}\\]\nwhere \\(f\\left(c_{ij} \\, | \\, \\theta_i, \\sigma, t_{ij} \\right)\\) is the density (pdf) and \\(F\\left(LLOQ \\, | \\, \\theta_i, \\sigma, t_{ij} \\right) = P\\left(c_{ij} \\leq LLOQ\\, | \\, \\theta_i, \\sigma, t_{ij} \\right)\\) is the cumulative distribution function (cdf).\nThe relevant snippet of Stan code is as follows:\n\nreal partial_sum_lpmf(array[] int seq_subj, int start, int end,\n                        vector dv_obs, array[] int dv_obs_id, array[] int i_obs,\n                        array[] real amt, array[] int cmt, array[] int evid,\n                        array[] real time, array[] real rate, array[] real ii,\n                        array[] int addl, array[] int ss,\n                        array[] int subj_start, array[] int subj_end,\n                        real TVCL, real TVVC, real TVQ, real TVVP,\n                        vector omega, matrix L, matrix Z,\n                        vector sigma, matrix L_Sigma,\n                        vector lloq, array[] int bloq,\n                        int n_random, int n_subjects, int n_total){\n\n    real ptarget = 0;\n    vector[n_random] typical_values = to_vector({TVCL, TVVC, TVQ, TVVP});\n\n    matrix[n_subjects, n_random] eta = diag_pre_multiply(omega, L * Z);\n\n    matrix[n_subjects, n_random] theta =\n                          (rep_matrix(typical_values, n_subjects) .* exp(eta))';\n\n    matrix[2, 2] R_Sigma = multiply_lower_tri_self_transpose(L_Sigma);\n    matrix[2, 2] Sigma = quad_form_diag(R_Sigma, sigma);\n\n    int N = end - start + 1;    // number of subjects in this slice\n    vector[n_total] dv_ipred;   // = rep_vector(0, n_total);\n    matrix[n_total, 3] x_ipred; // = rep_matrix(0, n_total, 2);\n\n    int n_obs_slice = num_between(subj_start[start], subj_end[end], i_obs);\n    array[n_obs_slice] int i_obs_slice = find_between(subj_start[start],\n                                                      subj_end[end], i_obs);\n\n    vector[n_obs_slice] dv_obs_slice = find_between_vec(start, end,\n                                                        dv_obs_id, dv_obs);\n\n    vector[n_obs_slice] ipred_slice;\n\n    vector[n_obs_slice] lloq_slice = lloq[i_obs_slice];\n    array[n_obs_slice] int bloq_slice = bloq[i_obs_slice];\n\n    for(n in 1:N){            // loop over subjects in this slice\n\n      int nn = n + start - 1; // nn is the ID of the current subject\n\n      row_vector[n_random] theta_nn = theta[nn]; // access the parameters for subject nn\n      real cl = theta_nn[1];\n      real vc = theta_nn[2];\n      real q = theta_nn[3];\n      real vp = theta_nn[4];\n      array[5] real theta_params = {cl, q, vc, vp, 0}; // The 0 is for KA. Skip the absorption\n\n      x_ipred[subj_start[nn]:subj_end[nn], ] =\n        pmx_solve_twocpt(time[subj_start[nn]:subj_end[nn]],\n                         amt[subj_start[nn]:subj_end[nn]],\n                         rate[subj_start[nn]:subj_end[nn]],\n                         ii[subj_start[nn]:subj_end[nn]],\n                         evid[subj_start[nn]:subj_end[nn]],\n                         cmt[subj_start[nn]:subj_end[nn]],\n                         addl[subj_start[nn]:subj_end[nn]],\n                         ss[subj_start[nn]:subj_end[nn]],\n                         theta_params)';\n\n      dv_ipred[subj_start[nn]:subj_end[nn]] =\n        x_ipred[subj_start[nn]:subj_end[nn], 2] ./ vc;\n\n    }\n\n    ipred_slice = dv_ipred[i_obs_slice];\n\n    for(i in 1:n_obs_slice){\n      real sigma_tmp = sqrt(square(ipred_slice[i]) * Sigma[1, 1] +\n                                         Sigma[2, 2] +\n                                         2*ipred_slice[i]*Sigma[2, 1]);\n      if(bloq_slice[i] == 1){\n        ptarget += normal_lcdf(lloq_slice[i] | ipred_slice[i], sigma_tmp);\n      }else{\n        ptarget += normal_lpdf(dv_obs_slice[i] | ipred_slice[i], sigma_tmp);\n      }\n    }\n\n    return ptarget;\n\n  }\n\n\n3.4.3 Treat the BLOQ values as Left-Censored Data and Truncated Below at 0 (M4)\nWe know that drug concentrations cannot be \\(< 0\\), but the normal distribution has support (\\(-\\infty, \\, \\infty\\))15, so we will assume a normal distribution truncated below at 0. This will have the effect of limiting the support of our assumed distribution to \\((0, \\, \\infty)\\). Since we’re assuming a truncated distribution, we need to adjust the likelihood contributions of our data16: \\[\\begin{align}\n\\mbox{observed data} &- \\frac{f\\left(c_{ij} \\, | \\, \\theta_i, \\sigma, t_{ij} \\right)}{1 - F\\left(0 \\, | \\, \\theta_i, \\sigma, t_{ij} \\right)} \\notag \\\\\n\\mbox{BLOQ data} &- \\frac{F\\left(LLOQ \\, | \\, \\theta_i, \\sigma, t_{ij} \\right) - F\\left(0 \\, | \\, \\theta_i, \\sigma, t_{ij} \\right)}{1 - F\\left(0 \\, | \\, \\theta_i, \\sigma, t_{ij} \\right)} \\notag \\\\\n\\end{align}\\]\nWe can see how this is implemented in the .stan file below:\n\n\nCode\nmodel_m4 <- cmdstan_model(\"Torsten/Fit/iv_2cmt_ppa_m4.stan\",\n                          cpp_options = list(stan_threads = TRUE))\n\nmodel_m4$print()\n\n\n// IV Infusion\n// Two-compartment PK Model\n// IIV on CL, VC, Q, VP (full covariance matrix)\n// proportional plus additive error - DV = CP(1 + eps_p) + eps_a\n// Closed form solution using Torsten\n// Implements threading for within-chain parallelization\n// Deals with BLOQ values by the \"CDF trick\" (M4)\n// Since we have a normal distribution on the error, but the DV must be > 0, it\n//   truncates the likelihood below at 0\n// For PPC, it generates values from a normal that is truncated below at 0\n\n\nfunctions{\n\n  array[] int sequence(int start, int end) { \n    array[end - start + 1] int seq;\n    for (n in 1:num_elements(seq)) {\n      seq[n] = n + start - 1;\n    }\n    return seq; \n  } \n  \n  int num_between(int lb, int ub, array[] int y){\n    \n    int n = 0;\n    for(i in 1:num_elements(y)){\n      if(y[i] >= lb && y[i] <= ub)\n                    n = n + 1;\n    }\n    return n;\n    \n  }\n  \n  array[] int find_between(int lb, int ub, array[] int y) {\n    // vector[num_between(lb, ub, y)] result;\n    array[num_between(lb, ub, y)] int result;\n    int n = 1;\n    for (i in 1:num_elements(y)) {\n      if (y[i] >= lb && y[i] <= ub) {\n        result[n] = y[i];\n        n = n + 1;\n      }\n    }\n    return result;\n  }\n  \n  vector find_between_vec(int lb, int ub, array[] int idx, vector y) {\n    \n    vector[num_between(lb, ub, idx)] result;\n    int n = 1;\n    if(num_elements(idx) != num_elements(y)) reject(\"illegal input\");\n    for (i in 1:rows(y)) {\n      if (idx[i] >= lb && idx[i] <= ub) {\n        result[n] = y[i];\n        n = n + 1;\n      }\n    }\n    return result;\n  }\n  \n  real normal_lb_rng(real mu, real sigma, real lb){\n    \n    real p_lb = normal_cdf(lb | mu, sigma);\n    real u = uniform_rng(p_lb, 1);\n    real y = mu + sigma * inv_Phi(u);\n    return y;\n\n  }\n  \n  real partial_sum_lpmf(array[] int seq_subj, int start, int end,\n                        vector dv_obs, array[] int dv_obs_id, array[] int i_obs,\n                        array[] real amt, array[] int cmt, array[] int evid, \n                        array[] real time, array[] real rate, array[] real ii, \n                        array[] int addl, array[] int ss,\n                        array[] int subj_start, array[] int subj_end, \n                        real TVCL, real TVVC, real TVQ, real TVVP, \n                        vector omega, matrix L, matrix Z, \n                        vector sigma, matrix L_Sigma,\n                        vector lloq, array[] int bloq,\n                        int n_random, int n_subjects, int n_total){\n                           \n    real ptarget = 0;\n    row_vector[n_random] typical_values = to_row_vector({TVCL, TVVC, TVQ, TVVP});\n    \n    matrix[n_subjects, n_random] eta = diag_pre_multiply(omega, L * Z)';\n\n    matrix[n_subjects, n_random] theta =\n                          (rep_matrix(typical_values, n_subjects) .* exp(eta));\n                          \n    matrix[2, 2] R_Sigma = multiply_lower_tri_self_transpose(L_Sigma);\n    matrix[2, 2] Sigma = quad_form_diag(R_Sigma, sigma);\n    \n                              \n    int N = end - start + 1;    // number of subjects in this slice  \n    vector[n_total] dv_ipred;   \n    matrix[n_total, 3] x_ipred; \n  \n\n    int n_obs_slice = num_between(subj_start[start], subj_end[end], i_obs);\n    array[n_obs_slice] int i_obs_slice = find_between(subj_start[start], \n                                                      subj_end[end], i_obs);\n                                                \n    vector[n_obs_slice] dv_obs_slice = find_between_vec(start, end, \n                                                        dv_obs_id, dv_obs);\n    \n    vector[n_obs_slice] ipred_slice;\n    \n    vector[n_obs_slice] lloq_slice = lloq[i_obs_slice];\n    array[n_obs_slice] int bloq_slice = bloq[i_obs_slice];\n    \n    \n    for(n in 1:N){            // loop over subjects in this slice\n    \n      int nn = n + start - 1; // nn is the ID of the current subject\n      \n      row_vector[n_random] theta_nn = theta[nn]; // access the parameters for subject nn\n      real cl = theta_nn[1];\n      real vc = theta_nn[2];\n      real q = theta_nn[3];\n      real vp = theta_nn[4];\n      array[5] real theta_params = {cl, q, vc, vp, 0}; // The 0 is for KA. Skip the absorption\n      \n      x_ipred[subj_start[nn]:subj_end[nn], ] =\n        pmx_solve_twocpt(time[subj_start[nn]:subj_end[nn]],\n                         amt[subj_start[nn]:subj_end[nn]],\n                         rate[subj_start[nn]:subj_end[nn]],\n                         ii[subj_start[nn]:subj_end[nn]],\n                         evid[subj_start[nn]:subj_end[nn]],\n                         cmt[subj_start[nn]:subj_end[nn]],\n                         addl[subj_start[nn]:subj_end[nn]],\n                         ss[subj_start[nn]:subj_end[nn]],\n                         theta_params)';\n                      \n      dv_ipred[subj_start[nn]:subj_end[nn]] = \n        x_ipred[subj_start[nn]:subj_end[nn], 2] ./ vc;\n    \n    }\n  \n    \n    ipred_slice = dv_ipred[i_obs_slice];\n    \n    for(i in 1:n_obs_slice){\n      real sigma_tmp = sqrt(square(ipred_slice[i]) * Sigma[1, 1] +\n                                         Sigma[2, 2] + \n                                         2*ipred_slice[i]*Sigma[2, 1]);\n      if(bloq_slice[i] == 1){\n        // ptarget += log(normal_cdf(lloq_slice[i] | ipred_slice[i], sigma_tmp) -\n        //                normal_cdf(0.0 | ipred_slice[i], sigma_tmp)) -\n        //            normal_lccdf(0.0 | ipred_slice[i], sigma_tmp);  \n        ptarget += log_diff_exp(normal_lcdf(lloq_slice[i] | ipred_slice[i], \n                                                            sigma_tmp),\n                                normal_lcdf(0.0 | ipred_slice[i], sigma_tmp)) -\n                   normal_lccdf(0.0 | ipred_slice[i], sigma_tmp); \n      }else{\n        ptarget += normal_lpdf(dv_obs_slice[i] | ipred_slice[i], sigma_tmp) -\n                   normal_lccdf(0.0 | ipred_slice[i], sigma_tmp);\n      }\n    }                                         \n                              \n    return ptarget;\n                           \n  }\n  \n}\ndata{\n  \n  int n_subjects;\n  int n_total;\n  int n_obs;\n  array[n_obs] int i_obs;\n  array[n_total] int ID;\n  array[n_total] real amt;\n  array[n_total] int cmt;\n  array[n_total] int evid;\n  array[n_total] real rate;\n  array[n_total] real ii;\n  array[n_total] int addl;\n  array[n_total] int ss;\n  array[n_total] real time;\n  vector<lower = 0>[n_total] dv;\n  array[n_subjects] int subj_start;\n  array[n_subjects] int subj_end;\n  vector[n_total] lloq;\n  array[n_total] int bloq;\n  \n  real<lower = 0> scale_tvcl;      // Prior Scale parameter for CL\n  real<lower = 0> scale_tvvc;      // Prior Scale parameter for VC\n  real<lower = 0> scale_tvq;       // Prior Scale parameter for Q\n  real<lower = 0> scale_tvvp;      // Prior Scale parameter for VP\n  \n  real<lower = 0> scale_omega_cl; // Prior scale parameter for omega_cl\n  real<lower = 0> scale_omega_vc; // Prior scale parameter for omega_vc\n  real<lower = 0> scale_omega_q;  // Prior scale parameter for omega_q\n  real<lower = 0> scale_omega_vp; // Prior scale parameter for omega_vp\n  \n  real<lower = 0> lkj_df_omega;   // Prior degrees of freedom for omega cor mat\n  \n  real<lower = 0> scale_sigma_p;  // Prior Scale parameter for proportional error\n  real<lower = 0> scale_sigma_a;  // Prior Scale parameter for additive error\n  real<lower = 0> lkj_df_sigma;   // Prior degrees of freedom for sigma cor mat\n \n}\ntransformed data{ \n  \n  int grainsize = 1;\n  \n  vector<lower = 0>[n_obs] dv_obs = dv[i_obs];\n  array[n_obs] int dv_obs_id = ID[i_obs];\n  \n  vector[n_obs] lloq_obs = lloq[i_obs];\n  array[n_obs] int bloq_obs = bloq[i_obs];\n  \n  int n_random = 4;                    // Number of random effects\n  \n  array[n_random] real scale_omega = {scale_omega_cl, scale_omega_vc, \n                                      scale_omega_q, scale_omega_vp}; \n  array[2] real scale_sigma = {scale_sigma_p, scale_sigma_a};\n  \n  array[n_subjects] int seq_subj = sequence(1, n_subjects); // reduce_sum over subjects\n  \n}\nparameters{ \n  \n  real<lower = 0> TVCL;       \n  real<lower = 0> TVVC; \n  real<lower = 0> TVQ;\n  real<lower = 0> TVVP; \n  \n  vector<lower = 0>[n_random] omega;\n  cholesky_factor_corr[n_random] L;\n  \n  vector<lower = 0>[2] sigma;\n  cholesky_factor_corr[2] L_Sigma;\n  \n  matrix[n_random, n_subjects] Z;\n  \n}\n\nmodel{ \n  \n  // Priors\n  TVCL ~ cauchy(0, scale_tvcl);\n  TVVC ~ cauchy(0, scale_tvvc);\n  TVQ ~ cauchy(0, scale_tvq);\n  TVVP ~ cauchy(0, scale_tvvp);\n\n  omega ~ normal(0, scale_omega);\n  L ~ lkj_corr_cholesky(lkj_df_omega);\n  \n  sigma ~ normal(0, scale_sigma);\n  L_Sigma ~ lkj_corr_cholesky(lkj_df_sigma);\n  \n  to_vector(Z) ~ std_normal();\n  \n  // Likelihood\n  target += reduce_sum(partial_sum_lupmf, seq_subj, grainsize,\n                       dv_obs, dv_obs_id, i_obs,\n                       amt, cmt, evid, time, \n                       rate, ii, addl, ss, subj_start, subj_end, \n                       TVCL, TVVC, TVQ, TVVP, omega, L, Z,\n                       sigma, L_Sigma, \n                       lloq, bloq,\n                       n_random, n_subjects, n_total);\n}\ngenerated quantities{\n  \n  real<lower = 0> sigma_p = sigma[1];\n  real<lower = 0> sigma_a = sigma[2];\n  \n  real<lower = 0> sigma_sq_p = square(sigma_p);\n  real<lower = 0> sigma_sq_a = square(sigma_a); \n\n  real<lower = 0> omega_cl = omega[1];\n  real<lower = 0> omega_vc = omega[2];\n  real<lower = 0> omega_q = omega[3];\n  real<lower = 0> omega_vp = omega[4];\n\n  real<lower = 0> omega_sq_cl = square(omega_cl);\n  real<lower = 0> omega_sq_vc = square(omega_vc);\n  real<lower = 0> omega_sq_q = square(omega_q);\n  real<lower = 0> omega_sq_vp = square(omega_vp);\n\n  real cor_cl_vc;\n  real cor_cl_q;\n  real cor_cl_vp;\n  real cor_vc_q;\n  real cor_vc_vp;\n  real cor_q_vp;\n  real omega_cl_vc;\n  real omega_cl_q;\n  real omega_cl_vp;\n  real omega_vc_q;\n  real omega_vc_vp;\n  real omega_q_vp;\n\n  real cor_p_a;\n  real sigma_p_a;\n\n  vector[n_subjects] eta_cl;\n  vector[n_subjects] eta_vc;\n  vector[n_subjects] eta_q;\n  vector[n_subjects] eta_vp;\n  vector[n_subjects] CL;\n  vector[n_subjects] VC;\n  vector[n_subjects] Q;\n  vector[n_subjects] VP;\n  \n  vector[n_obs] ipred;\n  vector[n_obs] pred;\n  vector[n_obs] dv_ppc;\n  vector[n_obs] log_lik;\n  vector[n_obs] res;\n  vector[n_obs] wres;\n  vector[n_obs] ires;\n  vector[n_obs] iwres;\n \n  {\n    row_vector[n_random] typical_values = to_row_vector({TVCL, TVVC, TVQ, TVVP});\n\n    matrix[n_random, n_random] R = multiply_lower_tri_self_transpose(L);\n    matrix[n_random, n_random] Omega = quad_form_diag(R, omega);\n\n    matrix[2, 2] R_Sigma = multiply_lower_tri_self_transpose(L_Sigma);\n    matrix[2, 2] Sigma = quad_form_diag(R_Sigma, sigma);\n\n    matrix[n_subjects, n_random] eta = diag_pre_multiply(omega, L * Z)';\n\n    matrix[n_subjects, n_random] theta =\n                          (rep_matrix(typical_values, n_subjects) .* exp(eta));\n\n    vector[n_total] dv_pred;\n    matrix[n_total, 3] x_pred;\n    vector[n_total] dv_ipred;\n    matrix[n_total, 3] x_ipred;\n    \n    array[5] real theta_params_tv = {TVCL, TVQ, TVVC, TVVP, 0}; // 0 is for KA to skip absorption\n    \n    eta_cl = col(eta, 1);\n    eta_vc = col(eta, 2);\n    eta_q = col(eta, 3);\n    eta_vp = col(eta, 4);\n    \n    CL = col(theta, 1);\n    VC = col(theta, 2);\n    Q = col(theta, 3);\n    VP = col(theta, 4);\n\n    cor_cl_vc = R[1, 2];\n    cor_cl_q = R[1, 3];\n    cor_cl_vp = R[1, 4];\n    cor_vc_q = R[2, 3];\n    cor_vc_vp = R[2, 4];\n    cor_q_vp = R[3, 4];\n    omega_cl_vc = Omega[1, 2];\n    omega_cl_q = Omega[1, 3];\n    omega_cl_vp = Omega[1, 4];\n    omega_vc_q = Omega[2, 3];\n    omega_vc_vp = Omega[2, 4];\n    omega_q_vp = Omega[3, 4];\n\n    cor_p_a = R_Sigma[2, 1];\n    sigma_p_a = Sigma[2, 1];\n\n    for(j in 1:n_subjects){\n\n      array[5] real theta_params = {CL[j], Q[j], VC[j], VP[j], 0};\n      \n      x_ipred[subj_start[j]:subj_end[j],] =\n        pmx_solve_twocpt(time[subj_start[j]:subj_end[j]],\n                         amt[subj_start[j]:subj_end[j]],\n                         rate[subj_start[j]:subj_end[j]],\n                         ii[subj_start[j]:subj_end[j]],\n                         evid[subj_start[j]:subj_end[j]],\n                         cmt[subj_start[j]:subj_end[j]],\n                         addl[subj_start[j]:subj_end[j]],\n                         ss[subj_start[j]:subj_end[j]],\n                         theta_params)';\n                      \n                      \n      dv_ipred[subj_start[j]:subj_end[j]] = \n        x_ipred[subj_start[j]:subj_end[j], 2] ./ VC[j];\n\n      x_pred[subj_start[j]:subj_end[j],] =\n        pmx_solve_twocpt(time[subj_start[j]:subj_end[j]],\n                         amt[subj_start[j]:subj_end[j]],\n                         rate[subj_start[j]:subj_end[j]],\n                         ii[subj_start[j]:subj_end[j]],\n                         evid[subj_start[j]:subj_end[j]],\n                         cmt[subj_start[j]:subj_end[j]],\n                         addl[subj_start[j]:subj_end[j]],\n                         ss[subj_start[j]:subj_end[j]],\n                         theta_params_tv)';\n\n      dv_pred[subj_start[j]:subj_end[j]] = \n        x_pred[subj_start[j]:subj_end[j], 2] ./ TVVC;\n    }\n\n    pred = dv_pred[i_obs];\n    ipred = dv_ipred[i_obs];\n\n  }\n\n  res = dv_obs - pred;                                                                             \n  ires = dv_obs - ipred;\n\n  for(i in 1:n_obs){\n    real ipred_tmp = ipred[i];\n    real sigma_tmp = sqrt(square(ipred_tmp) * sigma_sq_p + sigma_sq_a + \n                          2*ipred_tmp*sigma_p_a);\n    // dv_ppc[i] = normal_rng(ipred_tmp, sigma_tmp);\n    dv_ppc[i] = normal_lb_rng(ipred_tmp, sigma_tmp, 0.0);\n    if(bloq_obs[i] == 1){\n      // log_lik[i] = log(normal_cdf(lloq_obs[i] | ipred_tmp, sigma_tmp) - \n      //                  normal_cdf(0.0 | ipred_tmp, sigma_tmp)) -\n      //              normal_lccdf(0.0 | ipred_tmp, sigma_tmp);\n      log_lik[i] = log_diff_exp(normal_lcdf(lloq_obs[i] | ipred_tmp, sigma_tmp),\n                                normal_lcdf(0.0 | ipred_tmp, sigma_tmp)) -\n                   normal_lccdf(0.0 | ipred_tmp, sigma_tmp); \n    }else{\n      log_lik[i] = normal_lpdf(dv_obs[i] | ipred_tmp, sigma_tmp) - \n                   normal_lccdf(0.0 | ipred_tmp, sigma_tmp);\n    }\n    wres[i] = res[i]/sigma_tmp;\n    iwres[i] = ires[i]/sigma_tmp;\n  }\n  \n}\n\n\nPreparing and fitting the data is very similar to our earlier example. The difference here is that we add the lloq and bloq columns from our NONMEM dataset to stan_data:\n\n\nCode\nnonmem_data_bloq <- nonmem_data_bloq %>%\n  mutate(BLOQ = as.numeric(as.character(BLOQ)),\n         DV = if_else(BLOQ %in% c(1, NA), 5555555, DV), # This value can be anything > 0. It'll be indexed away\n         BLOQ = if_else(is.na(BLOQ), -999, BLOQ)) %>%\n  rename_all(tolower) %>%\n  rename(ID = \"id\",\n         DV = \"dv\")\n\nn_subjects <- nonmem_data_bloq %>%  # number of individuals\n  distinct(ID) %>%\n  count() %>%\n  deframe()\n\nn_total <- nrow(nonmem_data_bloq)   # total number of records\n\ni_obs <- nonmem_data_bloq %>%\n  mutate(row_num = 1:n()) %>%\n  filter(evid == 0) %>%\n  select(row_num) %>%\n  deframe()\n\nn_obs <- length(i_obs)\n\nsubj_start <- nonmem_data_bloq %>%\n  mutate(row_num = 1:n()) %>%\n  group_by(ID) %>%\n  slice_head(n = 1) %>%\n  ungroup() %>%\n  select(row_num) %>%\n  deframe()\n\nsubj_end <- c(subj_start[-1] - 1, n_total)\n\nstan_data <- list(n_subjects = n_subjects,\n                  n_total = n_total,\n                  n_obs = n_obs,\n                  i_obs = i_obs,\n                  ID = nonmem_data_bloq$ID,\n                  amt = nonmem_data_bloq$amt,\n                  cmt = nonmem_data_bloq$cmt,\n                  evid = nonmem_data_bloq$evid,\n                  rate = nonmem_data_bloq$rate,\n                  ii = nonmem_data_bloq$ii,\n                  addl = nonmem_data_bloq$addl,\n                  ss = nonmem_data_bloq$ss,\n                  time = nonmem_data_bloq$time,\n                  dv = nonmem_data_bloq$DV,\n                  subj_start = subj_start,\n                  subj_end = subj_end,\n                  lloq = nonmem_data_bloq$lloq,\n                  bloq = nonmem_data_bloq$bloq,\n                  scale_tvcl = 2,\n                  scale_tvvc = 10,\n                  scale_tvq = 2,\n                  scale_tvvp = 10,\n                  scale_omega_cl = 0.4,\n                  scale_omega_vc = 0.4,\n                  scale_omega_q = 0.4,\n                  scale_omega_vp = 0.4,\n                  lkj_df_omega = 2,\n                  scale_sigma_p = 0.5,\n                  scale_sigma_a = 1,\n                  lkj_df_sigma = 2)\n\nfit_m4 <- model_m4$sample(data = stan_data,\n                          chains = 4,\n                          parallel_chains = 4,\n                          threads_per_chain = 2,\n                          iter_warmup = 500,\n                          iter_sampling = 1000,\n                          adapt_delta = 0.8,\n                          refresh = 500,\n                          max_treedepth = 10,\n                          init = function() list(TVCL = rlnorm(1, log(0.3), 0.3),\n                                                 TVVC = rlnorm(1, log(5), 0.3),\n                                                 TVQ = rlnorm(1, log(1), 0.3),\n                                                 TVVP = rlnorm(1, log(10), 0.3),\n                                                 omega = rlnorm(4, log(0.3), 0.3),\n                                                 sigma = rlnorm(2, log(0.5), 0.3)))\n\n\nRunning MCMC with 4 parallel chains, with 2 thread(s) per chain...\n\nChain 1 Iteration:    1 / 1500 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 1500 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 1500 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 1500 [  0%]  (Warmup) \nChain 3 Iteration:  500 / 1500 [ 33%]  (Warmup) \nChain 3 Iteration:  501 / 1500 [ 33%]  (Sampling) \nChain 2 Iteration:  500 / 1500 [ 33%]  (Warmup) \nChain 2 Iteration:  501 / 1500 [ 33%]  (Sampling) \nChain 1 Iteration:  500 / 1500 [ 33%]  (Warmup) \nChain 1 Iteration:  501 / 1500 [ 33%]  (Sampling) \nChain 4 Iteration:  500 / 1500 [ 33%]  (Warmup) \nChain 4 Iteration:  501 / 1500 [ 33%]  (Sampling) \nChain 3 Iteration: 1000 / 1500 [ 66%]  (Sampling) \nChain 2 Iteration: 1000 / 1500 [ 66%]  (Sampling) \nChain 1 Iteration: 1000 / 1500 [ 66%]  (Sampling) \nChain 4 Iteration: 1000 / 1500 [ 66%]  (Sampling) \nChain 3 Iteration: 1500 / 1500 [100%]  (Sampling) \nChain 3 finished in 4076.2 seconds.\nChain 2 Iteration: 1500 / 1500 [100%]  (Sampling) \nChain 2 finished in 4236.9 seconds.\nChain 1 Iteration: 1500 / 1500 [100%]  (Sampling) \nChain 1 finished in 4348.2 seconds.\nChain 4 Iteration: 1500 / 1500 [100%]  (Sampling) \nChain 4 finished in 4399.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 4265.1 seconds.\nTotal execution time: 4400.5 seconds.\n\n\nCode\nfit_m4$save_object(\"Torsten/Fits/iv_2cmt_ppa_m4.rds\")\n\n\n\n3.4.3.1 Post-Processing\nAfter fitting we will want to check that the MCMC sampler worked properly and efficiently and that the chains mixed well, check model fits and assumptions, and obtain posterior estimates.\nWe can first check whether the sampler worked as expected and that the chains mixed well:\n\n\nCode\n# parameters_to_summarize <- map(c(\"TV\", \"omega_\", \"cor\", \"sigma\"),\n#                                str_subset,\n#                                string = fit_m4$metadata()$stan_variables) %>%\n#   reduce(union) %>%\n#   str_subset(\"sq\", negate = TRUE)\n\nparameters_to_summarize_main <- str_c(\"TV\", c(\"CL\", \"VC\", \"Q\", \"VP\"))\nparameters_to_summarize_variability <- str_c(\"omega_\", c(\"cl\", \"vc\", \"q\", \"vp\"))\nparameters_to_summarize_uncertainty <- str_c(\"sigma_\", c(\"p\", \"a\"))\nparameters_to_summarize_correlation <- str_subset(fit$metadata()$stan_variables,\n                                                  \"cor\")\n\nparameters_to_summarize <- c(parameters_to_summarize_main,\n                             parameters_to_summarize_variability,\n                             parameters_to_summarize_uncertainty,\n                             parameters_to_summarize_correlation)\n\n## Summary of parameter estimates\nsummary <- summarize_draws(fit_m4$draws(parameters_to_summarize),\n                           mean, median, sd, mcse_mean,\n                           ~quantile2(.x, probs = c(0.025, 0.975)), rhat,\n                           ess_bulk, ess_tail)\n\n## Check the sampler (this is very non-comprehensive)\nsampler_diagnostics <- fit_m4$sampler_diagnostics()\nsum(as_draws_df(sampler_diagnostics)$divergent__)\n\n\n[1] 0\n\n\nCode\n# Check your largest R-hats. These should be close to 1 (< 1.05 for sure, < 1.02\n# ideally)\nsummary %>%\n  select(variable, rhat) %>%\n  arrange(-rhat)\n\n\n# A tibble: 17 x 2\n   variable   rhat\n   <chr>     <dbl>\n 1 TVCL      1.01 \n 2 TVVC      1.01 \n 3 omega_cl  1.01 \n 4 TVVP      1.00 \n 5 sigma_p   1.00 \n 6 cor_cl_vc 1.00 \n 7 omega_vp  1.00 \n 8 cor_cl_q  1.00 \n 9 sigma_a   1.00 \n10 omega_q   1.00 \n11 omega_vc  1.00 \n12 TVQ       1.00 \n13 cor_vc_q  1.00 \n14 cor_p_a   1.00 \n15 cor_q_vp  1.00 \n16 cor_cl_vp 1.00 \n17 cor_vc_vp 0.999\n\n\n\n\n\nCode\n# Density Plots and Traceplots\nmcmc_combo(fit_m4$draws(parameters_to_summarize),\n           combo = c(\"dens_overlay\", \"trace\"))\n\n\n\n\n\n\nWe should look at the posterior to see if there are any parameters that are very highly correlated:\n\n\nCode\nmcmc_pairs(fit_m4$draws(c(parameters_to_summarize_main,\n                          parameters_to_summarize_variability,\n                          parameters_to_summarize_uncertainty)),\n           diag_fun = \"dens\")\n\n\n\n\n\nWe can look at plots of the auto-correlations:\n\n\nCode\nmcmc_acf(fit_m4$draws(parameters_to_summarize_main),\n         pars = parameters_to_summarize_main)\n\n\n\n\n\nAnd check the Leave-one-out cross-validation (LOO):\n\n\nCode\n## Check Leave-One-Out Cross-Validation\nfit_m4_loo <- fit_m4$loo()\nfit_m4_loo\n\n\n\nComputed from 4000 by 461 log-likelihood matrix\n\n         Estimate   SE\nelpd_loo  -1071.3 38.8\np_loo        50.7  3.4\nlooic      2142.6 77.6\n------\nMonte Carlo SE of elpd_loo is NA.\n\nPareto k diagnostic values:\n                         Count Pct.    Min. n_eff\n(-Inf, 0.5]   (good)     447   97.0%   1094      \n (0.5, 0.7]   (ok)        13    2.8%   349       \n   (0.7, 1]   (bad)        1    0.2%   146       \n   (1, Inf)   (very bad)   0    0.0%   <NA>      \nSee help('pareto-k-diagnostic') for details.\n\n\nCode\nplot(fit_m4_loo, label_points = TRUE)\n\n\n\n\n\nIf everything looks ok so far, we will want to obtain posterior parameter estimates (and, since this is simulated data using known parameters, you can compare these parameter estimates to the table with the “truth” (Table 3.1), but you won’t have that in real life, so we won’t do that directly here):\n\n\nCode\nsummary %>%\n  mutate(rse = sd/mean*100) %>%\n  select(variable, mean, sd, rse, q2.5, median, q97.5, rhat,\n         starts_with(\"ess\")) %>%\n  inner_join(neff_ratio(fit_m4, pars = parameters_to_summarize) %>%\n               enframe(name = \"variable\", value = \"n_eff_ratio\")) %>%\n  mutate(across(where(is.numeric), round, 3)) %>%\n  knitr::kable(col.names = c(\"Variable\", \"Mean\", \"Std. Dev.\", \"RSE\", \"2.5%\",\n                             \"Median\", \"97.5%\", \"$\\\\hat{R}$\", \"ESS Bulk\",\n                             \"ESS Tail\", \"ESS Ratio\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nMean\nStd. Dev.\nRSE\n2.5%\nMedian\n97.5%\n\\(\\hat{R}\\)\nESS Bulk\nESS Tail\nESS Ratio\n\n\n\n\nTVCL\n0.194\n0.015\n7.520\n0.167\n0.193\n0.225\n1.009\n517.449\n891.055\n0.129\n\n\nTVVC\n3.158\n0.181\n5.740\n2.817\n3.152\n3.525\n1.007\n1023.698\n1551.176\n0.251\n\n\nTVQ\n1.469\n0.141\n9.607\n1.210\n1.463\n1.770\n1.001\n4393.669\n3137.426\n1.061\n\n\nTVVP\n4.095\n0.201\n4.898\n3.704\n4.093\n4.497\n1.004\n6764.942\n3048.414\n1.704\n\n\nomega_cl\n0.358\n0.058\n16.238\n0.261\n0.352\n0.488\n1.006\n759.964\n1478.650\n0.191\n\n\nomega_vc\n0.268\n0.046\n17.095\n0.193\n0.263\n0.372\n1.001\n1529.692\n2132.127\n0.384\n\n\nomega_q\n0.130\n0.099\n76.268\n0.006\n0.110\n0.369\n1.001\n1863.222\n2134.794\n0.513\n\n\nomega_vp\n0.082\n0.060\n73.442\n0.003\n0.070\n0.221\n1.002\n1740.181\n2287.765\n0.451\n\n\nsigma_p\n0.202\n0.009\n4.417\n0.186\n0.202\n0.221\n1.003\n4560.104\n2978.996\n1.134\n\n\nsigma_a\n0.095\n0.063\n66.422\n0.005\n0.086\n0.233\n1.001\n1952.640\n2079.405\n0.553\n\n\ncor_cl_vc\n0.186\n0.195\n104.965\n-0.218\n0.197\n0.538\n1.002\n1174.512\n1945.675\n0.298\n\n\ncor_cl_q\n0.082\n0.339\n414.362\n-0.585\n0.089\n0.705\n1.002\n6614.288\n2936.288\n1.633\n\n\ncor_cl_vp\n-0.022\n0.341\n-1567.005\n-0.677\n-0.023\n0.642\n1.000\n6247.132\n2936.543\n1.581\n\n\ncor_vc_q\n0.153\n0.358\n234.041\n-0.570\n0.175\n0.764\n1.000\n5913.027\n3078.372\n1.512\n\n\ncor_vc_vp\n-0.068\n0.339\n-499.380\n-0.697\n-0.077\n0.605\n0.999\n5331.579\n3019.284\n1.336\n\n\ncor_q_vp\n-0.069\n0.378\n-544.911\n-0.747\n-0.080\n0.663\n1.000\n3009.205\n3185.843\n0.752\n\n\ncor_p_a\n-0.028\n0.333\n-1208.273\n-0.550\n-0.082\n0.695\n1.000\n3575.715\n2675.836\n0.842\n\n\n\n\n\nAlthough we’ve mostly looked at the population parameters so far, we can look at posterior distributions and parameter estimates for individuals:\n\n\nCode\ndraws_df <- fit_m4$draws(format = \"draws_df\")\n\ndraws_df %>%\n  gather_draws(CL[ID], VC[ID], Q[ID], VP[ID]) %>%\n  mean_qi(.width = 0.95) %>%\n  select(ID, .variable, .value, .lower, .upper) %>%\n  mutate(across(where(is.double), round, 3),\n         across(c(ID, .variable), as.factor)) %>%\n  DT::datatable(colnames = c(\"ID\", \"Variable\", \"Posterior Mean\", \"2.5%\",\n                             \"97.5%\"),\n                rownames = FALSE, filter = \"top\")\n\n\n\n\n\n\n\nWe can also visualize the full posterior density for each individual:\n\n\nCode\ndraws_df %>%\n  gather_draws(CL[ID], VC[ID], Q[ID], VP[ID]) %>%\n  ungroup() %>%\n  mutate(across(c(ID, .variable), as.factor)) %>%\n  ggplot(aes(x = .value, group = ID, color = ID)) +\n  stat_density(geom = \"line\", position = \"identity\") +\n  theme_bw() +\n  theme(axis.title.x = element_blank(),\n        plot.title = element_text(hjust = 0.5)) +\n  facet_wrap(~.variable, ncol = 1, scales = \"free\") +\n  ggtitle(\"Individual Parameter Posterior Densities\")\n\n\n\n\n\nWe will also want to assess the fits. We can look at a lot of the same plots as we would if we had done a NONMEM fit with FOCEI. We can start with DV vs. PRED/IPRED:\n\n\nCode\npost_preds_summary <- draws_df %>%\n  spread_draws(pred[i], ipred[i], dv_ppc[i]) %>%\n  mean_qi(pred, ipred, dv_ppc) %>%\n  mutate(DV = nonmem_data_bloq$DV[nonmem_data_bloq$evid == 0][i],\n         bloq = nonmem_data_bloq$bloq[nonmem_data_bloq$evid == 0][i])\n\nppc_ind <- post_preds_summary %>%\n  mutate(ID = nonmem_data_bloq$ID[nonmem_data_bloq$evid == 0][i],\n         time = nonmem_data_bloq$time[nonmem_data_bloq$evid == 0][i])\n\np_dv_vs_pred <- ggplot(post_preds_summary %>%\n                         filter(bloq == 0), aes(x = pred, y = DV)) +\n  geom_point() +\n  theme_bw() +\n  geom_abline(slope = 1, intercept = 0, color = \"blue\", size = 1.5) +\n  xlab(\"Population Predictions\") +\n  ylab(\"Observed Concentration\") +\n  theme(axis.text = element_text(size = 14, face = \"bold\"),\n        axis.title = element_text(size = 18, face = \"bold\"),\n        axis.line = element_line(size = 2)) +\n  scale_y_log10() +\n  scale_x_log10()\n\np_dv_vs_ipred <- ggplot(post_preds_summary %>%\n                          filter(bloq == 0), aes(x = ipred, y = DV)) +\n  geom_point() +\n  theme_bw() +\n  geom_abline(slope = 1, intercept = 0, color = \"blue\", size = 1.5) +\n  xlab(\"Individual Predictions\") +\n  ylab(\"Observed Concentration\") +\n  theme(axis.text = element_text(size = 14, face = \"bold\"),\n        axis.title = element_text(size = 18, face = \"bold\"),\n        axis.line = element_line(size = 2)) +\n  scale_y_log10() +\n  scale_x_log10()\n\nggpubr::ggarrange(p_dv_vs_pred, p_dv_vs_ipred)\n\n\n\n\n\nWe should also look at residual plots. We generally want to look at a QQ-plot to check our error distribution assumptions. The below animation shows the QQ-plot of the (individual weighted) residuals by draw.\n\n\nCode\nresiduals <- draws_df %>%\n  spread_draws(res[i], wres[i], ires[i], iwres[i], ipred[i]) %>%\n  mutate(time = nonmem_data_bloq$time[nonmem_data_bloq$evid == 0][i],\n         bloq = nonmem_data_bloq$bloq[nonmem_data_bloq$evid == 0][i])\n\nsome_residuals_qq <- residuals %>%\n  filter(bloq == 0) %>%\n  sample_draws(100) %>%\n  ggplot(aes(sample = iwres)) +\n  geom_qq() +\n  geom_abline() +\n  theme_bw() +\n  transition_manual(.draw)\n\nanimate(some_residuals_qq, nframes = 100, width = 384, height = 384, res = 96,\n        dev = \"png\", type = \"cairo\")\n\n\nWe also want to look at the individual weighted residuals against time and against individual prediction:\n\n\n\nCode\niwres_vs_time <- residuals %>%\n  filter(bloq == 0) %>%\n  sample_draws(9) %>%\n  mutate(qn_lower = qnorm(0.025),\n         qn_upper = qnorm(0.975)) %>%\n  ggplot(aes(x = time, y = iwres)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"red\", size = 1.5) +\n  geom_smooth(se = FALSE, color = \"blue\", size = 1.5) +\n  geom_hline(aes(yintercept = qn_lower), linetype = \"dashed\", color = \"red\",\n             size = 1.25) +\n  geom_hline(aes(yintercept = qn_upper), linetype = \"dashed\", color = \"red\",\n             size = 1.25) +\n  theme_bw() +\n  xlab(\"Time (d)\") +\n  facet_wrap(~ .draw, labeller = label_both)\n\niwres_vs_ipred <- residuals %>%\n  filter(bloq == 0) %>%\n  sample_draws(9) %>%\n  mutate(qn_lower = qnorm(0.025),\n         qn_upper = qnorm(0.975)) %>%\n  ggplot(aes(x = ipred, y = iwres)) +\n  geom_point() +\n  geom_smooth(se = FALSE, color = \"blue\", size = 1.5) +\n  geom_hline(yintercept = 0, color = \"red\", size = 1.5) +\n  geom_hline(aes(yintercept = qn_lower), linetype = \"dashed\", color = \"red\",\n             size = 1.25) +\n  geom_hline(aes(yintercept = qn_upper), linetype = \"dashed\", color = \"red\",\n             size = 1.25) +\n  theme_bw() +\n  xlab(\"Individual Predictions\") +\n  facet_wrap(~ .draw, labeller = label_both)\n\niwres_vs_time /\n  iwres_vs_ipred\n\n\n\n\n\n\nSince all the residuals look good, we can look at the distribution of the (standardized) random effects. We’ll use the posterior mean as the point estimate for each individual’s random effect.\n\n\nCode\neta_std <- draws_df %>%\n  spread_draws(eta_cl[ID], eta_vc[ID], eta_q[ID], eta_vp[ID]) %>%\n  mean_qi() %>%\n  select(ID, str_c(\"eta_\", c(\"cl\", \"vc\", \"q\", \"vp\"))) %>%\n  pivot_longer(cols = starts_with(\"eta\"),\n               names_to = \"parameter\",\n               values_to = \"eta\",\n               names_prefix = \"eta_\") %>%\n  group_by(parameter) %>%\n  mutate(eta_std = (eta - mean(eta))/sd(eta)) %>%\n  ungroup() %>%\n  mutate(parameter = toupper(parameter))\n\n\neta_hist <- eta_std %>%\n  ggplot(aes(x = eta_std, group = parameter)) +\n  geom_histogram(aes(y = ..density..)) +\n  geom_density(color = \"blue\", size = 1.5) +\n  stat_function(fun = dnorm, args = list(mean = 0, sd = 1),\n                color = \"red\", size = 1) +\n  scale_x_continuous(name = \"Standarized Indiv. Effect\",\n                     limits = c(-2.5, 2.5)) +\n  theme_bw(18) +\n  facet_wrap(~parameter, scales = \"free_x\")\n\neta_box <- eta_std %>%\n  ggplot(aes(x = parameter, y = eta_std)) +\n  geom_boxplot() +\n  scale_x_discrete(name = \"Parameter\") +\n  scale_y_continuous(name = \"Standarized Indiv. Effect\") +\n  theme_bw(18) +\n  geom_hline(yintercept = 0, linetype = \"dashed\")\n\neta_hist /\n  eta_box\n\n\n\n\n\nIf we had covariates, we would plot these random effects against the covariates to see any indication that a covariate should be included, but we don’t have that in this example, so we’ll skip that.\nWe can look at a posterior predictive check (PPC). To do these, we have simulated a dependent variable value (dv_ppc in the Stan code) for each sample from the posterior for each observation - a replication of each observation for each posterior sample. This is similar to a VPC that is commonly done with NONMEM, but is meant to provide replicates of the observed data for the observed individuals - we will show something even more similar to a VPC later where we simulate new individuals. The point of posterior predictive checking is to see if we can generate data from from our fitted models that looks a lot like the observed data.\n\n\n\nCode\nppc_ind %>%\n  ggplot(aes(x = time, group = ID)) +\n  geom_ribbon(aes(ymin = ipred.lower, ymax = ipred.upper),\n              fill = \"blue\", alpha = 0.5, show.legend = FALSE) +\n  geom_ribbon(aes(ymin = dv_ppc.lower, ymax = dv_ppc.upper),\n              fill = \"blue\", alpha = 0.25, show.legend = FALSE) +\n  geom_line(aes(y = ipred), linetype = 1, size = 1.15) +\n  geom_line(aes(y = pred), linetype = 2, size = 1.05) +\n  geom_point(data = ppc_ind %>%\n               filter(bloq == 0),\n             mapping = aes(x = time, y = DV, group = ID),\n             size = 3, color = \"red\", show.legend = FALSE) +\n  scale_y_log10(name = \"Drug Conc. (ug/mL)\",\n                limits = c(NA, NA)) +\n  scale_x_continuous(name = \"Time (w)\",\n                     breaks = seq(0, 168, by = 28),\n                     labels = seq(0, 168/7, by = 4)) +\n  theme_bw() +\n  theme(axis.text = element_text(size = 14, face = \"bold\"),\n        axis.title = element_text(size = 18, face = \"bold\"),\n        legend.position = \"bottom\") +\n  facet_wrap(~ID, labeller = label_both,\n             ncol = 4, scales = \"free\")\n\n\n\n\n\n\nIn the above plot, the solid line is the individual prediction, the dotted line is the population prediction, the red dot is the observed concentration, the darker blue ribbon is a 95% interval for the individual prediction, and the lighter blue ribbon is a 95% interval for an observation (individual prediction + residual error). This plot is a bit angular and can be a bit confusing since we only have replications at the observed time points, but we will revisit this shortly when we make predictions at a dense grid.\n\n\n3.4.3.2 Making Predictions\nAs mentioned above, we can also make predictions for\n\nThe observed individuals. We can simulate at a denser time grid (i.e. at unobserved times). I think this is mostly useful to make pictures like the previous one look a little nicer.\nFuture individuals\n\nWe can simulate many individuals with an already observed dosing regimen and compare the observations to the simulations to assess model fit (this is extremely similar to the standard VPC).\nWe can simulate many individuals with an unobserved dosing regimen to predict exposures and responses with that dosing regimen. This should help with dose selection.\nWe can simulate clinical trials\n\n\nFor any of these situations, we have a .stan file that is written to make the predictions, and we send our fitted object that contains the posterior draws (I’ve called the most recent fit fit_m4 in this presentation) through the .stan file to make predictions at requested time points given the dosing regimen(s).\n\n3.4.3.2.1 Predicting Observed Individuals\n\nPredicting Observed Individuals After FittingPredicting Observed Individuals Simultaneously with the Fitting\n\n\nAfter fitting the model, we have the CmdStanMCMC object fit_m4 that contains the draws from the posterior distribution. We will simulate\n\na population concentration-time profile (PRED) for each draw from the posterior that takes into account the uncertainty in the population parameters (\\(TVCL, \\; TVVC, \\; TVQ,\\) and \\(TVVP\\))\nan individual concentration-time profile (IPRED) for each individual for each draw from the posterior. This will create simulated profiles that take into account the uncertainty in the individual PK parameters (here, \\(CL, \\; V_c, \\; Q\\), and \\(V_p\\))\nan individual concentration-time profile (DV) that adds residual variability to the IPRED. This will create simulated profiles that take into account residual variability and also the uncertainty in the residual variability parameters (here, \\(\\sigma_p, \\; \\sigma_a,\\) and \\(\\rho_{p,a}\\)).\n\nWe can first look at the .stan file that makes the simulations:\n\n\nCode\nmodel_pred_obs <- cmdstan_model(\n  \"Torsten/Predict/iv_2cmt_ppa_m4_predict_observed_subjects.stan\")\n\nmodel_pred_obs$print()\n\n\n// IV Infusion\n// Two-compartment PK Model\n// IIV on CL, VC, Q, VP (full covariance matrix)\n// proportional plus additive error - DV = CP(1 + eps_p) + eps_a\n// Closed form solution using Torsten\n// Predictions are generated from a normal that is truncated below at 0\n\nfunctions{\n  \n  real normal_lb_rng(real mu, real sigma, real lb){\n    \n    real p_lb = normal_cdf(lb | mu, sigma);\n    real u = uniform_rng(p_lb, 1);\n    real y = mu + sigma * inv_Phi(u);\n    return y;\n\n  }\n  \n}\ndata{\n  \n  int n_subjects;\n  int n_time_new;\n  array[n_time_new] real time;\n  array[n_time_new] real amt;\n  array[n_time_new] int cmt;\n  array[n_time_new] int evid;\n  array[n_time_new] real rate;\n  array[n_time_new] real ii;\n  array[n_time_new] int addl;\n  array[n_time_new] int ss;\n  array[n_subjects] int subj_start;\n  array[n_subjects] int subj_end;\n \n}\ntransformed data{ \n  \n  int n_random = 4;                    // Number of random effects\n\n}\nparameters{ \n  \n  real<lower = 0> TVCL;       \n  real<lower = 0> TVVC; \n  real<lower = 0> TVQ;\n  real<lower = 0> TVVP; \n  \n  vector<lower = 0>[n_random] omega;\n  cholesky_factor_corr[n_random] L;\n  \n  vector<lower = 0>[2] sigma;\n  cholesky_factor_corr[2] L_Sigma;\n  \n  matrix[n_random, n_subjects] Z;\n  \n}\ngenerated quantities{\n  \n  vector[n_time_new] ipred; // ipred for the observed individuals at the new timepoints\n  vector[n_time_new] pred;  // pred for the observed individuals at the new timepoints\n  vector[n_time_new] dv;    // dv for the observed individuals at the new timepoints\n \n  {\n    row_vector[n_random] typical_values = to_row_vector({TVCL, TVVC, TVQ, TVVP});\n\n    matrix[n_random, n_random] R = multiply_lower_tri_self_transpose(L);\n    matrix[n_random, n_random] Omega = quad_form_diag(R, omega);\n\n    matrix[2, 2] R_Sigma = multiply_lower_tri_self_transpose(L_Sigma);\n    matrix[2, 2] Sigma = quad_form_diag(R_Sigma, sigma);\n\n    matrix[n_subjects, n_random] eta = diag_pre_multiply(omega, L * Z)';\n\n    matrix[n_subjects, n_random] theta =\n                          (rep_matrix(typical_values, n_subjects) .* exp(eta));\n\n    matrix[n_time_new, 3] x_pred;\n    matrix[n_time_new, 3] x_ipred;\n    \n    array[5] real theta_params_tv = {TVCL, TVQ, TVVC, TVVP, 0}; // 0 is for KA to skip absorption\n    \n    vector[n_subjects] CL = col(theta, 1);\n    vector[n_subjects] VC = col(theta, 2);\n    vector[n_subjects] Q = col(theta, 3);\n    vector[n_subjects] VP = col(theta, 4);\n\n    for(j in 1:n_subjects){\n\n      array[5] real theta_params = {CL[j], Q[j], VC[j], VP[j], 0};\n      \n      x_ipred[subj_start[j]:subj_end[j],] =\n        pmx_solve_twocpt(time[subj_start[j]:subj_end[j]],\n                         amt[subj_start[j]:subj_end[j]],\n                         rate[subj_start[j]:subj_end[j]],\n                         ii[subj_start[j]:subj_end[j]],\n                         evid[subj_start[j]:subj_end[j]],\n                         cmt[subj_start[j]:subj_end[j]],\n                         addl[subj_start[j]:subj_end[j]],\n                         ss[subj_start[j]:subj_end[j]],\n                         theta_params)';\n                      \n      ipred[subj_start[j]:subj_end[j]] = \n        x_ipred[subj_start[j]:subj_end[j], 2] ./ VC[j];\n\n      x_pred[subj_start[j]:subj_end[j],] =\n        pmx_solve_twocpt(time[subj_start[j]:subj_end[j]],\n                         amt[subj_start[j]:subj_end[j]],\n                         rate[subj_start[j]:subj_end[j]],\n                         ii[subj_start[j]:subj_end[j]],\n                         evid[subj_start[j]:subj_end[j]],\n                         cmt[subj_start[j]:subj_end[j]],\n                         addl[subj_start[j]:subj_end[j]],\n                         ss[subj_start[j]:subj_end[j]],\n                         theta_params_tv)';\n\n      pred[subj_start[j]:subj_end[j]] = \n        x_pred[subj_start[j]:subj_end[j], 2] ./ TVVC;;\n    }\n\n  \n    for(i in 1:n_time_new){\n      if(ipred[i] == 0){\n        dv[i] = 0;\n      }else{\n        real ipred_tmp = ipred[i];\n        real sigma_tmp = sqrt(square(ipred_tmp) * Sigma[1, 1] + Sigma[2, 2] + \n                              2*ipred_tmp*Sigma[2, 1]);\n        dv[i] = normal_lb_rng(ipred_tmp, sigma_tmp, 0.0);\n      }\n    }\n  }\n}\n\n\nYou’ll notice that this is a stripped-down version of the .stan file used to fit the data. This model takes in the previous fit and then makes predictions in the generated quantities block based solely on the fit we’ve already done. Because no fitting is done, only simulation, this is relatively fast. This particular file saves only simulated values for PRED, IPRED, and DV. But if we wanted, there are numerous other values we could simulate if we wanted (\\(AUC, \\, T_{max}, \\, C_{max}, \\, C_{trough},\\, \\ldots\\)).\nNow we’ll do the simulations with the $generate_quantities() method (as opposed to the $sample() method we used for fitting). Because we are making predictions for observed individuals, we need to make sure the dosing schemes match the original data set used for fitting, so we use the original nonmem_data_bloq data set created before the fitting and manipulate it to add in rows for new timepoints at which we want to simulate:\n\n\nCode\nnew_data_to_simulate <- nonmem_data_bloq %>%\n  # filter(evid == 0) %>%\n  group_by(ID) %>%\n  slice(c(1, n())) %>%\n  expand(time = seq(time[1], time[2], by = 1)) %>%\n  ungroup() %>%\n  mutate(amt = 0,\n         evid = 2,\n         rate = 0,\n         addl = 0,\n         ii = 0,\n         cmt = 2,\n         mdv = 1,\n         ss = 0,\n         DV = NA_real_,\n         c = NA_character_) %>%\n  select(c, ID, time, everything())\n\nobserved_data_to_simulate <- nonmem_data_bloq %>%\n  mutate(evid = if_else(evid == 1, 1, 2))\n\nend_of_infusion_to_simulate <- nonmem_data_bloq %>%\n  filter(evid == 1) %>%\n  rename_all(tolower) %>%\n  rename(ID = \"id\",\n         DV = \"dv\") %>%\n  mutate(tinf = amt/rate,\n         time = time + tinf,\n         evid = 2,\n         amt = 0,\n         rate = 0,\n         ii = 0,\n         addl = 0,\n         ss = 0)\n\nnew_data <- new_data_to_simulate %>%\n  bind_rows(observed_data_to_simulate, end_of_infusion_to_simulate) %>%\n  arrange(ID, time, evid) %>%\n  distinct(ID, time, .keep_all = TRUE) %>%\n  select(-DV, -timenom, -lloq, -bloq, -tinf, -mdv)\n\n\nn_subjects <- new_data %>%  # number of individuals\n  distinct(ID) %>%\n  count() %>%\n  deframe()\n\nn_time_new <- nrow(new_data) # total number of time points at which to predict\n\nsubj_start <- new_data %>%\n  mutate(row_num = 1:n()) %>%\n  group_by(ID) %>%\n  slice_head(n = 1) %>%\n  ungroup() %>%\n  select(row_num) %>%\n  deframe()\n\nsubj_end <- c(subj_start[-1] - 1, n_time_new)\n\nstan_data <- list(n_subjects = n_subjects,\n                  n_time_new = n_time_new,\n                  time = new_data$time,\n                  amt = new_data$amt,\n                  cmt = new_data$cmt,\n                  evid = new_data$evid,\n                  rate = new_data$rate,\n                  ii = new_data$ii,\n                  addl = new_data$addl,\n                  ss = new_data$ss,\n                  subj_start = subj_start,\n                  subj_end = subj_end)\n\npreds_obs <- model_pred_obs$generate_quantities(fit_m4,\n                                                data = stan_data,\n                                                parallel_chains = 4,\n                                                seed = 1234)\n\n\nRunning standalone generated quantities after 4 MCMC chains, all chains in parallel ...\n\nChain 4 finished in 0.0 seconds.\nChain 1 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 128.4 seconds.\n\n\nCode\n# preds_obs$save_object(\n#   \"Torsten/Preds/iv_2cmt_ppa_m4_predict_observed_subjects.rds\")\n\n\nAnd we can look at the predictions. They should look smoother than the PPC plot from earlier:\n\n\n\nCode\npreds_obs_df <- preds_obs$draws(format = \"draws_df\")\n\npost_preds_obs_summary <- preds_obs_df %>%\n  spread_draws(pred[i], ipred[i], dv[i]) %>%\n  mean_qi(pred, ipred, dv)\n\npreds_obs_ind <- post_preds_obs_summary %>%\n  mutate(ID = new_data$ID[i],\n         time = new_data$time[i]) %>%\n  left_join(nonmem_data_bloq %>%\n              filter(mdv == 0) %>%\n              select(ID, time, DV),\n            by = c(\"ID\", \"time\")) %>%\n  select(ID, time, everything(), -i)\n\npreds_obs_ind %>%\n  ggplot(aes(x = time, group = ID)) +\n  geom_ribbon(aes(ymin = ipred.lower, ymax = ipred.upper),\n              fill = \"blue\", alpha = 0.5, show.legend = FALSE) +\n  geom_ribbon(aes(ymin = dv.lower, ymax = dv.upper),\n              fill = \"blue\", alpha = 0.25, show.legend = FALSE) +\n  geom_line(aes(y = ipred), linetype = 1, size = 1.15) +\n  geom_line(aes(y = pred), linetype = 2, size = 1.05) +\n  geom_point(aes(y = DV),\n             size = 3, color = \"red\", show.legend = FALSE) +\n  scale_y_log10(name = \"Drug Conc. (ug/mL)\",\n                limits = c(NA, NA)) +\n  scale_x_continuous(name = \"Time (w)\",\n                     breaks = seq(0, 168, by = 28),\n                     labels = seq(0, 168/7, by = 4)) +\n  theme_bw() +\n  theme(axis.text = element_text(size = 14, face = \"bold\"),\n        axis.title = element_text(size = 18, face = \"bold\"),\n        legend.position = \"bottom\") +\n  facet_wrap(~ID, labeller = label_both,\n             ncol = 4, scales = \"free\")\n\n\n\n\n\n\n\n\nAs mentioned previously, it might make sense to go ahead and simulate on a dense time grid for the observed individuals at the same time as fitting, We basically combine what we did in Section 3.4.3 when we fit the model with what we did in the previous section when we made predictions for observed individuals.\nLet’s look at the .stan file:\n\n\nCode\nmodel_fit_and_predict <- cmdstan_model(\n  \"Torsten/Fit_and_Predict/iv_2cmt_ppa_m4_fit_and_predict.stan\",\n  cpp_options = list(stan_threads = TRUE))\n\nmodel_fit_and_predict$print()\n\n\n// IV Infusion\n// Two-compartment PK Model\n// IIV on CL, VC, Q, VP (full covariance matrix)\n// proportional plus additive error - DV = CP(1 + eps_p) + eps_a\n// Closed form solution using Torsten\n// Implements threading for within-chain parallelization\n// Deals with BLOQ values by the \"CDF trick\" (M4)\n// Since we have a normal distribution on the error, but the DV must be > 0, it\n//   truncates the likelihood below at 0\n// For PPC, it generates values from a normal that is truncated below at 0\n\n\nfunctions{\n\n  array[] int sequence(int start, int end) { \n    array[end - start + 1] int seq;\n    for (n in 1:num_elements(seq)) {\n      seq[n] = n + start - 1;\n    }\n    return seq; \n  } \n  \n  int num_between(int lb, int ub, array[] int y){\n    \n    int n = 0;\n    for(i in 1:num_elements(y)){\n      if(y[i] >= lb && y[i] <= ub)\n                    n = n + 1;\n    }\n    return n;\n    \n  }\n  \n  array[] int find_between(int lb, int ub, array[] int y) {\n    // vector[num_between(lb, ub, y)] result;\n    array[num_between(lb, ub, y)] int result;\n    int n = 1;\n    for (i in 1:num_elements(y)) {\n      if (y[i] >= lb && y[i] <= ub) {\n        result[n] = y[i];\n        n = n + 1;\n      }\n    }\n    return result;\n  }\n  \n  vector find_between_vec(int lb, int ub, array[] int idx, vector y) {\n    \n    vector[num_between(lb, ub, idx)] result;\n    int n = 1;\n    if(num_elements(idx) != num_elements(y)) reject(\"illegal input\");\n    for (i in 1:rows(y)) {\n      if (idx[i] >= lb && idx[i] <= ub) {\n        result[n] = y[i];\n        n = n + 1;\n      }\n    }\n    return result;\n  }\n  \n  real normal_lb_rng(real mu, real sigma, real lb){\n    \n    real p_lb = normal_cdf(lb | mu, sigma);\n    real u = uniform_rng(p_lb, 1);\n    real y = mu + sigma * inv_Phi(u);\n    return y;\n\n  }\n  \n  real partial_sum_lpmf(array[] int seq_subj, int start, int end,\n                        vector dv_obs, array[] int dv_obs_id, array[] int i_obs,\n                        array[] real amt, array[] int cmt, array[] int evid, \n                        array[] real time, array[] real rate, array[] real ii, \n                        array[] int addl, array[] int ss,\n                        array[] int subj_start, array[] int subj_end, \n                        real TVCL, real TVVC, real TVQ, real TVVP, \n                        vector omega, matrix L, matrix Z, \n                        vector sigma, matrix L_Sigma,\n                        vector lloq, array[] int bloq,\n                        int n_random, int n_subjects, int n_total){\n                           \n    real ptarget = 0;\n    row_vector[n_random] typical_values = to_row_vector({TVCL, TVVC, TVQ, TVVP});\n    \n    matrix[n_subjects, n_random] eta = diag_pre_multiply(omega, L * Z)';\n\n    matrix[n_subjects, n_random] theta =\n                          (rep_matrix(typical_values, n_subjects) .* exp(eta));\n                          \n    matrix[2, 2] R_Sigma = multiply_lower_tri_self_transpose(L_Sigma);\n    matrix[2, 2] Sigma = quad_form_diag(R_Sigma, sigma);\n    \n                              \n    int N = end - start + 1;    // number of subjects in this slice  \n    vector[n_total] dv_ipred;   \n    matrix[n_total, 3] x_ipred; \n  \n\n    int n_obs_slice = num_between(subj_start[start], subj_end[end], i_obs);\n    array[n_obs_slice] int i_obs_slice = find_between(subj_start[start], \n                                                      subj_end[end], i_obs);\n                                                \n    vector[n_obs_slice] dv_obs_slice = find_between_vec(start, end, \n                                                        dv_obs_id, dv_obs);\n    \n    vector[n_obs_slice] ipred_slice;\n    \n    vector[n_obs_slice] lloq_slice = lloq[i_obs_slice];\n    array[n_obs_slice] int bloq_slice = bloq[i_obs_slice];\n    \n    \n    for(n in 1:N){            // loop over subjects in this slice\n    \n      int nn = n + start - 1; // nn is the ID of the current subject\n      \n      row_vector[n_random] theta_nn = theta[nn]; // access the parameters for subject nn\n      real cl = theta_nn[1];\n      real vc = theta_nn[2];\n      real q = theta_nn[3];\n      real vp = theta_nn[4];\n      array[5] real theta_params = {cl, q, vc, vp, 0}; // The 0 is for KA. Skip the absorption\n      \n      x_ipred[subj_start[nn]:subj_end[nn], ] =\n        pmx_solve_twocpt(time[subj_start[nn]:subj_end[nn]],\n                         amt[subj_start[nn]:subj_end[nn]],\n                         rate[subj_start[nn]:subj_end[nn]],\n                         ii[subj_start[nn]:subj_end[nn]],\n                         evid[subj_start[nn]:subj_end[nn]],\n                         cmt[subj_start[nn]:subj_end[nn]],\n                         addl[subj_start[nn]:subj_end[nn]],\n                         ss[subj_start[nn]:subj_end[nn]],\n                         theta_params)';\n                      \n      dv_ipred[subj_start[nn]:subj_end[nn]] = \n        x_ipred[subj_start[nn]:subj_end[nn], 2] ./ vc;\n    \n    }\n  \n    \n    ipred_slice = dv_ipred[i_obs_slice];\n    \n    for(i in 1:n_obs_slice){\n      real sigma_tmp = sqrt(square(ipred_slice[i]) * Sigma[1, 1] +\n                                         Sigma[2, 2] + \n                                         2*ipred_slice[i]*Sigma[2, 1]);\n      if(bloq_slice[i] == 1){\n        // ptarget += log(normal_cdf(lloq_slice[i] | ipred_slice[i], sigma_tmp) -\n        //                normal_cdf(0.0 | ipred_slice[i], sigma_tmp)) -\n        //            normal_lccdf(0.0 | ipred_slice[i], sigma_tmp);  \n        ptarget += log_diff_exp(normal_lcdf(lloq_slice[i] | ipred_slice[i], \n                                                            sigma_tmp),\n                                normal_lcdf(0.0 | ipred_slice[i], sigma_tmp)) -\n                   normal_lccdf(0.0 | ipred_slice[i], sigma_tmp); \n      }else{\n        ptarget += normal_lpdf(dv_obs_slice[i] | ipred_slice[i], sigma_tmp) -\n                   normal_lccdf(0.0 | ipred_slice[i], sigma_tmp);\n      }\n    }                                         \n                              \n    return ptarget;\n                           \n  }\n  \n}\ndata{\n  \n  int n_subjects;\n  int n_total;\n  int n_obs;\n  array[n_obs] int i_obs;\n  array[n_total] int ID;\n  array[n_total] real amt;\n  array[n_total] int cmt;\n  array[n_total] int evid;\n  array[n_total] real rate;\n  array[n_total] real ii;\n  array[n_total] int addl;\n  array[n_total] int ss;\n  array[n_total] real time;\n  vector<lower = 0>[n_total] dv;\n  array[n_subjects] int subj_start;\n  array[n_subjects] int subj_end;\n  vector[n_total] lloq;\n  array[n_total] int bloq;\n  \n  real<lower = 0> scale_tvcl;      // Prior Scale parameter for CL\n  real<lower = 0> scale_tvvc;      // Prior Scale parameter for VC\n  real<lower = 0> scale_tvq;       // Prior Scale parameter for Q\n  real<lower = 0> scale_tvvp;      // Prior Scale parameter for VP\n  \n  real<lower = 0> scale_omega_cl; // Prior scale parameter for omega_cl\n  real<lower = 0> scale_omega_vc; // Prior scale parameter for omega_vc\n  real<lower = 0> scale_omega_q;  // Prior scale parameter for omega_q\n  real<lower = 0> scale_omega_vp; // Prior scale parameter for omega_vp\n  \n  real<lower = 0> lkj_df_omega;   // Prior degrees of freedom for omega cor mat\n  \n  real<lower = 0> scale_sigma_p;  // Prior Scale parameter for proportional error\n  real<lower = 0> scale_sigma_a;  // Prior Scale parameter for additive error\n  real<lower = 0> lkj_df_sigma;   // Prior degrees of freedom for sigma cor mat\n  \n  // These are data variables needed to make predictions for observed subjects \n  // at unobserved timepoints\n  int n_time_new_obs;\n  array[n_time_new_obs] real time_new_obs;\n  array[n_time_new_obs] real amt_new_obs;\n  array[n_time_new_obs] int cmt_new_obs;\n  array[n_time_new_obs] int evid_new_obs;\n  array[n_time_new_obs] real rate_new_obs;\n  array[n_time_new_obs] real ii_new_obs;\n  array[n_time_new_obs] int addl_new_obs;\n  array[n_time_new_obs] int ss_new_obs;\n  array[n_subjects] int subj_start_new_obs;\n  array[n_subjects] int subj_end_new_obs;\n \n}\ntransformed data{ \n  \n  int grainsize = 1;\n  \n  vector<lower = 0>[n_obs] dv_obs = dv[i_obs];\n  array[n_obs] int dv_obs_id = ID[i_obs];\n  \n  vector[n_obs] lloq_obs = lloq[i_obs];\n  array[n_obs] int bloq_obs = bloq[i_obs];\n  \n  int n_random = 4;                    // Number of random effects\n  \n  array[n_random] real scale_omega = {scale_omega_cl, scale_omega_vc, \n                                      scale_omega_q, scale_omega_vp}; \n  array[2] real scale_sigma = {scale_sigma_p, scale_sigma_a};\n  \n  array[n_subjects] int seq_subj = sequence(1, n_subjects); // reduce_sum over subjects\n  \n}\nparameters{ \n  \n  real<lower = 0> TVCL;       \n  real<lower = 0> TVVC; \n  real<lower = 0> TVQ;\n  real<lower = 0> TVVP; \n  \n  vector<lower = 0>[n_random] omega;\n  cholesky_factor_corr[n_random] L;\n  \n  vector<lower = 0>[2] sigma;\n  cholesky_factor_corr[2] L_Sigma;\n  \n  matrix[n_random, n_subjects] Z;\n  \n}\n\nmodel{ \n  \n  // Priors\n  TVCL ~ cauchy(0, scale_tvcl);\n  TVVC ~ cauchy(0, scale_tvvc);\n  TVQ ~ cauchy(0, scale_tvq);\n  TVVP ~ cauchy(0, scale_tvvp);\n\n  omega ~ normal(0, scale_omega);\n  L ~ lkj_corr_cholesky(lkj_df_omega);\n  \n  sigma ~ normal(0, scale_sigma);\n  L_Sigma ~ lkj_corr_cholesky(lkj_df_sigma);\n  \n  to_vector(Z) ~ std_normal();\n  \n  // Likelihood\n  target += reduce_sum(partial_sum_lupmf, seq_subj, grainsize,\n                       dv_obs, dv_obs_id, i_obs,\n                       amt, cmt, evid, time, \n                       rate, ii, addl, ss, subj_start, subj_end, \n                       TVCL, TVVC, TVQ, TVVP, omega, L, Z,\n                       sigma, L_Sigma, \n                       lloq, bloq,\n                       n_random, n_subjects, n_total);\n}\ngenerated quantities{\n  \n  real<lower = 0> sigma_p = sigma[1];\n  real<lower = 0> sigma_a = sigma[2];\n  \n  real<lower = 0> sigma_sq_p = square(sigma_p);\n  real<lower = 0> sigma_sq_a = square(sigma_a); \n\n  real<lower = 0> omega_cl = omega[1];\n  real<lower = 0> omega_vc = omega[2];\n  real<lower = 0> omega_q = omega[3];\n  real<lower = 0> omega_vp = omega[4];\n\n  real<lower = 0> omega_sq_cl = square(omega_cl);\n  real<lower = 0> omega_sq_vc = square(omega_vc);\n  real<lower = 0> omega_sq_q = square(omega_q);\n  real<lower = 0> omega_sq_vp = square(omega_vp);\n\n  real cor_cl_vc;\n  real cor_cl_q;\n  real cor_cl_vp;\n  real cor_vc_q;\n  real cor_vc_vp;\n  real cor_q_vp;\n  real omega_cl_vc;\n  real omega_cl_q;\n  real omega_cl_vp;\n  real omega_vc_q;\n  real omega_vc_vp;\n  real omega_q_vp;\n\n  real cor_p_a;\n  real sigma_p_a;\n\n  vector[n_subjects] eta_cl;\n  vector[n_subjects] eta_vc;\n  vector[n_subjects] eta_q;\n  vector[n_subjects] eta_vp;\n  vector[n_subjects] CL;\n  vector[n_subjects] VC;\n  vector[n_subjects] Q;\n  vector[n_subjects] VP;\n  \n  vector[n_obs] ipred;\n  vector[n_obs] pred;\n  vector[n_obs] dv_ppc;\n  vector[n_obs] log_lik;\n  vector[n_obs] res;\n  vector[n_obs] wres;\n  vector[n_obs] ires;\n  vector[n_obs] iwres;\n  \n  vector[n_time_new_obs] ipred_new_obs; // ipred for the observed individuals at the new timepoints\n  vector[n_time_new_obs] pred_new_obs;  // pred for the observed individuals at the new timepoints\n  vector[n_time_new_obs] dv_new_obs;    // dv for the observed individuals at the new timepoints\n \n  {\n    row_vector[n_random] typical_values = to_row_vector({TVCL, TVVC, TVQ, TVVP});\n\n    matrix[n_random, n_random] R = multiply_lower_tri_self_transpose(L);\n    matrix[n_random, n_random] Omega = quad_form_diag(R, omega);\n\n    matrix[2, 2] R_Sigma = multiply_lower_tri_self_transpose(L_Sigma);\n    matrix[2, 2] Sigma = quad_form_diag(R_Sigma, sigma);\n\n    matrix[n_subjects, n_random] eta = diag_pre_multiply(omega, L * Z)';\n\n    matrix[n_subjects, n_random] theta =\n                          (rep_matrix(typical_values, n_subjects) .* exp(eta));\n\n    vector[n_total] dv_pred;\n    matrix[n_total, 3] x_pred;\n    vector[n_total] dv_ipred;\n    matrix[n_total, 3] x_ipred;\n    \n    matrix[n_time_new_obs, 3] x_pred_new_obs;\n    matrix[n_time_new_obs, 3] x_ipred_new_obs;\n    \n    array[5] real theta_params_tv = {TVCL, TVQ, TVVC, TVVP, 0}; // 0 is for KA to skip absorption\n    \n    eta_cl = col(eta, 1);\n    eta_vc = col(eta, 2);\n    eta_q = col(eta, 3);\n    eta_vp = col(eta, 4);\n    \n    CL = col(theta, 1);\n    VC = col(theta, 2);\n    Q = col(theta, 3);\n    VP = col(theta, 4);\n\n    cor_cl_vc = R[1, 2];\n    cor_cl_q = R[1, 3];\n    cor_cl_vp = R[1, 4];\n    cor_vc_q = R[2, 3];\n    cor_vc_vp = R[2, 4];\n    cor_q_vp = R[3, 4];\n    omega_cl_vc = Omega[1, 2];\n    omega_cl_q = Omega[1, 3];\n    omega_cl_vp = Omega[1, 4];\n    omega_vc_q = Omega[2, 3];\n    omega_vc_vp = Omega[2, 4];\n    omega_q_vp = Omega[3, 4];\n\n    cor_p_a = R_Sigma[2, 1];\n    sigma_p_a = Sigma[2, 1];\n\n    for(j in 1:n_subjects){\n\n      array[5] real theta_params = {CL[j], Q[j], VC[j], VP[j], 0};\n      \n      x_ipred[subj_start[j]:subj_end[j],] =\n        pmx_solve_twocpt(time[subj_start[j]:subj_end[j]],\n                         amt[subj_start[j]:subj_end[j]],\n                         rate[subj_start[j]:subj_end[j]],\n                         ii[subj_start[j]:subj_end[j]],\n                         evid[subj_start[j]:subj_end[j]],\n                         cmt[subj_start[j]:subj_end[j]],\n                         addl[subj_start[j]:subj_end[j]],\n                         ss[subj_start[j]:subj_end[j]],\n                         theta_params)';\n                      \n                      \n      dv_ipred[subj_start[j]:subj_end[j]] = \n        x_ipred[subj_start[j]:subj_end[j], 2] ./ VC[j];\n\n      x_pred[subj_start[j]:subj_end[j],] =\n        pmx_solve_twocpt(time[subj_start[j]:subj_end[j]],\n                         amt[subj_start[j]:subj_end[j]],\n                         rate[subj_start[j]:subj_end[j]],\n                         ii[subj_start[j]:subj_end[j]],\n                         evid[subj_start[j]:subj_end[j]],\n                         cmt[subj_start[j]:subj_end[j]],\n                         addl[subj_start[j]:subj_end[j]],\n                         ss[subj_start[j]:subj_end[j]],\n                         theta_params_tv)';\n\n      dv_pred[subj_start[j]:subj_end[j]] = \n        x_pred[subj_start[j]:subj_end[j], 2] ./ TVVC;\n        \n        x_ipred_new_obs[subj_start_new_obs[j]:subj_end_new_obs[j],] =\n        pmx_solve_twocpt(time_new_obs[subj_start_new_obs[j]:subj_end_new_obs[j]],\n                         amt_new_obs[subj_start_new_obs[j]:subj_end_new_obs[j]],\n                         rate_new_obs[subj_start_new_obs[j]:subj_end_new_obs[j]],\n                         ii_new_obs[subj_start_new_obs[j]:subj_end_new_obs[j]],\n                         evid_new_obs[subj_start_new_obs[j]:subj_end_new_obs[j]],\n                         cmt_new_obs[subj_start_new_obs[j]:subj_end_new_obs[j]],\n                         addl_new_obs[subj_start_new_obs[j]:subj_end_new_obs[j]],\n                         ss_new_obs[subj_start_new_obs[j]:subj_end_new_obs[j]],\n                         theta_params)';\n                      \n      ipred_new_obs[subj_start_new_obs[j]:subj_end_new_obs[j]] = \n        x_ipred_new_obs[subj_start_new_obs[j]:subj_end_new_obs[j], 2] ./ VC[j];\n\n      x_pred_new_obs[subj_start_new_obs[j]:subj_end_new_obs[j],] =\n        pmx_solve_twocpt(time_new_obs[subj_start_new_obs[j]:subj_end_new_obs[j]],\n                         amt_new_obs[subj_start_new_obs[j]:subj_end_new_obs[j]],\n                         rate_new_obs[subj_start_new_obs[j]:subj_end_new_obs[j]],\n                         ii_new_obs[subj_start_new_obs[j]:subj_end_new_obs[j]],\n                         evid_new_obs[subj_start_new_obs[j]:subj_end_new_obs[j]],\n                         cmt_new_obs[subj_start_new_obs[j]:subj_end_new_obs[j]],\n                         addl_new_obs[subj_start_new_obs[j]:subj_end_new_obs[j]],\n                         ss_new_obs[subj_start_new_obs[j]:subj_end_new_obs[j]],\n                         theta_params_tv)';\n\n      pred_new_obs[subj_start_new_obs[j]:subj_end_new_obs[j]] = \n        x_pred_new_obs[subj_start_new_obs[j]:subj_end_new_obs[j], 2] ./ TVVC;\n    }\n\n    pred = dv_pred[i_obs];\n    ipred = dv_ipred[i_obs];\n\n  }\n\n  res = dv_obs - pred;                                                                             \n  ires = dv_obs - ipred;\n\n  for(i in 1:n_obs){\n    real ipred_tmp = ipred[i];\n    real sigma_tmp = sqrt(square(ipred_tmp) * sigma_sq_p + sigma_sq_a + \n                          2*ipred_tmp*sigma_p_a);\n    // dv_ppc[i] = normal_rng(ipred_tmp, sigma_tmp);\n    dv_ppc[i] = normal_lb_rng(ipred_tmp, sigma_tmp, 0.0);\n    if(bloq_obs[i] == 1){\n      // log_lik[i] = log(normal_cdf(lloq_obs[i] | ipred_tmp, sigma_tmp) - \n      //                  normal_cdf(0.0 | ipred_tmp, sigma_tmp)) -\n      //              normal_lccdf(0.0 | ipred_tmp, sigma_tmp);\n      log_lik[i] = log_diff_exp(normal_lcdf(lloq_obs[i] | ipred_tmp, sigma_tmp),\n                                normal_lcdf(0.0 | ipred_tmp, sigma_tmp)) -\n                   normal_lccdf(0.0 | ipred_tmp, sigma_tmp); \n    }else{\n      log_lik[i] = normal_lpdf(dv_obs[i] | ipred_tmp, sigma_tmp) - \n                   normal_lccdf(0.0 | ipred_tmp, sigma_tmp);\n    }\n    wres[i] = res[i]/sigma_tmp;\n    iwres[i] = ires[i]/sigma_tmp;\n  }\n  \n  for(i in 1:n_time_new_obs){\n    if(ipred_new_obs[i] == 0){\n      dv_new_obs[i] = 0;\n    }else{\n      real ipred_tmp = ipred_new_obs[i];\n      real sigma_tmp = sqrt(square(ipred_tmp) * sigma_sq_p + sigma_sq_a + \n                            2*ipred_tmp*sigma_p_a);\n      dv_new_obs[i] = normal_lb_rng(ipred_tmp, sigma_tmp, 0.0); \n    }\n    \n  }\n  \n}\n\n\nWe must prepare both the original data for fitting and also create the new dataset with added rows for new time points at which we want to simulate. Then we can look at the posterior summary, and we can look at the predictions. They should look smoother than the PPC plot from earlier:\n\n\nCode\nn_subjects <- nonmem_data_bloq %>%  # number of individuals\n  distinct(ID) %>% \n  count() %>%\n  deframe() \n\nn_total <- nrow(nonmem_data_bloq)   # total number of records\n\ni_obs <- nonmem_data_bloq %>%\n  mutate(row_num = 1:n()) %>%\n  filter(evid == 0) %>%\n  select(row_num) %>%\n  deframe()\n\nn_obs <- length(i_obs)\n\nsubj_start <- nonmem_data_bloq %>%\n  mutate(row_num = 1:n()) %>%\n  group_by(ID) %>%\n  slice_head(n = 1) %>%\n  ungroup() %>%\n  select(row_num) %>%\n  deframe()\n\nsubj_end <- c(subj_start[-1] - 1, n_total)\n\nnew_data_to_simulate <- nonmem_data_bloq %>%\n  # filter(evid == 0) %>%\n  group_by(ID) %>%\n  slice(c(1, n())) %>%\n  expand(time = seq(time[1], time[2], by = 1)) %>%\n  ungroup() %>%\n  mutate(amt = 0,\n         evid = 2,\n         rate = 0,\n         addl = 0,\n         ii = 0,\n         cmt = 2,\n         mdv = 1,\n         ss = 0,\n         DV = NA_real_,\n         c = NA_character_) %>%\n  select(c, ID, time, everything())\n\nobserved_data_to_simulate <- nonmem_data_bloq %>%\n  mutate(evid = if_else(evid == 1, 1, 2))\n\nend_of_infusion_to_simulate <- nonmem_data_bloq %>%\n  filter(evid == 1) %>%\n  mutate(tinf = amt/rate,\n         time = time + tinf,\n         evid = 2,\n         amt = 0,\n         rate = 0,\n         ii = 0,\n         addl = 0,\n         ss = 0)\n\nnew_data <- new_data_to_simulate %>%\n  bind_rows(observed_data_to_simulate, end_of_infusion_to_simulate) %>%\n  arrange(ID, time, evid) %>%\n  distinct(ID, time, .keep_all = TRUE) %>%\n  select(-DV, -timenom, -lloq, -bloq, -tinf, -mdv)\n\nn_time_new_obs <- nrow(new_data) # total number of time points at which to predict\n\nsubj_start_new_obs <- new_data %>%\n  mutate(row_num = 1:n()) %>%\n  group_by(ID) %>%\n  slice_head(n = 1) %>%\n  ungroup() %>%\n  select(row_num) %>%\n  deframe()\n\nsubj_end_new_obs <- c(subj_start_new_obs[-1] - 1, n_time_new_obs)\n\nstan_data <- list(n_subjects = n_subjects,\n                  n_total = n_total,\n                  n_obs = n_obs,\n                  i_obs = i_obs,\n                  ID = nonmem_data_bloq$ID,\n                  amt = nonmem_data_bloq$amt,\n                  cmt = nonmem_data_bloq$cmt,\n                  evid = nonmem_data_bloq$evid,\n                  rate = nonmem_data_bloq$rate,\n                  ii = nonmem_data_bloq$ii,\n                  addl = nonmem_data_bloq$addl,\n                  ss = nonmem_data_bloq$ss,\n                  time = nonmem_data_bloq$time,\n                  dv = nonmem_data_bloq$DV,\n                  lloq = nonmem_data_bloq$lloq,\n                  bloq = nonmem_data_bloq$bloq,\n                  subj_start = subj_start,\n                  subj_end = subj_end,\n                  scale_tvcl = 2,\n                  scale_tvvc = 10,\n                  scale_tvq = 2,\n                  scale_tvvp = 20,\n                  scale_tvka = 1,\n                  scale_omega_cl = 0.4,\n                  scale_omega_vc = 0.4,\n                  scale_omega_q = 0.4,\n                  scale_omega_vp = 0.4,\n                  scale_omega_ka = 0.4,\n                  lkj_df_omega = 2,\n                  scale_sigma_p = 0.5,\n                  scale_sigma_a = 1,\n                  lkj_df_sigma = 2,\n                  n_time_new_obs = n_time_new_obs,\n                  time_new_obs = new_data$time,\n                  amt_new_obs = new_data$amt,\n                  cmt_new_obs = new_data$cmt,\n                  evid_new_obs = new_data$evid,\n                  rate_new_obs = new_data$rate,\n                  ii_new_obs = new_data$ii,\n                  addl_new_obs = new_data$addl,\n                  ss_new_obs = new_data$ss,\n                  subj_start_new_obs = subj_start_new_obs,\n                  subj_end_new_obs = subj_end_new_obs)\n\nfit_with_preds <- model_fit_and_predict$sample(\n  data = stan_data,\n  chains = 4,\n  parallel_chains = 4,\n  threads_per_chain = 2,\n  iter_warmup = 500,\n  iter_sampling = 1000,\n  adapt_delta = 0.8,\n  refresh = 500,\n  max_treedepth = 10,\n  init = function() list(TVCL = rlnorm(1, log(0.3), 0.3),\n                         TVVC = rlnorm(1, log(5), 0.3),\n                         TVQ = rlnorm(1, log(2), 0.3),\n                         TVVP = rlnorm(1, log(5), 0.3),\n                         omega = rlnorm(5, log(0.3), 0.3),\n                         sigma = rlnorm(2, log(0.5), 0.3)))\n\n\nRunning MCMC with 4 parallel chains, with 2 thread(s) per chain...\n\nChain 1 Iteration:    1 / 1500 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 1500 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 1500 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 1500 [  0%]  (Warmup) \nChain 3 Iteration:  500 / 1500 [ 33%]  (Warmup) \nChain 3 Iteration:  501 / 1500 [ 33%]  (Sampling) \nChain 2 Iteration:  500 / 1500 [ 33%]  (Warmup) \nChain 2 Iteration:  501 / 1500 [ 33%]  (Sampling) \nChain 1 Iteration:  500 / 1500 [ 33%]  (Warmup) \nChain 1 Iteration:  501 / 1500 [ 33%]  (Sampling) \nChain 3 Iteration: 1000 / 1500 [ 66%]  (Sampling) \nChain 4 Iteration:  500 / 1500 [ 33%]  (Warmup) \nChain 4 Iteration:  501 / 1500 [ 33%]  (Sampling) \nChain 2 Iteration: 1000 / 1500 [ 66%]  (Sampling) \nChain 1 Iteration: 1000 / 1500 [ 66%]  (Sampling) \nChain 3 Iteration: 1500 / 1500 [100%]  (Sampling) \nChain 3 finished in 2492.3 seconds.\nChain 2 Iteration: 1500 / 1500 [100%]  (Sampling) \nChain 2 finished in 2587.7 seconds.\nChain 4 Iteration: 1000 / 1500 [ 66%]  (Sampling) \nChain 1 Iteration: 1500 / 1500 [100%]  (Sampling) \nChain 1 finished in 2880.3 seconds.\nChain 4 Iteration: 1500 / 1500 [100%]  (Sampling) \nChain 4 finished in 3047.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 2751.9 seconds.\nTotal execution time: 3048.8 seconds.\n\n\nCode\nsummary_with_preds <- summarize_draws(\n  fit_with_preds$draws(parameters_to_summarize),\n  mean, median, sd, mcse_mean,\n  ~quantile2(.x, probs = c(0.025, 0.975)), rhat,\n  ess_bulk, ess_tail)\n\nsummary_with_preds %>%\n  mutate(rse = sd/mean*100) %>%\n  select(variable, mean, sd, rse, q2.5, median, q97.5, rhat,\n         starts_with(\"ess\")) %>%\n  inner_join(neff_ratio(fit_with_preds, pars = parameters_to_summarize) %>%\n               enframe(name = \"variable\", value = \"n_eff_ratio\")) %>%\n  mutate(across(where(is.numeric), round, 3)) %>%\n  knitr::kable(col.names = c(\"Variable\", \"Mean\", \"Std. Dev.\", \"RSE\", \"2.5%\",\n                             \"Median\", \"97.5%\", \"$\\\\hat{R}$\", \"ESS Bulk\",\n                             \"ESS Tail\", \"ESS Ratio\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nMean\nStd. Dev.\nRSE\n2.5%\nMedian\n97.5%\n\\(\\hat{R}\\)\nESS Bulk\nESS Tail\nESS Ratio\n\n\n\n\nTVCL\n0.194\n0.015\n7.602\n0.166\n0.193\n0.225\n1.003\n482.124\n888.994\n0.122\n\n\nTVVC\n3.167\n0.199\n6.277\n2.805\n3.161\n3.594\n1.001\n729.424\n1190.699\n0.180\n\n\nTVQ\n1.468\n0.145\n9.871\n1.209\n1.460\n1.776\n1.000\n3737.598\n2624.715\n0.879\n\n\nTVVP\n4.087\n0.197\n4.823\n3.703\n4.088\n4.467\n1.001\n4798.689\n2900.526\n1.194\n\n\nomega_cl\n0.356\n0.059\n16.517\n0.262\n0.349\n0.494\n1.006\n745.246\n1491.696\n0.190\n\n\nomega_vc\n0.272\n0.047\n17.196\n0.194\n0.267\n0.374\n1.003\n1217.847\n2074.456\n0.312\n\n\nomega_q\n0.126\n0.100\n79.281\n0.004\n0.102\n0.371\n1.003\n1404.111\n1233.373\n0.416\n\n\nomega_vp\n0.082\n0.060\n73.038\n0.004\n0.072\n0.222\n1.001\n1271.662\n1392.455\n0.358\n\n\nsigma_p\n0.202\n0.009\n4.343\n0.186\n0.202\n0.220\n1.001\n4318.280\n2757.096\n1.094\n\n\nsigma_a\n0.093\n0.064\n68.396\n0.003\n0.082\n0.235\n1.001\n1770.485\n1646.260\n0.526\n\n\ncor_cl_vc\n0.185\n0.195\n105.153\n-0.206\n0.193\n0.549\n1.004\n905.967\n1726.624\n0.228\n\n\ncor_cl_q\n0.077\n0.340\n439.067\n-0.601\n0.088\n0.720\n1.001\n5251.666\n2731.927\n1.310\n\n\ncor_cl_vp\n-0.018\n0.336\n-1867.506\n-0.631\n-0.027\n0.635\n1.000\n4524.073\n2463.419\n1.137\n\n\ncor_vc_q\n0.143\n0.364\n253.916\n-0.611\n0.169\n0.769\n1.000\n4428.347\n3164.363\n1.069\n\n\ncor_vc_vp\n-0.075\n0.345\n-457.333\n-0.707\n-0.080\n0.615\n1.000\n4922.227\n2948.907\n1.235\n\n\ncor_q_vp\n-0.061\n0.386\n-631.944\n-0.736\n-0.073\n0.707\n1.001\n2870.129\n2643.677\n0.713\n\n\ncor_p_a\n-0.030\n0.335\n-1099.788\n-0.553\n-0.071\n0.687\n1.001\n3795.925\n2565.835\n0.913\n\n\n\n\n\nCode\npreds_obs_df_2 <- fit_with_preds$draws(c(\"ipred_new_obs\", \"pred_new_obs\",\n                                         \"dv_new_obs\"),\n                      format = \"draws_df\") %>%\n  rename_with(~str_remove(., \"_new_obs\"))\n\npost_preds_obs_summary_2 <- preds_obs_df_2 %>%\n  spread_draws(pred[i], ipred[i], dv[i]) %>%\n  mean_qi(pred, ipred, dv)\n\npreds_obs_ind_2 <- post_preds_obs_summary_2 %>%\n  mutate(ID = new_data$ID[i],\n         time = new_data$time[i]) %>%\n  left_join(nonmem_data_bloq %>%\n              filter(mdv == 0) %>%\n              select(ID, time, DV),\n            by = c(\"ID\", \"time\")) %>%\n  select(ID, time, everything(), -i)\n\npreds_obs_ind_2 %>%\n  ggplot(aes(x = time, group = ID)) +\n  geom_ribbon(aes(ymin = ipred.lower, ymax = ipred.upper),\n              fill = \"blue\", alpha = 0.5, show.legend = FALSE) +\n  geom_ribbon(aes(ymin = dv.lower, ymax = dv.upper),\n              fill = \"blue\", alpha = 0.25, show.legend = FALSE) +\n  geom_line(aes(y = ipred), linetype = 1, size = 1.15) +\n  geom_line(aes(y = pred), linetype = 2, size = 1.05) +\n  geom_point(aes(y = DV),\n             size = 3, color = \"red\", show.legend = FALSE) +\n  scale_y_log10(name = \"Drug Conc. (ug/mL)\",\n                limits = c(NA, NA)) +\n  scale_x_continuous(name = \"Time (w)\",\n                     breaks = seq(0, 168, by = 28),\n                     labels = seq(0, 168/7, by = 4)) +\n  theme_bw() +\n  theme(axis.text = element_text(size = 14, face = \"bold\"),\n        axis.title = element_text(size = 18, face = \"bold\"),\n        legend.position = \"bottom\") +\n  facet_wrap(~ID, labeller = label_both,\n             ncol = 4, scales = \"free\")\n\n\n\n\n\n\n\n\n\n\n3.4.3.2.2 Predicting New Individuals\nThis has a relatively similar setup as when we were predicting the observed individuals after fitting. The difference now is that\n\nWe can look at new dosing regimens if we want.\nFor each draw from the posterior, we create new individuals by sampling new individual parameter values (while before we used the same individual parameter values as the observed individuals)\n\nWhen we simulate future subjects, we will simulate \\(n_{mcmc}\\) (here, 4000) individuals for each dosing regimen, one individual for each sample from the posterior. We will simulate:\n\nindividual concentration-time profiles (CP) for each new individual for each draw from the posterior.\nan individual concentration-time profile (DV) that adds residual variability to the CP.\n\nThese simulations will create simulated profiles that take into account all sources of uncertainty (i.e. uncertainty in the parameter values) and variability (between and within individuals).\nWe can first look at the .stan file that makes the simulations:\n\n\nCode\nmodel_pred_new <- cmdstan_model(\n  \"Torsten/Predict/iv_2cmt_ppa_m4_predict_new_subjects.stan\")\n\nmodel_pred_new$print()\n\n\n// IV Infusion\n// Two-compartment PK Model\n// IIV on CL, VC, Q, VP (full covariance matrix)\n// proportional plus additive error - DV = CP(1 + eps_p) + eps_a\n// Closed form solution using Torsten\n// Predictions are generated from a normal that is truncated below at 0\n\nfunctions{\n  \n  real normal_lb_rng(real mu, real sigma, real lb){\n    \n    real p_lb = normal_cdf(lb | mu, sigma);\n    real u = uniform_rng(p_lb, 1);\n    real y = mu + sigma * inv_Phi(u);\n    return y;\n\n  }\n  \n}\n\ndata{\n  \n  int n_subjects;\n  int n_subjects_new;\n  int n_time_new;\n  array[n_time_new] real time;\n  array[n_time_new] real amt;\n  array[n_time_new] int cmt;\n  array[n_time_new] int evid;\n  array[n_time_new] real rate;\n  array[n_time_new] real ii;\n  array[n_time_new] int addl;\n  array[n_time_new] int ss;\n  array[n_subjects_new] int subj_start;\n  array[n_subjects_new] int subj_end;\n  \n}\ntransformed data{ // done\n  \n  int n_random = 4;                    // Number of random effects\n  \n}\nparameters{  // done\n  \n  real<lower = 0> TVCL;       \n  real<lower = 0> TVVC; \n  real<lower = 0> TVQ;\n  real<lower = 0> TVVP; \n  \n  vector<lower = 0>[n_random] omega;\n  cholesky_factor_corr[n_random] L;\n  \n  vector<lower = 0>[2] sigma;\n  cholesky_factor_corr[2] L_Sigma;\n  \n  matrix[n_random, n_subjects] Z;\n  \n}\ngenerated quantities{\n\n  vector[n_time_new] cp; // concentration with no residual error\n  vector[n_time_new] dv; // concentration with residual error\n\n  {\n    vector[n_random] typical_values = to_vector({TVCL, TVVC, TVQ, TVVP});\n    \n    matrix[2, 2] R_Sigma = multiply_lower_tri_self_transpose(L_Sigma);\n    matrix[2, 2] Sigma = quad_form_diag(R_Sigma, sigma);\n    \n    matrix[n_random, n_subjects_new] eta_new;\n    matrix[n_subjects_new, n_random] theta_new;\n    vector[n_subjects_new] CL;\n    vector[n_subjects_new] VC;\n    vector[n_subjects_new] Q;\n    vector[n_subjects_new] VP;\n    matrix[n_time_new, 3] x_cp;\n    \n    for(i in 1:n_subjects_new){\n      eta_new[, i] = multi_normal_cholesky_rng(rep_vector(0, n_random),\n                                               diag_pre_multiply(omega, L));\n    }\n    theta_new = (rep_matrix(typical_values, n_subjects_new) .* exp(eta_new))';\n    \n    CL = col(theta_new, 1);\n    VC = col(theta_new, 2);\n    Q = col(theta_new, 3);\n    VP = col(theta_new, 4);\n    \n    for(j in 1:n_subjects_new){\n      \n      array[5] real theta_params = {CL[j], Q[j], VC[j], VP[j], 0};\n      \n      x_cp[subj_start[j]:subj_end[j],] =\n        pmx_solve_twocpt(time[subj_start[j]:subj_end[j]],\n                         amt[subj_start[j]:subj_end[j]],\n                         rate[subj_start[j]:subj_end[j]],\n                         ii[subj_start[j]:subj_end[j]],\n                         evid[subj_start[j]:subj_end[j]],\n                         cmt[subj_start[j]:subj_end[j]],\n                         addl[subj_start[j]:subj_end[j]],\n                         ss[subj_start[j]:subj_end[j]],\n                         theta_params)';\n\n      cp[subj_start[j]:subj_end[j]] = x_cp[subj_start[j]:subj_end[j], 2] ./ VC[j];\n    \n    }\n\n    for(i in 1:n_time_new){\n      if(cp[i] == 0){\n        dv[i] = 0;\n      }else{\n        real cp_tmp = cp[i];\n        real sigma_tmp = sqrt(square(cp_tmp) * Sigma[1, 1] + Sigma[2, 2] + \n                              2*cp_tmp*Sigma[2, 1]);\n        dv[i] = normal_lb_rng(cp_tmp, sigma_tmp, 0.0);\n        \n      }\n    }\n  \n  }\n\n}\n\n\nIt looks similar to the file that predicts for observed individuals, but you’ll notice that here, new \\(\\eta_{CL_i}, \\; \\eta_{V_{c_i}}, \\; \\eta_{Q_i}\\) and \\(\\eta_{V_{p_i}}\\) values are simulated as opposed to the previous file which uses the fitted \\(\\eta\\) values.\nAs our example, we’ll simulate new individuals with differing dosing schemes:\n\n400 mg Q4W\n800 mg Q4W\n1200 mg Q4W\n1600 mg Q4W\n400 mg Q2W\n800 mg Q2W\n600 mg Q3W\n1200 mg Q3W\n\nI want to simulate 1) and 2) so I can do a pseudo-VPC to see how our model has done and to generalize our observed data to a larger population. 3) and 4) might be to explore a possible dose escalation. For 5)-8), we might be wondering if Q2W or Q3W dosing will better keep the subjects’ exposures in the therapeutic range. We can also predict the population distribution of exposure metrics, e.g. \\(AUC, \\; C_{max_{ss}}, \\, C_{trough_{ss}},\\, \\ldots\\), for any of these dosing regimens.\n\n\nCode\ndosing_data_q4w <- mrgsolve::expand.ev(ii = 28, until = 168, tinf = 1/24,\n                                       cmt = 2, evid = 1,\n                                       amt = c(400, 800, 1200, 1600),\n                                       ss = 0, mdv = 1) %>%\n  mrgsolve::realize_addl() %>%\n  as_tibble()\n\ndosing_data_q2w <- mrgsolve::expand.ev(ID = 1, ii = 14, until = 168,\n                                       tinf = 1/24, cmt = 2, evid = 1,\n                                       amt = c(400, 800), ss = 0, mdv = 1) %>%\n  mrgsolve::realize_addl() %>%\n  as_tibble() %>%\n  mutate(ID = ID + 4)\n\ndosing_data_q3w <- mrgsolve::expand.ev(ID = 1, ii = 21, until = 168,\n                                       tinf = 1/24, cmt = 2, evid = 1,\n                                       amt = c(600, 1200), ss = 0, mdv = 1) %>%\n  mrgsolve::realize_addl() %>%\n  as_tibble() %>%\n  mutate(ID = ID + 6)\n\ndosing_data <- bind_rows(dosing_data_q4w, dosing_data_q2w, dosing_data_q3w)\n\nt1 <- dosing_data %>%\n  select(time) %>%\n  distinct() %>%\n  deframe()\n\nt2 <- dosing_data %>%\n  mutate(time = time + tinf) %>%\n  select(time) %>%\n  distinct() %>%\n  deframe()\n\ntimes_new <- tibble(time = sort(unique(c(t1, t2, seq(1, 168, by = 1)))))\n\nnew_data <- bind_rows(replicate(max(dosing_data$ID), times_new,\n                                simplify = FALSE)) %>%\n  mutate(ID = rep(1:max(dosing_data$ID), each = nrow(times_new)),\n         amt = 0,\n         evid = 0,\n         rate = 0,\n         addl = 0,\n         ii = 0,\n         cmt = 2,\n         mdv = 1,\n         ss = 0) %>%\n  filter(time != 0) %>%\n  select(ID, time, everything()) %>%\n  bind_rows(dosing_data) %>%\n  arrange(ID, time)\n\n\n# number of individuals in the original dataset\nn_subjects <- fit_m4$metadata()$stan_variable_sizes$Z[2]\n\nn_subjects_new <- new_data %>%  # number of new individuals\n  distinct(ID) %>%\n  count() %>%\n  deframe()\n\nn_time_new <- nrow(new_data) # total number of time points at which to predict\n\nsubj_start <- new_data %>%\n  mutate(row_num = 1:n()) %>%\n  group_by(ID) %>%\n  slice_head(n = 1) %>%\n  ungroup() %>%\n  select(row_num) %>%\n  deframe()\n\nsubj_end <- c(subj_start[-1] - 1, n_time_new)\n\nstan_data <- list(n_subjects = n_subjects,\n                  n_subjects_new = n_subjects_new,\n                  n_time_new = n_time_new,\n                  time = new_data$time,\n                  amt = new_data$amt,\n                  cmt = new_data$cmt,\n                  evid = new_data$evid,\n                  rate = new_data$rate,\n                  ii = new_data$ii,\n                  addl = new_data$addl,\n                  ss = new_data$ss,\n                  subj_start = subj_start,\n                  subj_end = subj_end)\n\npreds_new <- model_pred_new$generate_quantities(fit_m4,\n                                                data = stan_data,\n                                                parallel_chains = 4,\n                                                seed = 1234)\n\n\nRunning standalone generated quantities after 4 MCMC chains, all chains in parallel ...\n\nChain 1 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 59.6 seconds.\n\n\nAnd now we can look at the predictions:\n\n\nCode\npreds_new_df <- preds_new$draws(format = \"draws_df\")\n\npost_preds_new_summary <- preds_new_df %>%\n  spread_draws(cp[i], dv[i]) %>%\n  median_qi(cp, dv)\n\nregimens <- c(str_c(c(400, 800, 1200, 1600), \" mg Q4W\"),\n              str_c(c(400, 800), \" mg Q2W\"),\n              str_c(c(600, 1200), \" mg Q3W\"))\n\npreds_new_ind <- post_preds_new_summary %>%\n  mutate(ID = new_data$ID[i],\n         time = new_data$time[i]) %>%\n  select(ID, time, everything(), -i) %>%\n  mutate(regimen = factor(regimens[ID],\n                          levels = regimens))\n\nggplot(preds_new_ind, aes(x = time, group = regimen)) +\n  geom_ribbon(aes(ymin = dv.lower, ymax = dv.upper),\n              fill = \"blue\", alpha = 0.25, show.legend = FALSE) +\n  geom_ribbon(aes(ymin = cp.lower, ymax = cp.upper),\n              fill = \"blue\", alpha = 0.5, show.legend = FALSE) +\n  geom_line(aes(y = cp), linetype = 1, size = 1.15) +\n  geom_line(aes(y = dv), linetype = 2, size = 1.05) +\n  scale_y_log10(name = \"Drug Conc. (ug/mL)\",\n                limits = c(NA, NA)) +\n  scale_x_continuous(name = \"Time (w)\",\n                     breaks = seq(0, 168, by = 28),\n                     labels = seq(0, 168/7, by = 4)) +\n  theme_bw() +\n  theme(axis.text = element_text(size = 14, face = \"bold\"),\n        axis.title = element_text(size = 18, face = \"bold\"),\n        legend.position = \"bottom\") +\n  facet_wrap(~ regimen, ncol = 2)\n\n\n\n\n\nAnd to look at the pseudo-VPC, we can overlay the observed data on the plots with the same planned dosing regimens:\n\n\nCode\npreds_new_ind %>%\n  filter(regimen %in% c(\"400 mg Q4W\", \"800 mg Q4W\")) %>%\n  ggplot(aes(x = time, group = regimen)) +\n  geom_ribbon(aes(ymin = dv.lower, ymax = dv.upper),\n              fill = \"blue\", alpha = 0.25, show.legend = FALSE) +\n  geom_ribbon(aes(ymin = cp.lower, ymax = cp.upper),\n              fill = \"blue\", alpha = 0.5, show.legend = FALSE) +\n  geom_line(aes(y = cp), linetype = 1, size = 1.15) +\n  geom_line(aes(y = dv), linetype = 2, size = 1.05) +\n  geom_point(data = nonmem_data_bloq %>%\n               rename_all(tolower) %>%\n               group_by(id) %>%\n               mutate(Dose = max(amt, na.rm = TRUE),\n                      regimen = str_c(Dose, \" mg Q4W\")) %>%\n               ungroup() %>%\n               filter(regimen %in% c(\"400 mg Q4W\", \"800 mg Q4W\"),\n                      mdv == 0),\n             mapping = aes(x = time, y = dv, group = regimen),\n             color = \"red\", inherit.aes = FALSE) +\n  scale_y_log10(name = \"Drug Conc. (ug/mL)\",\n                limits = c(NA, NA)) +\n  scale_x_continuous(name = \"Time (w)\",\n                     breaks = seq(0, 168, by = 28),\n                     labels = seq(0, 168/7, by = 4)) +\n  theme_bw() +\n  theme(axis.text = element_text(size = 14, face = \"bold\"),\n        axis.title = element_text(size = 18, face = \"bold\"),\n        legend.position = \"bottom\") +\n  facet_wrap(~regimen, ncol = 2)"
  },
  {
    "objectID": "poppk.html#other-topics",
    "href": "poppk.html#other-topics",
    "title": "3  A Bayesian Approach to PK/PD using Stan and Torsten",
    "section": "3.5 Other Topics",
    "text": "3.5 Other Topics\nWe won’t have enough time to thoroughly go through these further topics, but I think it will be useful to show a couple other things that are common.\n\n3.5.1 A Model with Covariates\nWe often want to estimate the effect of covariates on PK. Common covariates are patient characteristics like demographics, body weight, body mass index, concomitant medications, renal function, etc. There are multiple functional forms for both continuous and categorical variables that we can choose from to incorporate covariate effects, and I encourage you to refer to Ch. 5 in the book here17 to see the most common forms for incorporating covariate effects. For this presentation I’ll just give an idea of how it could be done, so I’ll incorporate body weight (wt) as a continuous covariate on clearance using a power model and sex as a categorical covariate on central compartment volume using a proportional shift.\n\\[\\begin{align}\nCL_i &= TVCL * \\left(\\frac{wt}{70}\\right)^{\\theta_{CL}} * e^{\\eta_{CL_i}} \\notag \\\\\nV_{c_i} &= TVVC * \\left(1 + \\theta_{sex}\\times sex_i\\right) * e^{\\eta_{V_{c_i}}}\n\\end{align}\\]\n\n\nCode\nmodel_cov <- cmdstan_model(\"Torsten/Fit/iv_2cmt_ppa_m4_cov.stan\")\n\nmodel_cov$print()\n\n\n// IV Infusion\n// Two-compartment PK Model\n// IIV on CL, VC, Q, VP (full covariance matrix)\n// proportional plus additive error - DV = CP(1 + eps_p) + eps_a\n// Closed form solution using Torsten\n// Implements threading for within-chain parallelization\n// Deals with BLOQ values by the \"CDF trick\" (M4)\n// Since we have a normal distribution on the error, but the DV must be > 0, it\n//   truncates the likelihood below at 0\n// For PPC, it generates values from a normal that is truncated below at 0\n// Covariates: Body Weight on CL, Sex on VC\n\n\nfunctions{\n\n  array[] int sequence(int start, int end) { \n    array[end - start + 1] int seq;\n    for (n in 1:num_elements(seq)) {\n      seq[n] = n + start - 1;\n    }\n    return seq; \n  } \n  \n  int num_between(int lb, int ub, array[] int y){\n    \n    int n = 0;\n    for(i in 1:num_elements(y)){\n      if(y[i] >= lb && y[i] <= ub)\n                    n = n + 1;\n    }\n    return n;\n    \n  }\n  \n  array[] int find_between(int lb, int ub, array[] int y) {\n    // vector[num_between(lb, ub, y)] result;\n    array[num_between(lb, ub, y)] int result;\n    int n = 1;\n    for (i in 1:num_elements(y)) {\n      if (y[i] >= lb && y[i] <= ub) {\n        result[n] = y[i];\n        n = n + 1;\n      }\n    }\n    return result;\n  }\n  \n  vector find_between_vec(int lb, int ub, array[] int idx, vector y) {\n    \n    vector[num_between(lb, ub, idx)] result;\n    int n = 1;\n    if(num_elements(idx) != num_elements(y)) reject(\"illegal input\");\n    for (i in 1:rows(y)) {\n      if (idx[i] >= lb && idx[i] <= ub) {\n        result[n] = y[i];\n        n = n + 1;\n      }\n    }\n    return result;\n  }\n  \n  real normal_lb_rng(real mu, real sigma, real lb){\n    \n    real p_lb = normal_cdf(lb | mu, sigma);\n    real u = uniform_rng(p_lb, 1);\n    real y = mu + sigma * inv_Phi(u);\n    return y;\n\n  }\n  \n  real partial_sum_lpmf(array[] int seq_subj, int start, int end,\n                        vector dv_obs, array[] int dv_obs_id, array[] int i_obs,\n                        array[] real amt, array[] int cmt, array[] int evid, \n                        array[] real time, array[] real rate, array[] real ii, \n                        array[] int addl, array[] int ss,\n                        array[] int subj_start, array[] int subj_end, \n                        real TVCL, real TVVC, real TVQ, real TVVP, \n                        vector omega, matrix L, matrix Z, \n                        vector sigma, matrix L_Sigma,\n                        vector lloq, array[] int bloq,\n                        vector wt, vector sex, \n                        real theta_cl_wt, real theta_vc_sex,\n                        int n_random, int n_subjects, int n_total){\n                           \n    real ptarget = 0;\n    row_vector[n_random] typical_values = to_row_vector({TVCL, TVVC, TVQ, TVVP});\n    \n    matrix[n_subjects, n_random] eta = diag_pre_multiply(omega, L * Z)';\n\n    matrix[n_subjects, n_random] theta =\n                          (rep_matrix(typical_values, n_subjects) .* exp(eta));\n                          \n    matrix[2, 2] R_Sigma = multiply_lower_tri_self_transpose(L_Sigma);\n    matrix[2, 2] Sigma = quad_form_diag(R_Sigma, sigma);\n    \n                              \n    int N = end - start + 1;    // number of subjects in this slice  \n    vector[n_total] dv_ipred;   \n    matrix[n_total, 3] x_ipred; \n  \n\n    int n_obs_slice = num_between(subj_start[start], subj_end[end], i_obs);\n    array[n_obs_slice] int i_obs_slice = find_between(subj_start[start], \n                                                      subj_end[end], i_obs);\n                                                \n    vector[n_obs_slice] dv_obs_slice = find_between_vec(start, end, \n                                                        dv_obs_id, dv_obs);\n    \n    vector[n_obs_slice] ipred_slice;\n    \n    vector[n_obs_slice] lloq_slice = lloq[i_obs_slice];\n    array[n_obs_slice] int bloq_slice = bloq[i_obs_slice];\n    \n    \n    for(n in 1:N){            // loop over subjects in this slice\n    \n      int nn = n + start - 1; // nn is the ID of the current subject\n      \n      // real wt_over_70 = wt[nn]/70;\n      \n      real wt_adjustment_cl = (wt[nn]/70)^theta_cl_wt;\n      real sex_adjustment_vc = 1 + theta_vc_sex*sex[nn];\n      \n      row_vector[n_random] theta_nn = theta[nn]; // access the parameters for subject nn\n      real cl = theta_nn[1] * wt_adjustment_cl;\n      real vc = theta_nn[2] * sex_adjustment_vc;\n      real q = theta_nn[3];\n      real vp = theta_nn[4];\n      array[5] real theta_params = {cl, q, vc, vp, 0}; // The 0 is for KA. Skip the absorption\n      \n      x_ipred[subj_start[nn]:subj_end[nn], ] =\n        pmx_solve_twocpt(time[subj_start[nn]:subj_end[nn]],\n                         amt[subj_start[nn]:subj_end[nn]],\n                         rate[subj_start[nn]:subj_end[nn]],\n                         ii[subj_start[nn]:subj_end[nn]],\n                         evid[subj_start[nn]:subj_end[nn]],\n                         cmt[subj_start[nn]:subj_end[nn]],\n                         addl[subj_start[nn]:subj_end[nn]],\n                         ss[subj_start[nn]:subj_end[nn]],\n                         theta_params)';\n                      \n      dv_ipred[subj_start[nn]:subj_end[nn]] = \n        x_ipred[subj_start[nn]:subj_end[nn], 2] ./ vc;\n    \n    }\n  \n    \n    ipred_slice = dv_ipred[i_obs_slice];\n    \n    for(i in 1:n_obs_slice){\n      real sigma_tmp = sqrt(square(ipred_slice[i]) * Sigma[1, 1] +\n                                         Sigma[2, 2] + \n                                         2*ipred_slice[i]*Sigma[2, 1]);\n      if(bloq_slice[i] == 1){\n        // ptarget += log(normal_cdf(lloq_slice[i] | ipred_slice[i], sigma_tmp) -\n        //                normal_cdf(0.0 | ipred_slice[i], sigma_tmp)) -\n        //            normal_lccdf(0.0 | ipred_slice[i], sigma_tmp);  \n        ptarget += log_diff_exp(normal_lcdf(lloq_slice[i] | ipred_slice[i], \n                                                            sigma_tmp),\n                                normal_lcdf(0.0 | ipred_slice[i], sigma_tmp)) -\n                   normal_lccdf(0.0 | ipred_slice[i], sigma_tmp);   \n      }else{\n        ptarget += normal_lpdf(dv_obs_slice[i] | ipred_slice[i], sigma_tmp) -\n                   normal_lccdf(0.0 | ipred_slice[i], sigma_tmp);\n      }\n    }                                         \n                              \n    return ptarget;\n                           \n  }\n  \n}\ndata{\n  \n  int n_subjects;\n  int n_total;\n  int n_obs;\n  array[n_obs] int i_obs;\n  array[n_total] int ID;\n  array[n_total] real amt;\n  array[n_total] int cmt;\n  array[n_total] int evid;\n  array[n_total] real rate;\n  array[n_total] real ii;\n  array[n_total] int addl;\n  array[n_total] int ss;\n  array[n_total] real time;\n  vector<lower = 0>[n_total] dv;\n  array[n_subjects] int subj_start;\n  array[n_subjects] int subj_end;\n  vector[n_total] lloq;\n  array[n_total] int bloq;\n  \n  vector<lower = 0>[n_subjects] wt;             // baseline bodyweight (kg)\n  vector<lower = 0, upper = 1>[n_subjects] sex; // sex (F = 1, M = 0)\n  \n  real<lower = 0> scale_tvcl;      // Prior Scale parameter for CL\n  real<lower = 0> scale_tvvc;      // Prior Scale parameter for VC\n  real<lower = 0> scale_tvq;       // Prior Scale parameter for Q\n  real<lower = 0> scale_tvvp;      // Prior Scale parameter for VP\n  \n  real<lower = 0> scale_omega_cl; // Prior scale parameter for omega_cl\n  real<lower = 0> scale_omega_vc; // Prior scale parameter for omega_vc\n  real<lower = 0> scale_omega_q;  // Prior scale parameter for omega_q\n  real<lower = 0> scale_omega_vp; // Prior scale parameter for omega_vp\n  \n  real<lower = 0> lkj_df_omega;   // Prior degrees of freedom for omega cor mat\n  \n  real<lower = 0> scale_sigma_p;  // Prior Scale parameter for proportional error\n  real<lower = 0> scale_sigma_a;  // Prior Scale parameter for additive error\n  real<lower = 0> lkj_df_sigma;   // Prior degrees of freedom for sigma cor mat\n \n}\ntransformed data{ \n  \n  int grainsize = 1;\n  \n  vector<lower = 0>[n_obs] dv_obs = dv[i_obs];\n  array[n_obs] int dv_obs_id = ID[i_obs];\n  \n  vector[n_obs] lloq_obs = lloq[i_obs];\n  array[n_obs] int bloq_obs = bloq[i_obs];\n  \n  int n_random = 4;                    // Number of random effects\n  \n  array[n_random] real scale_omega = {scale_omega_cl, scale_omega_vc, \n                                      scale_omega_q, scale_omega_vp}; \n  array[2] real scale_sigma = {scale_sigma_p, scale_sigma_a};\n  \n  array[n_subjects] int seq_subj = sequence(1, n_subjects); // reduce_sum over subjects\n  \n}\nparameters{ \n  \n  real<lower = 0> TVCL;       \n  real<lower = 0> TVVC; \n  real<lower = 0> TVQ;\n  real<lower = 0> TVVP; \n  \n  real theta_cl_wt;\n  real theta_vc_sex;\n  \n  vector<lower = 0>[n_random] omega;\n  cholesky_factor_corr[n_random] L;\n  \n  vector<lower = 0>[2] sigma;\n  cholesky_factor_corr[2] L_Sigma;\n  \n  matrix[n_random, n_subjects] Z;\n  \n}\n\nmodel{ \n  \n  // Priors\n  TVCL ~ cauchy(0, scale_tvcl);\n  TVVC ~ cauchy(0, scale_tvvc);\n  TVQ ~ cauchy(0, scale_tvq);\n  TVVP ~ cauchy(0, scale_tvvp);\n  \n  theta_cl_wt ~ std_normal();\n  theta_vc_sex ~ std_normal();\n\n  omega ~ normal(0, scale_omega);\n  L ~ lkj_corr_cholesky(lkj_df_omega);\n  \n  sigma ~ normal(0, scale_sigma);\n  L_Sigma ~ lkj_corr_cholesky(lkj_df_sigma);\n  \n  to_vector(Z) ~ std_normal();\n  \n  // Likelihood\n  target += reduce_sum(partial_sum_lupmf, seq_subj, grainsize,\n                       dv_obs, dv_obs_id, i_obs,\n                       amt, cmt, evid, time, \n                       rate, ii, addl, ss, subj_start, subj_end, \n                       TVCL, TVVC, TVQ, TVVP, omega, L, Z,\n                       sigma, L_Sigma, \n                       lloq, bloq,\n                       wt, sex, theta_cl_wt, theta_vc_sex,\n                       n_random, n_subjects, n_total);\n}\ngenerated quantities{\n  \n  real<lower = 0> sigma_p = sigma[1];\n  real<lower = 0> sigma_a = sigma[2];\n  \n  real<lower = 0> sigma_sq_p = square(sigma_p);\n  real<lower = 0> sigma_sq_a = square(sigma_a); \n\n  real<lower = 0> omega_cl = omega[1];\n  real<lower = 0> omega_vc = omega[2];\n  real<lower = 0> omega_q = omega[3];\n  real<lower = 0> omega_vp = omega[4];\n\n  real<lower = 0> omega_sq_cl = square(omega_cl);\n  real<lower = 0> omega_sq_vc = square(omega_vc);\n  real<lower = 0> omega_sq_q = square(omega_q);\n  real<lower = 0> omega_sq_vp = square(omega_vp);\n\n  real cor_cl_vc;\n  real cor_cl_q;\n  real cor_cl_vp;\n  real cor_vc_q;\n  real cor_vc_vp;\n  real cor_q_vp;\n  real omega_cl_vc;\n  real omega_cl_q;\n  real omega_cl_vp;\n  real omega_vc_q;\n  real omega_vc_vp;\n  real omega_q_vp;\n\n  real cor_p_a;\n  real sigma_p_a;\n\n  vector[n_subjects] eta_cl;\n  vector[n_subjects] eta_vc;\n  vector[n_subjects] eta_q;\n  vector[n_subjects] eta_vp;\n  vector[n_subjects] CL;\n  vector[n_subjects] VC;\n  vector[n_subjects] Q;\n  vector[n_subjects] VP;\n  \n  vector[n_obs] ipred;\n  vector[n_obs] pred;\n  vector[n_obs] dv_ppc;\n  vector[n_obs] log_lik;\n  vector[n_obs] res;\n  vector[n_obs] wres;\n  vector[n_obs] ires;\n  vector[n_obs] iwres;\n \n  {\n    row_vector[n_random] typical_values = to_row_vector({TVCL, TVVC, TVQ, TVVP});\n\n    matrix[n_random, n_random] R = multiply_lower_tri_self_transpose(L);\n    matrix[n_random, n_random] Omega = quad_form_diag(R, omega);\n\n    matrix[2, 2] R_Sigma = multiply_lower_tri_self_transpose(L_Sigma);\n    matrix[2, 2] Sigma = quad_form_diag(R_Sigma, sigma);\n\n    matrix[n_subjects, n_random] eta = diag_pre_multiply(omega, L * Z)';\n\n    matrix[n_subjects, n_random] theta =\n                          (rep_matrix(typical_values, n_subjects) .* exp(eta));\n\n    vector[n_total] dv_pred;\n    matrix[n_total, 3] x_pred;\n    vector[n_total] dv_ipred;\n    matrix[n_total, 3] x_ipred;\n    \n    eta_cl = col(eta, 1);\n    eta_vc = col(eta, 2);\n    eta_q = col(eta, 3);\n    eta_vp = col(eta, 4);\n    \n    cor_cl_vc = R[1, 2];\n    cor_cl_q = R[1, 3];\n    cor_cl_vp = R[1, 4];\n    cor_vc_q = R[2, 3];\n    cor_vc_vp = R[2, 4];\n    cor_q_vp = R[3, 4];\n    omega_cl_vc = Omega[1, 2];\n    omega_cl_q = Omega[1, 3];\n    omega_cl_vp = Omega[1, 4];\n    omega_vc_q = Omega[2, 3];\n    omega_vc_vp = Omega[2, 4];\n    omega_q_vp = Omega[3, 4];\n\n    cor_p_a = R_Sigma[2, 1];\n    sigma_p_a = Sigma[2, 1];\n\n    for(j in 1:n_subjects){\n\n      array[5] real theta_params;\n      array[5] real theta_params_tv;\n      \n      // real wt_over_70 = wt[j]/70;\n      real wt_adjustment_cl = (wt[j]/70)^theta_cl_wt;\n      real sex_adjustment_vc = 1 + theta_vc_sex*sex[j];\n      \n      real cl_p = TVCL * wt_adjustment_cl;\n      real vc_p = TVVC * sex_adjustment_vc;\n      row_vector[n_random] theta_j = theta[j]; // access the parameters for subject j\n      CL[j] = theta_j[1] * wt_adjustment_cl;\n      VC[j] = theta_j[2] * sex_adjustment_vc;\n      Q[j] = theta_j[3];\n      VP[j] = theta_j[4];\n      theta_params = {CL[j], Q[j], VC[j], VP[j], 0}; // The 0 is for KA. Skip the absorption\n      theta_params_tv = {cl_p, TVQ, vc_p, TVVP, 0};  // 0 is for KA to skip absorption\n      \n      x_ipred[subj_start[j]:subj_end[j],] =\n        pmx_solve_twocpt(time[subj_start[j]:subj_end[j]],\n                         amt[subj_start[j]:subj_end[j]],\n                         rate[subj_start[j]:subj_end[j]],\n                         ii[subj_start[j]:subj_end[j]],\n                         evid[subj_start[j]:subj_end[j]],\n                         cmt[subj_start[j]:subj_end[j]],\n                         addl[subj_start[j]:subj_end[j]],\n                         ss[subj_start[j]:subj_end[j]],\n                         theta_params)';\n                      \n                      \n      dv_ipred[subj_start[j]:subj_end[j]] = \n        x_ipred[subj_start[j]:subj_end[j], 2] ./ VC[j];\n\n      x_pred[subj_start[j]:subj_end[j],] =\n        pmx_solve_twocpt(time[subj_start[j]:subj_end[j]],\n                         amt[subj_start[j]:subj_end[j]],\n                         rate[subj_start[j]:subj_end[j]],\n                         ii[subj_start[j]:subj_end[j]],\n                         evid[subj_start[j]:subj_end[j]],\n                         cmt[subj_start[j]:subj_end[j]],\n                         addl[subj_start[j]:subj_end[j]],\n                         ss[subj_start[j]:subj_end[j]],\n                         theta_params_tv)';\n\n      dv_pred[subj_start[j]:subj_end[j]] = \n        x_pred[subj_start[j]:subj_end[j], 2] ./ vc_p;\n    }\n\n    pred = dv_pred[i_obs];\n    ipred = dv_ipred[i_obs];\n\n  }\n\n  res = dv_obs - pred;                                                                             \n  ires = dv_obs - ipred;\n\n  for(i in 1:n_obs){\n    real ipred_tmp = ipred[i];\n    real sigma_tmp = sqrt(square(ipred_tmp) * sigma_sq_p + sigma_sq_a + \n                          2*ipred_tmp*sigma_p_a);\n    // dv_ppc[i] = normal_rng(ipred_tmp, sigma_tmp);\n    dv_ppc[i] = normal_lb_rng(ipred_tmp, sigma_tmp, 0.0);\n    if(bloq_obs[i] == 1){\n      // log_lik[i] = log(normal_cdf(lloq_obs[i] | ipred_tmp, sigma_tmp) - \n      //                  normal_cdf(0.0 | ipred_tmp, sigma_tmp)) -\n      //              normal_lccdf(0.0 | ipred_tmp, sigma_tmp);\n      log_lik[i] = log_diff_exp(normal_lcdf(lloq_obs[i] | ipred_tmp, sigma_tmp),\n                                normal_lcdf(0.0 | ipred_tmp, sigma_tmp)) -\n                   normal_lccdf(0.0 | ipred_tmp, sigma_tmp); \n    }else{\n      log_lik[i] = normal_lpdf(dv_obs[i] | ipred_tmp, sigma_tmp) - \n                   normal_lccdf(0.0 | ipred_tmp, sigma_tmp);\n    }\n    wres[i] = res[i]/sigma_tmp;\n    iwres[i] = ires[i]/sigma_tmp;\n  }\n  \n}\n\n\n\n\n3.5.2 ODE Models\nOrdinary differential equations (ODEs) arise when we want to determine a function \\(y(t)\\), but we only know the derivative of that function \\(\\frac{dy}{dt}\\), so we know the rate at which the quantity changes, but not the quantity itself. Many of the models we use in the PK/PD world are based off of a system of ODEs, whether implicitly or explicitly. Some systems such as the one in Equation 3.1 have analytical solutions, while others must be solved numerically.\nIn the special case where the system of ODEs is linear, we can use a matrix exponential solution to solve the system. This can be implemented directly in Stan or Torsten. In the general case, we use numerical integrators (they work with both linear and non-linear systems) to solve the system of ODEs. Due to the computational complexity of solving ODEs, it is generally good practice to use the analytical solution if it’s available, a matrix exponential if we have a linear system, and a numerical integrator if we have a non-linear system. Don’t use a tool that is more computationally expensive if you don’t have to.\n\n3.5.2.1 One-Compartment Depot\nThe system of ODEs shown below is a simple system describing the one-compartment model with first order absorption. We know this system has a simple analytical solution, but for the purposes of learning, I’m going to ignore my best practices advice and show how to implement the matrix exponential solution for this linear system and also the general ODE solution using a numerical solver:\\[\\begin{align}\n\\frac{dA_d}{dt} &= -k_a*A_d \\notag \\\\\n\\frac{dA_c}{dt} &= k_a*A_d - \\frac{CL}{V}*A_c   \\\\\n\\end{align} \\tag{3.2}\\]\n\n3.5.2.1.1 Matrix Exponential\nSince the system in Equation 3.2 is linear, we can write this in the form \\[y'(t) = \\mathbf{K}y(t)\\] where \\[\\begin{equation*}\n\\mathbf{K} =\n\\begin{bmatrix}\n-k_a & 0            \\\\\nk_a  & -\\frac{CL}{V} \\\\\n\\end{bmatrix}\n\\end{equation*}\\]\nTo implement this in Stan with Torsten, we use the pmx_solve_linode ODE function. It is similar to the pmx_solve_onecpt and pmx_solve_twocpt functions that we have used in previous sections in that it accepts the standard NONMEM data arguments such as AMT, EVID, II, RATE, …. The main difference here is that we must input the \\(\\mathbf{K}\\) matrix here (instead of an array of the individual parameters), and the bioavailability and lag-time parameters are required here (they were optional before). The Stan code can be seen here:\n\n\nCode\nlinear_ode_model <- cmdstan_model(\n  \"Torsten/Fit/depot_1cmt_ppa_m4_ode_linear.stan\",\n  cpp_options = list(stan_threads = TRUE))\n\nlinear_ode_model$print()\n\n\n// First Order Absorption (oral/subcutaneous)\n// One-compartment PK Model\n// IIV on CL. V. and Ka (full covariance matrix)\n// proportional plus additive error - DV = CP(1 + eps_p) + eps_a\n// Linear ODE solution using Torsten\n// Implements threading for within-chain parallelization \n// Deals with BLOQ values by the \"CDF trick\" (M4)\n// Since we have a normal distribution on the error, but the DV must be > 0, it\n//   truncates the likelihood below at 0\n// For PPC, it generates values from a normal that is truncated below at 0\n\n\nfunctions{\n\n  array[] int sequence(int start, int end) { \n    array[end - start + 1] int seq;\n    for (n in 1:num_elements(seq)) {\n      seq[n] = n + start - 1;\n    }\n    return seq; \n  } \n  \n  int num_between(int lb, int ub, array[] int y){\n    \n    int n = 0;\n    for(i in 1:num_elements(y)){\n      if(y[i] >= lb && y[i] <= ub)\n                    n = n + 1;\n    }\n    return n;\n    \n  }\n  \n  array[] int find_between(int lb, int ub, array[] int y) {\n    // vector[num_between(lb, ub, y)] result;\n    array[num_between(lb, ub, y)] int result;\n    int n = 1;\n    for (i in 1:num_elements(y)) {\n      if (y[i] >= lb && y[i] <= ub) {\n        result[n] = y[i];\n        n = n + 1;\n      }\n    }\n    return result;\n  }\n  \n  vector find_between_vec(int lb, int ub, array[] int idx, vector y) {\n    \n    vector[num_between(lb, ub, idx)] result;\n    int n = 1;\n    if(num_elements(idx) != num_elements(y)) reject(\"illegal input\");\n    for (i in 1:rows(y)) {\n      if (idx[i] >= lb && idx[i] <= ub) {\n        result[n] = y[i];\n        n = n + 1;\n      }\n    }\n    return result;\n  }\n  \n  real normal_lb_rng(real mu, real sigma, real lb){\n    \n    real p_lb = normal_cdf(lb | mu, sigma);\n    real u = uniform_rng(p_lb, 1);\n    real y = mu + sigma * inv_Phi(u);\n    return y;\n\n  }\n  \n  real partial_sum_lpmf(array[] int seq_subj, int start, int end,\n                        vector dv_obs, array[] int dv_obs_id, array[] int i_obs,\n                        array[] real amt, array[] int cmt, array[] int evid, \n                        array[] real time, array[] real rate, array[] real ii, \n                        array[] int addl, array[] int ss,\n                        array[] int subj_start, array[] int subj_end, \n                        real TVCL, real TVV, real TVKA, \n                        vector omega, matrix L, matrix Z, \n                        vector sigma, matrix L_Sigma,\n                        vector lloq, array[] int bloq, \n                        int n_cmt, array[] real bioav, array[] real tlag, \n                        int n_random, int n_subjects, int n_total){\n                           \n    real ptarget = 0;\n    row_vector[n_random] typical_values = to_row_vector({TVCL, TVV, TVKA});\n    \n    matrix[n_subjects, n_random] eta = diag_pre_multiply(omega, L * Z)';\n\n    matrix[n_subjects, n_random] theta =\n                          (rep_matrix(typical_values, n_subjects) .* exp(eta));\n                          \n    matrix[2, 2] R_Sigma = multiply_lower_tri_self_transpose(L_Sigma);\n    matrix[2, 2] Sigma = quad_form_diag(R_Sigma, sigma);\n    \n                              \n    int N = end - start + 1;    // number of subjects in this slice  \n    vector[n_total] dv_ipred;   \n    matrix[n_total, 2] x_ipred; \n  \n    int n_obs_slice = num_between(subj_start[start], subj_end[end], i_obs);\n    array[n_obs_slice] int i_obs_slice = find_between(subj_start[start], \n                                                      subj_end[end], i_obs);\n                                                \n    vector[n_obs_slice] dv_obs_slice = find_between_vec(start, end, \n                                                        dv_obs_id, dv_obs);\n    \n    vector[n_obs_slice] ipred_slice;\n    \n    vector[n_obs_slice] lloq_slice = lloq[i_obs_slice];\n    array[n_obs_slice] int bloq_slice = bloq[i_obs_slice];\n    \n    \n    for(n in 1:N){            // loop over subjects in this slice\n    \n      int nn = n + start - 1; // nn is the ID of the current subject\n      \n      row_vector[n_random] theta_nn = theta[nn]; // access the parameters for subject nn\n      real cl = theta_nn[1];\n      real v = theta_nn[2];\n      real ka = theta_nn[3];\n      \n      matrix[n_cmt, n_cmt] K = rep_matrix(0, n_cmt, n_cmt);\n      K[1, 1] = -ka;\n      K[2, 1] = ka;\n      K[2, 2] = -cl/v;\n      \n      x_ipred[subj_start[nn]:subj_end[nn], ] =\n        pmx_solve_linode(time[subj_start[nn]:subj_end[nn]],\n                         amt[subj_start[nn]:subj_end[nn]],\n                         rate[subj_start[nn]:subj_end[nn]],\n                         ii[subj_start[nn]:subj_end[nn]],\n                         evid[subj_start[nn]:subj_end[nn]],\n                         cmt[subj_start[nn]:subj_end[nn]],\n                         addl[subj_start[nn]:subj_end[nn]],\n                         ss[subj_start[nn]:subj_end[nn]],\n                         K, bioav, tlag)';\n                      \n      dv_ipred[subj_start[nn]:subj_end[nn]] = \n        x_ipred[subj_start[nn]:subj_end[nn], 2] ./ v;\n    \n    }\n  \n    \n    ipred_slice = dv_ipred[i_obs_slice];\n    \n    for(i in 1:n_obs_slice){\n      real sigma_tmp = sqrt(square(ipred_slice[i]) * Sigma[1, 1] +\n                                         Sigma[2, 2] + \n                                         2*ipred_slice[i]*Sigma[2, 1]);\n      if(bloq_slice[i] == 1){\n        // ptarget += log(normal_cdf(lloq_slice[i] | ipred_slice[i], sigma_tmp) -\n        //                normal_cdf(0.0 | ipred_slice[i], sigma_tmp)) -\n        //            normal_lccdf(0.0 | ipred_slice[i], sigma_tmp);  \n        ptarget += log_diff_exp(normal_lcdf(lloq_slice[i] | ipred_slice[i], \n                                                            sigma_tmp),\n                                normal_lcdf(0.0 | ipred_slice[i], sigma_tmp)) -\n                   normal_lccdf(0.0 | ipred_slice[i], sigma_tmp); \n      }else{\n        ptarget += normal_lpdf(dv_obs_slice[i] | ipred_slice[i], sigma_tmp) -\n                   normal_lccdf(0.0 | ipred_slice[i], sigma_tmp);\n      }\n    }                                         \n                              \n    return ptarget;\n                           \n  }\n  \n}\ndata{\n  \n  int n_subjects;\n  int n_total;\n  int n_obs;\n  array[n_obs] int i_obs;\n  array[n_total] int ID;\n  array[n_total] real amt;\n  array[n_total] int cmt;\n  array[n_total] int evid;\n  array[n_total] real rate;\n  array[n_total] real ii;\n  array[n_total] int addl;\n  array[n_total] int ss;\n  array[n_total] real time;\n  vector<lower = 0>[n_total] dv;\n  array[n_subjects] int subj_start;\n  array[n_subjects] int subj_end;\n  vector[n_total] lloq;\n  array[n_total] int bloq;\n  \n  real<lower = 0> scale_tvcl;     // Prior Scale parameter for CL\n  real<lower = 0> scale_tvv;      // Prior Scale parameter for V\n  real<lower = 0> scale_tvka;     // Prior Scale parameter for KA\n  \n  real<lower = 0> scale_omega_cl; // Prior scale parameter for omega_cl\n  real<lower = 0> scale_omega_v;  // Prior scale parameter for omega_v\n  real<lower = 0> scale_omega_ka; // Prior scale parameter for omega_ka\n  \n  real<lower = 0> lkj_df_omega;   // Prior degrees of freedom for omega cor mat\n  \n  real<lower = 0> scale_sigma_p;  // Prior Scale parameter for proportional error\n  real<lower = 0> scale_sigma_a;  // Prior Scale parameter for additive error\n  real<lower = 0> lkj_df_sigma;   // Prior degrees of freedom for sigma cor mat\n \n}\ntransformed data{ \n  \n  int grainsize = 1;\n  \n  vector<lower = 0>[n_obs] dv_obs = dv[i_obs];\n  array[n_obs] int dv_obs_id = ID[i_obs];\n  \n  vector[n_obs] lloq_obs = lloq[i_obs];\n  array[n_obs] int bloq_obs = bloq[i_obs];\n  \n  int n_random = 3;                    // Number of random effects\n  \n  array[n_random] real scale_omega = {scale_omega_cl, scale_omega_v, \n                                      scale_omega_ka}; \n  array[2] real scale_sigma = {scale_sigma_p, scale_sigma_a};\n  \n  array[n_subjects] int seq_subj = sequence(1, n_subjects); // reduce_sum over subjects\n  \n  int n_cmt = 2;\n  array[n_cmt] real bioav = rep_array(1.0, n_cmt);\n  array[n_cmt] real tlag = rep_array(0.0, n_cmt);\n  \n}\nparameters{ \n  \n  real<lower = 0> TVCL;       \n  real<lower = 0> TVV; \n  real<lower = TVCL/TVV> TVKA;\n  \n  vector<lower = 0>[n_random] omega;\n  cholesky_factor_corr[n_random] L;\n  \n  vector<lower = 0>[2] sigma;\n  cholesky_factor_corr[2] L_Sigma;\n  \n  matrix[n_random, n_subjects] Z;\n  \n}\n\nmodel{ \n  \n  // Priors\n  TVCL ~ cauchy(0, scale_tvcl);\n  TVV ~ cauchy(0, scale_tvv);\n  TVKA ~ normal(0, scale_tvka) T[TVCL/TVV, ];\n\n  omega ~ normal(0, scale_omega);\n  L ~ lkj_corr_cholesky(lkj_df_omega);\n  \n  sigma ~ normal(0, scale_sigma);\n  L_Sigma ~ lkj_corr_cholesky(lkj_df_sigma);\n  \n  to_vector(Z) ~ std_normal();\n  \n  // Likelihood\n  target += reduce_sum(partial_sum_lupmf, seq_subj, grainsize,\n                       dv_obs, dv_obs_id, i_obs,\n                       amt, cmt, evid, time, \n                       rate, ii, addl, ss, subj_start, subj_end, \n                       TVCL, TVV, TVKA, omega, L, Z,\n                       sigma, L_Sigma, \n                       lloq, bloq,\n                       n_cmt, bioav, tlag,\n                       n_random, n_subjects, n_total);\n}\ngenerated quantities{\n  \n  real<lower = 0> sigma_p = sigma[1];\n  real<lower = 0> sigma_a = sigma[2];\n  \n  real<lower = 0> sigma_sq_p = square(sigma_p);\n  real<lower = 0> sigma_sq_a = square(sigma_a); \n\n  real<lower = 0> omega_cl = omega[1];\n  real<lower = 0> omega_v = omega[2];\n  real<lower = 0> omega_ka = omega[3];\n\n  real<lower = 0> omega_sq_cl = square(omega_cl);\n  real<lower = 0> omega_sq_v = square(omega_v);\n  real<lower = 0> omega_sq_ka = square(omega_ka);\n\n  real cor_cl_v;\n  real cor_cl_ka;\n  real cor_v_ka;\n  real omega_cl_v;\n  real omega_cl_ka;\n  real omega_v_ka;\n  \n  real cor_p_a;\n  real sigma_p_a;\n\n  vector[n_subjects] eta_cl;\n  vector[n_subjects] eta_v;\n  vector[n_subjects] eta_ka;\n  vector[n_subjects] CL;\n  vector[n_subjects] V;\n  vector[n_subjects] KA;\n  vector[n_subjects] KE;\n\n  vector[n_obs] ipred;\n  vector[n_obs] pred;\n  vector[n_obs] dv_ppc;\n  vector[n_obs] log_lik;\n  vector[n_obs] res;\n  vector[n_obs] wres;\n  vector[n_obs] ires;\n  vector[n_obs] iwres;\n \n  {\n    row_vector[n_random] typical_values = to_row_vector({TVCL, TVV, TVKA});\n\n    matrix[n_random, n_random] R = multiply_lower_tri_self_transpose(L);\n    matrix[n_random, n_random] Omega = quad_form_diag(R, omega);\n\n    matrix[2, 2] R_Sigma = multiply_lower_tri_self_transpose(L_Sigma);\n    matrix[2, 2] Sigma = quad_form_diag(R_Sigma, sigma);\n\n    matrix[n_subjects, n_random] eta = diag_pre_multiply(omega, L * Z)';\n\n    matrix[n_subjects, n_random] theta =\n                          (rep_matrix(typical_values, n_subjects) .* exp(eta));\n\n    vector[n_total] dv_pred;\n    matrix[n_total, 2] x_pred;\n    vector[n_total] dv_ipred;\n    matrix[n_total, 2] x_ipred;\n\n    matrix[n_cmt, n_cmt] K_tv = rep_matrix(0, n_cmt, n_cmt);\n    K_tv[1, 1] = -TVKA;\n    K_tv[2, 1] = TVKA;\n    K_tv[2, 2] = -TVCL/TVV;\n\n    eta_cl = col(eta, 1);\n    eta_v = col(eta, 2);\n    eta_ka = col(eta, 3);\n\n    CL = col(theta, 1);\n    V = col(theta, 2);\n    KA = col(theta, 3);\n    KE = CL ./ V;\n\n    cor_cl_v = R[1, 2];\n    cor_cl_ka = R[1, 3];\n    cor_v_ka = R[2, 3];\n\n    omega_cl_v = Omega[1, 2];\n    omega_cl_ka = Omega[1, 3];\n    omega_v_ka = Omega[2, 3];\n    \n    cor_p_a = R_Sigma[2, 1];\n    sigma_p_a = Sigma[2, 1];\n\n    for(j in 1:n_subjects){\n\n      matrix[n_cmt, n_cmt] K = rep_matrix(0, n_cmt, n_cmt);\n      K[1, 1] = -KA[j];\n      K[2, 1] = KA[j];\n      K[2, 2] = -CL[j]/V[j];\n      \n      x_ipred[subj_start[j]:subj_end[j],] =\n        pmx_solve_linode(time[subj_start[j]:subj_end[j]],\n                         amt[subj_start[j]:subj_end[j]],\n                         rate[subj_start[j]:subj_end[j]],\n                         ii[subj_start[j]:subj_end[j]],\n                         evid[subj_start[j]:subj_end[j]],\n                         cmt[subj_start[j]:subj_end[j]],\n                         addl[subj_start[j]:subj_end[j]],\n                         ss[subj_start[j]:subj_end[j]],\n                         K, bioav, tlag)';\n                      \n                      \n      dv_ipred[subj_start[j]:subj_end[j]] = \n        x_ipred[subj_start[j]:subj_end[j], 2] ./ V[j];\n\n      x_pred[subj_start[j]:subj_end[j],] =\n        pmx_solve_linode(time[subj_start[j]:subj_end[j]],\n                         amt[subj_start[j]:subj_end[j]],\n                         rate[subj_start[j]:subj_end[j]],\n                         ii[subj_start[j]:subj_end[j]],\n                         evid[subj_start[j]:subj_end[j]],\n                         cmt[subj_start[j]:subj_end[j]],\n                         addl[subj_start[j]:subj_end[j]],\n                         ss[subj_start[j]:subj_end[j]],\n                         K_tv, bioav, tlag)';\n\n      dv_pred[subj_start[j]:subj_end[j]] = \n        x_pred[subj_start[j]:subj_end[j], 2] ./ TVV;\n    }\n\n    pred = dv_pred[i_obs];\n    ipred = dv_ipred[i_obs];\n\n  }\n\n  res = dv_obs - pred;                                                                             \n  ires = dv_obs - ipred;\n\n  for(i in 1:n_obs){\n    real ipred_tmp = ipred[i];\n    real sigma_tmp = sqrt(square(ipred_tmp) * sigma_sq_p + sigma_sq_a + \n                          2*ipred_tmp*sigma_p_a);\n    // dv_ppc[i] = normal_rng(ipred_tmp, sigma_tmp);\n    dv_ppc[i] = normal_lb_rng(ipred_tmp, sigma_tmp, 0.0);\n    if(bloq_obs[i] == 1){\n      // log_lik[i] = log(normal_cdf(lloq_obs[i] | ipred_tmp, sigma_tmp) - \n      //                  normal_cdf(0.0 | ipred_tmp, sigma_tmp)) -\n      //              normal_lccdf(0.0 | ipred_tmp, sigma_tmp);\n      log_lik[i] = log_diff_exp(normal_lcdf(lloq_obs[i] | ipred_tmp, sigma_tmp),\n                                normal_lcdf(0.0 | ipred_tmp, sigma_tmp)) -\n                   normal_lccdf(0.0 | ipred_tmp, sigma_tmp); \n    }else{\n      log_lik[i] = normal_lpdf(dv_obs[i] | ipred_tmp, sigma_tmp) - \n                   normal_lccdf(0.0 | ipred_tmp, sigma_tmp);\n    }\n    wres[i] = res[i]/sigma_tmp;\n    iwres[i] = ires[i]/sigma_tmp;\n  }\n  \n}\n\n\n\n\n3.5.2.1.2 General ODE\nTo implement the system of ODEs shown above in Equation 3.2 we need to write a function in the functions block of our Stan code to describe the system and then use one of the ODE solvers provided by Stan/Torsten to solve the system. Torsten provides 3 solvers:\n\npmx_solve_rk45 - Implements the Runge-Kutta 4/5 method. It is generally useful for non-stiff systems.\npmx_solve_bdf - Implements the backward-differentiation formula. It is generally useful for stiff systems.\npmx_solve_adams - Implements the Adams-Moulton method. It is generally useful for non-stiff systems when a higher accuracy is needed.\n\nIn practice, if you don’t know much about the system, try the rk45 integrator first and then move on to the bdf integrator if the system is potentially stiff.\n\n\nCode\ngeneral_ode_model <- cmdstan_model(\n  \"Torsten/Fit/depot_1cmt_ppa_m4_ode_general.stan\",\n  cpp_options = list(stan_threads = TRUE))\n\ngeneral_ode_model$print()\n\n\n// First Order Absorption (oral/subcutaneous)\n// One-compartment PK Model\n// IIV on CL. V. and Ka (full covariance matrix)\n// proportional plus additive error - DV = CP(1 + eps_p) + eps_a\n// General ODE solution using Torsten\n// Implements threading for within-chain parallelization \n// Deals with BLOQ values by the \"CDF trick\" (M4)\n// Since we have a normal distribution on the error, but the DV must be > 0, it\n//   truncates the likelihood below at 0\n// For PPC, it generates values from a normal that is truncated below at 0\n\n\nfunctions{\n\n  array[] int sequence(int start, int end) { \n    array[end - start + 1] int seq;\n    for (n in 1:num_elements(seq)) {\n      seq[n] = n + start - 1;\n    }\n    return seq; \n  } \n  \n  int num_between(int lb, int ub, array[] int y){\n    \n    int n = 0;\n    for(i in 1:num_elements(y)){\n      if(y[i] >= lb && y[i] <= ub)\n                    n = n + 1;\n    }\n    return n;\n    \n  }\n  \n  array[] int find_between(int lb, int ub, array[] int y) {\n    // vector[num_between(lb, ub, y)] result;\n    array[num_between(lb, ub, y)] int result;\n    int n = 1;\n    for (i in 1:num_elements(y)) {\n      if (y[i] >= lb && y[i] <= ub) {\n        result[n] = y[i];\n        n = n + 1;\n      }\n    }\n    return result;\n  }\n  \n  vector find_between_vec(int lb, int ub, array[] int idx, vector y) {\n    \n    vector[num_between(lb, ub, idx)] result;\n    int n = 1;\n    if(num_elements(idx) != num_elements(y)) reject(\"illegal input\");\n    for (i in 1:rows(y)) {\n      if (idx[i] >= lb && idx[i] <= ub) {\n        result[n] = y[i];\n        n = n + 1;\n      }\n    }\n    return result;\n  }\n  \n  real normal_lb_rng(real mu, real sigma, real lb){\n    \n    real p_lb = normal_cdf(lb | mu, sigma);\n    real u = uniform_rng(p_lb, 1);\n    real y = mu + sigma * inv_Phi(u);\n    return y;\n\n  }\n  \n  vector depot_1cmt_ode(real t, vector y, array[] real params, \n                              array[] real x_r, array[] int x_i){\n    \n    real cl = params[1];\n    real v = params[2];\n    real ka = params[3];\n    real ke = cl/v;\n    \n    vector[2] dydt;\n\n    dydt[1] = -ka*y[1];\n    dydt[2] = ka*y[1] - ke*y[2];\n\n    return dydt;\n  }\n  \n  real partial_sum_lpmf(array[] int seq_subj, int start, int end,\n                        vector dv_obs, array[] int dv_obs_id, array[] int i_obs,\n                        array[] real amt, array[] int cmt, array[] int evid, \n                        array[] real time, array[] real rate, array[] real ii, \n                        array[] int addl, array[] int ss,\n                        array[] int subj_start, array[] int subj_end, \n                        real TVCL, real TVV, real TVKA, \n                        vector omega, matrix L, matrix Z, \n                        vector sigma, matrix L_Sigma,\n                        vector lloq, array[] int bloq,\n                        int n_cmt,\n                        int n_random, int n_subjects, int n_total){\n                           \n    real ptarget = 0;\n    row_vector[n_random] typical_values = to_row_vector({TVCL, TVV, TVKA});\n    \n    matrix[n_subjects, n_random] eta = diag_pre_multiply(omega, L * Z)';\n\n    matrix[n_subjects, n_random] theta =\n                          (rep_matrix(typical_values, n_subjects) .* exp(eta));\n                          \n    matrix[2, 2] R_Sigma = multiply_lower_tri_self_transpose(L_Sigma);\n    matrix[2, 2] Sigma = quad_form_diag(R_Sigma, sigma);\n    \n                              \n    int N = end - start + 1;    // number of subjects in this slice  \n    vector[n_total] dv_ipred;   \n    matrix[n_total, 2] x_ipred;\n  \n    int n_obs_slice = num_between(subj_start[start], subj_end[end], i_obs);\n    array[n_obs_slice] int i_obs_slice = find_between(subj_start[start], \n                                                      subj_end[end], i_obs);\n                                                \n    vector[n_obs_slice] dv_obs_slice = find_between_vec(start, end, \n                                                        dv_obs_id, dv_obs);\n    \n    vector[n_obs_slice] ipred_slice;\n    \n    vector[n_obs_slice] lloq_slice = lloq[i_obs_slice];\n    array[n_obs_slice] int bloq_slice = bloq[i_obs_slice];\n    \n    \n    for(n in 1:N){            // loop over subjects in this slice\n    \n      int nn = n + start - 1; // nn is the ID of the current subject\n      \n      row_vector[n_random] theta_nn = theta[nn]; // access the parameters for subject nn\n      real cl = theta_nn[1];\n      real v = theta_nn[2];\n      real ka = theta_nn[3];\n      \n      array[3] real theta_params = {cl, v, ka}; \n      \n      x_ipred[subj_start[nn]:subj_end[nn], ] =\n        pmx_solve_rk45(depot_1cmt_ode,\n                       n_cmt,\n                       time[subj_start[nn]:subj_end[nn]],\n                       amt[subj_start[nn]:subj_end[nn]],\n                       rate[subj_start[nn]:subj_end[nn]],\n                       ii[subj_start[nn]:subj_end[nn]],\n                       evid[subj_start[nn]:subj_end[nn]],\n                       cmt[subj_start[nn]:subj_end[nn]],\n                       addl[subj_start[nn]:subj_end[nn]],\n                       ss[subj_start[nn]:subj_end[nn]],\n                       theta_params)';\n                      \n      dv_ipred[subj_start[nn]:subj_end[nn]] = \n        x_ipred[subj_start[nn]:subj_end[nn], 2] ./ v;\n    \n    }\n  \n    \n    ipred_slice = dv_ipred[i_obs_slice];\n    \n    for(i in 1:n_obs_slice){\n      real sigma_tmp = sqrt(square(ipred_slice[i]) * Sigma[1, 1] +\n                                         Sigma[2, 2] + \n                                         2*ipred_slice[i]*Sigma[2, 1]);\n      if(bloq_slice[i] == 1){\n        // ptarget += log(normal_cdf(lloq_slice[i] | ipred_slice[i], sigma_tmp) -\n        //                normal_cdf(0.0 | ipred_slice[i], sigma_tmp)) -\n        //            normal_lccdf(0.0 | ipred_slice[i], sigma_tmp);  \n        ptarget += log_diff_exp(normal_lcdf(lloq_slice[i] | ipred_slice[i], \n                                                            sigma_tmp),\n                                normal_lcdf(0.0 | ipred_slice[i], sigma_tmp)) -\n                   normal_lccdf(0.0 | ipred_slice[i], sigma_tmp); \n      }else{\n        ptarget += normal_lpdf(dv_obs_slice[i] | ipred_slice[i], sigma_tmp) -\n                   normal_lccdf(0.0 | ipred_slice[i], sigma_tmp);\n      }\n    }                                         \n                              \n    return ptarget;\n                           \n  }\n  \n}\ndata{\n  \n  int n_subjects;\n  int n_total;\n  int n_obs;\n  array[n_obs] int i_obs;\n  array[n_total] int ID;\n  array[n_total] real amt;\n  array[n_total] int cmt;\n  array[n_total] int evid;\n  array[n_total] real rate;\n  array[n_total] real ii;\n  array[n_total] int addl;\n  array[n_total] int ss;\n  array[n_total] real time;\n  vector<lower = 0>[n_total] dv;\n  array[n_subjects] int subj_start;\n  array[n_subjects] int subj_end;\n  vector[n_total] lloq;\n  array[n_total] int bloq;\n  \n  real<lower = 0> scale_tvcl;     // Prior Scale parameter for CL\n  real<lower = 0> scale_tvv;      // Prior Scale parameter for V\n  real<lower = 0> scale_tvka;     // Prior Scale parameter for KA\n  \n  real<lower = 0> scale_omega_cl; // Prior scale parameter for omega_cl\n  real<lower = 0> scale_omega_v;  // Prior scale parameter for omega_v\n  real<lower = 0> scale_omega_ka; // Prior scale parameter for omega_ka\n  \n  real<lower = 0> lkj_df_omega;   // Prior degrees of freedom for omega cor mat\n  \n  real<lower = 0> scale_sigma_p;  // Prior Scale parameter for proportional error\n  real<lower = 0> scale_sigma_a;  // Prior Scale parameter for additive error\n  real<lower = 0> lkj_df_sigma;   // Prior degrees of freedom for sigma cor mat\n \n}\ntransformed data{ \n  \n  int grainsize = 1;\n  \n  vector<lower = 0>[n_obs] dv_obs = dv[i_obs];\n  array[n_obs] int dv_obs_id = ID[i_obs];\n  \n  vector[n_obs] lloq_obs = lloq[i_obs];\n  array[n_obs] int bloq_obs = bloq[i_obs];\n  \n  int n_random = 3;                    // Number of random effects\n  \n  array[n_random] real scale_omega = {scale_omega_cl, scale_omega_v, \n                                      scale_omega_ka}; \n  array[2] real scale_sigma = {scale_sigma_p, scale_sigma_a};\n  \n  array[n_subjects] int seq_subj = sequence(1, n_subjects); // reduce_sum over subjects\n  \n  int n_cmt = 2;\n  \n}\nparameters{ \n  \n  real<lower = 0> TVCL;       \n  real<lower = 0> TVV; \n  real<lower = TVCL/TVV> TVKA;\n  \n  vector<lower = 0>[n_random] omega;\n  cholesky_factor_corr[n_random] L;\n  \n  vector<lower = 0>[2] sigma;\n  cholesky_factor_corr[2] L_Sigma;\n  \n  matrix[n_random, n_subjects] Z;\n  \n}\n\nmodel{ \n  \n  // Priors\n  TVCL ~ cauchy(0, scale_tvcl);\n  TVV ~ cauchy(0, scale_tvv);\n  TVKA ~ normal(0, scale_tvka) T[TVCL/TVV, ];\n\n  omega ~ normal(0, scale_omega);\n  L ~ lkj_corr_cholesky(lkj_df_omega);\n  \n  sigma ~ normal(0, scale_sigma);\n  L_Sigma ~ lkj_corr_cholesky(lkj_df_sigma);\n  \n  to_vector(Z) ~ std_normal();\n  \n  // Likelihood\n  target += reduce_sum(partial_sum_lupmf, seq_subj, grainsize,\n                       dv_obs, dv_obs_id, i_obs,\n                       amt, cmt, evid, time, \n                       rate, ii, addl, ss, subj_start, subj_end, \n                       TVCL, TVV, TVKA, omega, L, Z,\n                       sigma, L_Sigma, \n                       lloq, bloq, n_cmt,\n                       n_random, n_subjects, n_total);\n}\ngenerated quantities{\n  \n  real<lower = 0> sigma_p = sigma[1];\n  real<lower = 0> sigma_a = sigma[2];\n  \n  real<lower = 0> sigma_sq_p = square(sigma_p);\n  real<lower = 0> sigma_sq_a = square(sigma_a); \n\n  real<lower = 0> omega_cl = omega[1];\n  real<lower = 0> omega_v = omega[2];\n  real<lower = 0> omega_ka = omega[3];\n\n  real<lower = 0> omega_sq_cl = square(omega_cl);\n  real<lower = 0> omega_sq_v = square(omega_v);\n  real<lower = 0> omega_sq_ka = square(omega_ka);\n\n  real cor_cl_v;\n  real cor_cl_ka;\n  real cor_v_ka;\n  real omega_cl_v;\n  real omega_cl_ka;\n  real omega_v_ka;\n  \n  real cor_p_a;\n  real sigma_p_a;\n\n  vector[n_subjects] eta_cl;\n  vector[n_subjects] eta_v;\n  vector[n_subjects] eta_ka;\n  vector[n_subjects] CL;\n  vector[n_subjects] V;\n  vector[n_subjects] KA;\n  vector[n_subjects] KE;\n\n  vector[n_obs] ipred;\n  vector[n_obs] pred;\n  vector[n_obs] dv_ppc;\n  vector[n_obs] log_lik;\n  vector[n_obs] res;\n  vector[n_obs] wres;\n  vector[n_obs] ires;\n  vector[n_obs] iwres;\n \n  {\n    row_vector[n_random] typical_values = to_row_vector({TVCL, TVV, TVKA});\n\n    matrix[n_random, n_random] R = multiply_lower_tri_self_transpose(L);\n    matrix[n_random, n_random] Omega = quad_form_diag(R, omega);\n\n    matrix[2, 2] R_Sigma = multiply_lower_tri_self_transpose(L_Sigma);\n    matrix[2, 2] Sigma = quad_form_diag(R_Sigma, sigma);\n\n    matrix[n_subjects, n_random] eta = diag_pre_multiply(omega, L * Z)';\n\n    matrix[n_subjects, n_random] theta =\n                          (rep_matrix(typical_values, n_subjects) .* exp(eta));\n\n    vector[n_total] dv_pred;\n    matrix[n_total, 2] x_pred;\n    vector[n_total] dv_ipred;\n    matrix[n_total, 2] x_ipred;\n\n    array[3] real theta_params_tv = {TVCL, TVV, TVKA};\n\n    eta_cl = col(eta, 1);\n    eta_v = col(eta, 2);\n    eta_ka = col(eta, 3);\n\n    CL = col(theta, 1);\n    V = col(theta, 2);\n    KA = col(theta, 3);\n    KE = CL ./ V;\n\n    cor_cl_v = R[1, 2];\n    cor_cl_ka = R[1, 3];\n    cor_v_ka = R[2, 3];\n\n    omega_cl_v = Omega[1, 2];\n    omega_cl_ka = Omega[1, 3];\n    omega_v_ka = Omega[2, 3];\n    \n    cor_p_a = R_Sigma[2, 1];\n    sigma_p_a = Sigma[2, 1];\n\n    for(j in 1:n_subjects){\n\n      array[3] real theta_params = {CL[j], V[j], KA[j]};\n      \n      x_ipred[subj_start[j]:subj_end[j],] =\n        pmx_solve_rk45(depot_1cmt_ode,\n                       n_cmt,\n                       time[subj_start[j]:subj_end[j]],\n                       amt[subj_start[j]:subj_end[j]],\n                       rate[subj_start[j]:subj_end[j]],\n                       ii[subj_start[j]:subj_end[j]],\n                       evid[subj_start[j]:subj_end[j]],\n                       cmt[subj_start[j]:subj_end[j]],\n                       addl[subj_start[j]:subj_end[j]],\n                       ss[subj_start[j]:subj_end[j]],\n                       theta_params)';\n            \n                      \n      dv_ipred[subj_start[j]:subj_end[j]] = \n        x_ipred[subj_start[j]:subj_end[j], 2] ./ V[j];\n\n      x_pred[subj_start[j]:subj_end[j],] =\n        pmx_solve_rk45(depot_1cmt_ode,\n                       n_cmt,\n                       time[subj_start[j]:subj_end[j]],\n                       amt[subj_start[j]:subj_end[j]],\n                       rate[subj_start[j]:subj_end[j]],\n                       ii[subj_start[j]:subj_end[j]],\n                       evid[subj_start[j]:subj_end[j]],\n                       cmt[subj_start[j]:subj_end[j]],\n                       addl[subj_start[j]:subj_end[j]],\n                       ss[subj_start[j]:subj_end[j]],\n                       theta_params_tv)';\n\n      dv_pred[subj_start[j]:subj_end[j]] = \n        x_pred[subj_start[j]:subj_end[j], 2] ./ TVV;\n    }\n\n    pred = dv_pred[i_obs];\n    ipred = dv_ipred[i_obs];\n\n  }\n\n  res = dv_obs - pred;                                                                             \n  ires = dv_obs - ipred;\n\n  for(i in 1:n_obs){\n    real ipred_tmp = ipred[i];\n    real sigma_tmp = sqrt(square(ipred_tmp) * sigma_sq_p + sigma_sq_a + \n                          2*ipred_tmp*sigma_p_a);\n    // dv_ppc[i] = normal_rng(ipred_tmp, sigma_tmp);\n    dv_ppc[i] = normal_lb_rng(ipred_tmp, sigma_tmp, 0.0);\n    if(bloq_obs[i] == 1){\n      // log_lik[i] = log(normal_cdf(lloq_obs[i] | ipred_tmp, sigma_tmp) - \n      //                  normal_cdf(0.0 | ipred_tmp, sigma_tmp)) -\n      //              normal_lccdf(0.0 | ipred_tmp, sigma_tmp);\n      log_lik[i] = log_diff_exp(normal_lcdf(lloq_obs[i] | ipred_tmp, sigma_tmp),\n                                normal_lcdf(0.0 | ipred_tmp, sigma_tmp)) -\n                   normal_lccdf(0.0 | ipred_tmp, sigma_tmp); \n    }else{\n      log_lik[i] = normal_lpdf(dv_obs[i] | ipred_tmp, sigma_tmp) - \n                   normal_lccdf(0.0 | ipred_tmp, sigma_tmp);\n    }\n    wres[i] = res[i]/sigma_tmp;\n    iwres[i] = ires[i]/sigma_tmp;\n  }\n  \n}\n\n\n\n\n\n3.5.2.2 Indirect Response Models\nOne common type of ODE model that we use are indirect response models that exhibit “stimulation of the production or dissipation of factors controlling the measured effect.” There are 4 common types of indirect response models:\n\n\n\n\n\n\nIn the notation above \\(R\\) is the response, \\(k_{in}\\) represents the zero-order rate constant for production of the response, \\(k_{out}\\) represents the first-order rate constant for loss of the response, \\(C_p\\) is the plasma concentration (often described by a one- or two-compartment PK model), \\(I_{max}\\) is the maximum fractional inhibition (sometimes fixed to 1) and \\(IC_{50}\\) is the drug concentration that produces 50% of the maximum inhibition for IR1 and IR2, and \\(S_{max}\\) (more commonly \\(E_{max}\\)) is the maximum stimulation and \\(SC_{50}\\) (more commonly \\(EC_{50}\\)) is the drug concentration that produces 50% of the maximum stimulation for IR3 and IR4.\n\n3.5.2.2.1 Indirect Response I: PK Model\nIf we assume a two-compartment model with IV infusion, the full system of ODEs for the Indirect Response I (IR1) model is:\\[\\begin{align}\n\\frac{dA_c}{dt} &= rate_{in} - \\left(\\frac{CL}{V_c} + \\frac{Q}{V_c}\\right)A_C + \\frac{Q}{V_p}A_p  \\notag \\\\\n\\frac{dA_p}{dt} &= \\frac{Q}{V_c}A_c - \\frac{Q}{V_p}A_p \\\\\n\\frac{dR}{dt} &= k_{in}\\left(1 - \\frac{I_{max}*\\left(\\frac{A_c}{V_c}\\right)^\\gamma}{IC_{50}^\\gamma + \\left(\\frac{A_c}{V_c}\\right)^\\gamma}\\right) - k_{out}R\n\\end{align} \\tag{3.3}\\]\nThis model has the standard two-compartment PK model, and the response equation is just like the one in the figure above but also has the parameter \\(\\gamma\\) controlling the steepness of the response curve, but this is commonly set to 1.\n\nIndirect Response I: Full System of ODEsCoupled Model with an Analytical Solution to PK and Numerical Solution to PD ODE\n\n\nWe can fit this data with a model that is written with the full system of ODEs in Equation 3.3. This is done in a similar fashion as the one-compartment model from Section 3.5.2.1.2, i.e. we need to write a function in the functions block of our Stan code to describe the system and then use one of the ODE solvers provided by Stan/Torsten to solve the system. Let’s look at the Stan code:\n\n\nCode\nir1_ode_model <- cmdstan_model(\"Torsten/Fit/iv_2cmt_ppa_ir1_lognormal_m4.stan\",\n  cpp_options = list(stan_threads = TRUE))\n\nir1_ode_model$print()\n\n\n// IV Infusion\n// Two-compartment PK Model with Indirect Response 1 PD Model\n// IIV on CL, VC, Q, VP (full covariance matrix) for PK\n// IIV on KIN, KOUT, IC50 (full covariance matrix) for PD. IMAX is fixed to be 1\n// proportional plus additive error for PK - DV = CP(1 + eps_p_pk) + eps_a_pk\n// lognormal error for PD - DV = f(.)*exp(eps_pd)\n// General ODE solution using Torsten\n// Implements threading for within-chain parallelization\n// Deals with BLOQ values by the \"CDF trick\"\n// Since we have a normal distribution on the error for PK, but the DV for PK \n//   must be > 0, it truncates the likelihood below at 0\n// For PPC for PK, it generates values from a normal that is truncated below at \n//   0.\n\n\nfunctions{\n\n  array[] int sequence(int start, int end) { \n    array[end - start + 1] int seq;\n    for (n in 1:num_elements(seq)) {\n      seq[n] = n + start - 1;\n    }\n    return seq; \n  } \n  \n  int num_between(int lb, int ub, array[] int y){\n    \n    int n = 0;\n    for(i in 1:num_elements(y)){\n      if(y[i] >= lb && y[i] <= ub)\n                    n = n + 1;\n    }\n    return n;\n    \n  }\n  \n  array[] int find_between(int lb, int ub, array[] int y) {\n    // vector[num_between(lb, ub, y)] result;\n    array[num_between(lb, ub, y)] int result;\n    int n = 1;\n    for (i in 1:num_elements(y)) {\n      if (y[i] >= lb && y[i] <= ub) {\n        result[n] = y[i];\n        n = n + 1;\n      }\n    }\n    return result;\n  }\n  \n  vector find_between_vec(int lb, int ub, array[] int idx, vector y) {\n    \n    vector[num_between(lb, ub, idx)] result;\n    int n = 1;\n    if(num_elements(idx) != num_elements(y)) reject(\"illegal input\");\n    for (i in 1:rows(y)) {\n      if (idx[i] >= lb && idx[i] <= ub) {\n        result[n] = y[i];\n        n = n + 1;\n      }\n    }\n    return result;\n  }\n  \n  real normal_lb_rng(real mu, real sigma, real lb){\n    \n    real p_lb = normal_cdf(lb | mu, sigma);\n    real u = uniform_rng(p_lb, 1);\n    real y = mu + sigma * inv_Phi(u);\n    return y;\n\n  }\n  \n  vector two_cmt_ir1_ode(real t, vector y, array[] real params, \n                         array[] real x_r, array[] int x_i){\n    \n    real cl = params[1];\n    real q = params[2];\n    real vc = params[3];\n    real vp = params[4];\n    real ka = params[5];\n    real kin = params[6];\n    real kout = params[7];\n    real ic50 = params[8];\n    real imax = params[9];  // It's fixed to 1 in this particular model\n    real hill = params[10]; // It's fixed to 1 in this particular model\n    real r_0 = params[11];\n    \n    real ke = cl/vc;\n    real k_cp = q/vc;\n    real k_pc = q/vp;\n    \n    real conc = y[2]/vc;\n    \n    real inh = (imax*pow(conc, hill))/(pow(ic50, hill) + pow(conc, hill));\n    real response = y[4] + r_0;\n    \n    \n    vector[4] dydt;\n\n    dydt[1] = -ka*y[1];\n    dydt[2] = ka*y[1] - (ke + k_cp)*y[2] + k_pc*y[3];\n    dydt[3] = k_cp*y[2] - k_pc*y[3];\n    dydt[4] = kin*(1 - inh) - kout*response;\n    \n    return dydt;\n  }\n  \n  real partial_sum_lpmf(array[] int seq_subj, int start, int end,\n                        vector dv_obs, array[] int dv_obs_id, array[] int i_obs,\n                        array[] real amt, array[] int cmt, array[] int evid, \n                        array[] real time, array[] real rate, array[] real ii, \n                        array[] int addl, array[] int ss,\n                        array[] int subj_start, array[] int subj_end, \n                        real TVCL, real TVVC, real TVQ, real TVVP, \n                        vector omega, matrix L, matrix Z, \n                        vector sigma, matrix L_Sigma,\n                        real TVKIN, real TVKOUT, real TVIC50, real imax, real hill,\n                        vector omega_pd, matrix L_pd, matrix Z_pd, \n                        real sigma_pd,\n                        vector lloq, array[] int bloq,\n                        int n_cmt,\n                        int n_random, int n_subjects, int n_total, \n                        int n_random_pd){\n                           \n    real ptarget = 0;\n    row_vector[n_random] typical_values = to_row_vector({TVCL, TVVC, TVQ, TVVP});\n    \n    matrix[n_subjects, n_random] eta = diag_pre_multiply(omega, L * Z)';\n\n    matrix[n_subjects, n_random] theta =\n                          (rep_matrix(typical_values, n_subjects) .* exp(eta));\n                          \n    matrix[2, 2] R_Sigma = multiply_lower_tri_self_transpose(L_Sigma);\n    matrix[2, 2] Sigma = quad_form_diag(R_Sigma, sigma);\n    \n    row_vector[n_random_pd] typical_values_pd = to_row_vector({TVKIN, TVKOUT, \n                                                               TVIC50});\n    \n    matrix[n_subjects, n_random_pd] eta_pd = diag_pre_multiply(omega_pd, \n                                                               L_pd * Z_pd)';\n\n    matrix[n_subjects, n_random_pd] theta_pd =\n                          (rep_matrix(typical_values_pd, n_subjects) .* \n                              exp(eta_pd));\n    \n                              \n    int N = end - start + 1;        // number of subjects in this slice  \n    vector[n_total] dv_ipred;       \n    matrix[n_total, n_cmt] x_ipred; \n  \n\n    int n_obs_slice = num_between(subj_start[start], subj_end[end], i_obs);\n    array[n_obs_slice] int i_obs_slice = find_between(subj_start[start], \n                                                      subj_end[end], i_obs);\n                                                \n    vector[n_obs_slice] dv_obs_slice = find_between_vec(start, end, \n                                                        dv_obs_id, dv_obs);\n    \n    vector[n_obs_slice] ipred_slice;\n    \n    vector[n_obs_slice] lloq_slice = lloq[i_obs_slice];\n    array[n_obs_slice] int bloq_slice = bloq[i_obs_slice];\n    \n    array[n_obs_slice] int cmt_slice = cmt[i_obs_slice];\n    \n    \n    for(n in 1:N){            // loop over subjects in this slice\n    \n      int nn = n + start - 1; // nn is the ID of the current subject\n      \n      row_vector[n_random] theta_nn = theta[nn]; // access the PK parameters for subject nn\n      real cl = theta_nn[1];\n      real vc = theta_nn[2];\n      real q = theta_nn[3];\n      real vp = theta_nn[4];\n      \n      row_vector[n_random_pd] theta_nn_pd = theta_pd[nn]; // access the PD parameters for subject nn\n      real kin = theta_nn_pd[1];\n      real kout = theta_nn_pd[2];\n      real ic50 = theta_nn_pd[3];\n      real r_0 = kin/kout;\n      \n      array[11] real params = {cl, q, vc, vp, 0,  // The 0 is for KA. Skip the absorption\n                               kin, kout, ic50, imax, hill, r_0};\n      \n      x_ipred[subj_start[nn]:subj_end[nn], ] =\n        pmx_solve_rk45(two_cmt_ir1_ode,\n                       n_cmt,\n                       time[subj_start[nn]:subj_end[nn]],\n                       amt[subj_start[nn]:subj_end[nn]],\n                       rate[subj_start[nn]:subj_end[nn]],\n                       ii[subj_start[nn]:subj_end[nn]],\n                       evid[subj_start[nn]:subj_end[nn]],\n                       cmt[subj_start[nn]:subj_end[nn]],\n                       addl[subj_start[nn]:subj_end[nn]],\n                       ss[subj_start[nn]:subj_end[nn]],\n                       params)';\n                      \n      for(k in subj_start[nn]:subj_end[nn]){\n        if(cmt[k] == 2){\n          dv_ipred[k] = x_ipred[k, 2] / vc;\n        }else if(cmt[k] == 4){\n          dv_ipred[k] = x_ipred[k, 4] + r_0;\n        }\n      }       \n    \n    }\n  \n    ipred_slice = dv_ipred[i_obs_slice];\n    \n    for(i in 1:n_obs_slice){\n      if(cmt_slice[i] == 2){\n        real sigma_tmp = sqrt(square(ipred_slice[i]) * Sigma[1, 1] +\n                                           Sigma[2, 2] + \n                                           2*ipred_slice[i]*Sigma[2, 1]);\n        if(bloq_slice[i] == 1){\n          ptarget += log_diff_exp(normal_lcdf(lloq_slice[i] | ipred_slice[i], \n                                                              sigma_tmp),\n                                  normal_lcdf(0.0 | ipred_slice[i], sigma_tmp)) -\n                     normal_lccdf(0.0 | ipred_slice[i], sigma_tmp); \n        }else{\n          ptarget += normal_lpdf(dv_obs_slice[i] | ipred_slice[i], sigma_tmp) -\n                     normal_lccdf(0.0 | ipred_slice[i], sigma_tmp);\n        }\n      }else if(cmt_slice[i] == 4){\n        if(bloq_slice[i] == 1){\n          ptarget += lognormal_lcdf(lloq_slice[i] | log(ipred_slice[i]), sigma_pd);\n        }else{\n          ptarget += lognormal_lpdf(dv_obs_slice[i] | log(ipred_slice[i]), sigma_pd);\n        }\n      }                                         \n    }                          \n    return ptarget;\n                           \n  }\n  \n}\ndata{\n  \n  int n_subjects;\n  int n_total;\n  int n_obs;\n  array[n_obs] int i_obs;\n  array[n_total] int ID;\n  array[n_total] real amt;\n  array[n_total] int cmt;\n  array[n_total] int evid;\n  array[n_total] real rate;\n  array[n_total] real ii;\n  array[n_total] int addl;\n  array[n_total] int ss;\n  array[n_total] real time;\n  vector<lower = 0>[n_total] dv;\n  array[n_subjects] int subj_start;\n  array[n_subjects] int subj_end;\n  vector[n_total] lloq;\n  array[n_total] int bloq;\n  \n  real<lower = 0> scale_tvcl;      // Prior Scale parameter for CL\n  real<lower = 0> scale_tvvc;      // Prior Scale parameter for VC\n  real<lower = 0> scale_tvq;       // Prior Scale parameter for Q\n  real<lower = 0> scale_tvvp;      // Prior Scale parameter for VP\n  \n  real<lower = 0> scale_omega_cl; // Prior scale parameter for omega_cl\n  real<lower = 0> scale_omega_vc; // Prior scale parameter for omega_vc\n  real<lower = 0> scale_omega_q;  // Prior scale parameter for omega_q\n  real<lower = 0> scale_omega_vp; // Prior scale parameter for omega_vp\n  \n  real<lower = 0> lkj_df_omega;   // Prior degrees of freedom for omega cor mat\n  \n  real<lower = 0> scale_sigma_p;  // Prior Scale parameter for proportional error\n  real<lower = 0> scale_sigma_a;  // Prior Scale parameter for additive error\n  real<lower = 0> lkj_df_sigma;   // Prior degrees of freedom for sigma cor mat\n \n  real<lower = 0> scale_tvkin;    // Prior Scale parameter for CL\n  real<lower = 0> scale_tvkout;   // Prior Scale parameter for VC\n  real<lower = 0> scale_tvic50;   // Prior Scale parameter for Q\n  \n  real<lower = 0> scale_omega_kin;  // Prior scale parameter for omega_kin\n  real<lower = 0> scale_omega_kout; // Prior scale parameter for omega_kout\n  real<lower = 0> scale_omega_ic50; // Prior scale parameter for omega_ic50\n  \n  real<lower = 0> lkj_df_omega_pd;  // Prior degrees of freedom for omega cor mat for PD\n  \n  real<lower = 0> scale_sigma_pd;   // Prior Scale parameter for lognormal error for PD\n \n}\ntransformed data{ \n  \n  int grainsize = 1;\n  \n  vector<lower = 0>[n_obs] dv_obs = dv[i_obs];\n  array[n_obs] int dv_obs_id = ID[i_obs];\n  \n  vector[n_obs] lloq_obs = lloq[i_obs];\n  array[n_obs] int bloq_obs = bloq[i_obs];\n  \n  int n_random = 4;                    // Number of random effects for PK\n  int n_random_pd = 3;                 // Number of random effects for PD\n  \n  array[n_random] real scale_omega = {scale_omega_cl, scale_omega_vc, \n                                      scale_omega_q, scale_omega_vp}; \n  array[2] real scale_sigma = {scale_sigma_p, scale_sigma_a};\n  \n  array[n_random_pd] real scale_omega_pd = {scale_omega_kin, scale_omega_kout, \n                                            scale_omega_ic50}; \n  \n  array[n_subjects] int seq_subj = sequence(1, n_subjects); // reduce_sum over subjects\n  \n  int n_cmt = 4;\n  \n  real imax = 1.0;\n  real hill = 1.0;\n}\nparameters{ \n  \n  real<lower = 0> TVCL;       \n  real<lower = 0> TVVC; \n  real<lower = 0> TVQ;\n  real<lower = 0> TVVP; \n  \n  vector<lower = 0>[n_random] omega;\n  cholesky_factor_corr[n_random] L;\n  \n  vector<lower = 0>[2] sigma;\n  cholesky_factor_corr[2] L_Sigma;\n  \n  matrix[n_random, n_subjects] Z;\n  \n  real<lower = 0> TVKIN;       \n  real<lower = 0> TVKOUT; \n  real<lower = 0> TVIC50;\n  \n  vector<lower = 0>[n_random_pd] omega_pd;\n  cholesky_factor_corr[n_random_pd] L_pd;\n  \n  real<lower = 0> sigma_pd;\n  \n  matrix[n_random_pd, n_subjects] Z_pd;\n  \n}\n\nmodel{ \n  \n  // Priors\n  TVCL ~ cauchy(0, scale_tvcl);\n  TVVC ~ cauchy(0, scale_tvvc);\n  TVQ ~ cauchy(0, scale_tvq);\n  TVVP ~ cauchy(0, scale_tvvp);\n\n  omega ~ normal(0, scale_omega);\n  L ~ lkj_corr_cholesky(lkj_df_omega);\n  \n  sigma ~ normal(0, scale_sigma);\n  L_Sigma ~ lkj_corr_cholesky(lkj_df_sigma);\n  \n  to_vector(Z) ~ std_normal();\n  \n  TVKIN ~ cauchy(0, scale_tvkin);\n  TVKOUT ~ cauchy(0, scale_tvkout);\n  TVIC50 ~ cauchy(0, scale_tvic50);\n  \n  omega_pd ~ normal(0, scale_omega_pd);\n  L_pd ~ lkj_corr_cholesky(lkj_df_omega_pd);\n  \n  sigma_pd ~ normal(0, scale_sigma_pd);\n  \n  to_vector(Z_pd) ~ std_normal();\n  \n  // Likelihood\n  target += reduce_sum(partial_sum_lupmf, seq_subj, grainsize,\n                       dv_obs, dv_obs_id, i_obs,\n                       amt, cmt, evid, time, \n                       rate, ii, addl, ss, subj_start, subj_end, \n                       TVCL, TVVC, TVQ, TVVP, omega, L, Z,\n                       sigma, L_Sigma, \n                       TVKIN, TVKOUT, TVIC50, imax, hill,\n                       omega_pd, L_pd, Z_pd, \n                       sigma_pd,\n                       lloq, bloq, n_cmt,\n                       n_random, n_subjects, n_total, n_random_pd);\n}\ngenerated quantities{\n  \n  real<lower = 0> sigma_p = sigma[1];\n  real<lower = 0> sigma_a = sigma[2];\n  \n  real<lower = 0> sigma_sq_p = square(sigma_p);\n  real<lower = 0> sigma_sq_a = square(sigma_a); \n\n  real<lower = 0> omega_cl = omega[1];\n  real<lower = 0> omega_vc = omega[2];\n  real<lower = 0> omega_q = omega[3];\n  real<lower = 0> omega_vp = omega[4];\n\n  real<lower = 0> omega_sq_cl = square(omega_cl);\n  real<lower = 0> omega_sq_vc = square(omega_vc);\n  real<lower = 0> omega_sq_q = square(omega_q);\n  real<lower = 0> omega_sq_vp = square(omega_vp);\n\n  real<lower = 0> omega_kin = omega_pd[1];\n  real<lower = 0> omega_kout = omega_pd[2];\n  real<lower = 0> omega_ic50 = omega_pd[3];\n\n  real<lower = 0> omega_sq_kin = square(omega_kin);\n  real<lower = 0> omega_sq_kout = square(omega_kout);\n  real<lower = 0> omega_sq_ic50 = square(omega_ic50);\n\n  real cor_cl_vc;\n  real cor_cl_q;\n  real cor_cl_vp;\n  real cor_vc_q;\n  real cor_vc_vp;\n  real cor_q_vp;\n  real omega_cl_vc;\n  real omega_cl_q;\n  real omega_cl_vp;\n  real omega_vc_q;\n  real omega_vc_vp;\n  real omega_q_vp;\n\n  real cor_p_a;\n  real sigma_p_a;\n\n  vector[n_subjects] eta_cl;\n  vector[n_subjects] eta_vc;\n  vector[n_subjects] eta_q;\n  vector[n_subjects] eta_vp;\n  vector[n_subjects] CL;\n  vector[n_subjects] VC;\n  vector[n_subjects] Q;\n  vector[n_subjects] VP;\n  \n  real cor_kin_kout;\n  real cor_kin_ic50;\n  real cor_kout_ic50;\n  real omega_kin_kout;\n  real omega_kin_ic50;\n  real omega_kout_ic50;\n  \n  vector[n_subjects] eta_kin;\n  vector[n_subjects] eta_kout;\n  vector[n_subjects] eta_ic50;\n  vector[n_subjects] KIN;\n  vector[n_subjects] KOUT;\n  vector[n_subjects] IC50;\n  \n  vector[n_obs] ipred;\n  vector[n_obs] pred;\n  vector[n_obs] dv_ppc;\n  vector[n_obs] log_lik;\n  vector[n_obs] res;\n  vector[n_obs] wres;\n  vector[n_obs] ires;\n  vector[n_obs] iwres;\n \n  {\n    row_vector[n_random] typical_values = to_row_vector({TVCL, TVVC, TVQ, TVVP});\n\n    matrix[n_random, n_random] R = multiply_lower_tri_self_transpose(L);\n    matrix[n_random, n_random] Omega = quad_form_diag(R, omega);\n\n    matrix[2, 2] R_Sigma = multiply_lower_tri_self_transpose(L_Sigma);\n    matrix[2, 2] Sigma = quad_form_diag(R_Sigma, sigma);\n\n    matrix[n_subjects, n_random] eta = diag_pre_multiply(omega, L * Z)';\n\n    matrix[n_subjects, n_random] theta =\n                          (rep_matrix(typical_values, n_subjects) .* exp(eta));\n                          \n    row_vector[n_random_pd] typical_values_pd = to_row_vector({TVKIN, TVKOUT, \n                                                               TVIC50});\n    \n    matrix[n_random_pd, n_random_pd] R_pd = \n                                        multiply_lower_tri_self_transpose(L_pd);\n    matrix[n_random_pd, n_random_pd] Omega_pd = quad_form_diag(R_pd, omega_pd);\n    \n    matrix[n_subjects, n_random_pd] eta_pd = diag_pre_multiply(omega_pd, \n                                                               L_pd * Z_pd)';\n\n    matrix[n_subjects, n_random_pd] theta_pd =\n                          (rep_matrix(typical_values_pd, n_subjects) .* \n                              exp(eta_pd));\n\n    vector[n_total] dv_pred;\n    matrix[n_total, n_cmt] x_pred;\n    vector[n_total] dv_ipred;\n    matrix[n_total, n_cmt] x_ipred;\n    \n    real r_0_tv = TVKIN/TVKOUT;\n    \n    array[11] real params_tv = {TVCL, TVQ, TVVC, TVVP, 0,  // 0 is for KA to skip absorption\n                                TVKIN, TVKOUT, TVIC50, imax, hill,\n                                r_0_tv};\n    \n    eta_cl = col(eta, 1);\n    eta_vc = col(eta, 2);\n    eta_q = col(eta, 3);\n    eta_vp = col(eta, 4);\n    \n    CL = col(theta, 1);\n    VC = col(theta, 2);\n    Q = col(theta, 3);\n    VP = col(theta, 4);\n\n    cor_cl_vc = R[1, 2];\n    cor_cl_q = R[1, 3];\n    cor_cl_vp = R[1, 4];\n    cor_vc_q = R[2, 3];\n    cor_vc_vp = R[2, 4];\n    cor_q_vp = R[3, 4];\n    omega_cl_vc = Omega[1, 2];\n    omega_cl_q = Omega[1, 3];\n    omega_cl_vp = Omega[1, 4];\n    omega_vc_q = Omega[2, 3];\n    omega_vc_vp = Omega[2, 4];\n    omega_q_vp = Omega[3, 4];\n\n    cor_p_a = R_Sigma[2, 1];\n    sigma_p_a = Sigma[2, 1];\n    \n    eta_kin = col(eta_pd, 1);\n    eta_kout = col(eta_pd, 2);\n    eta_ic50 = col(eta_pd, 3);\n    \n    KIN = col(theta_pd, 1);\n    KOUT = col(theta_pd, 2);\n    IC50 = col(theta_pd, 3);\n\n    cor_kin_kout = R_pd[1, 2];\n    cor_kin_ic50 = R_pd[1, 3];\n    cor_kout_ic50 = R_pd[2, 3];\n    \n    omega_kin_kout = Omega_pd[1, 2];\n    omega_kin_ic50 = Omega_pd[1, 3];\n    omega_kout_ic50 = Omega_pd[2, 3];\n    \n    \n    for(j in 1:n_subjects){\n      \n      real r_0 = KIN[j]/KOUT[j];\n      array[11] real params = {CL[j], Q[j], VC[j], VP[j], 0,\n                               KIN[j], KOUT[j], IC50[j], imax, hill,\n                               r_0};\n      \n      x_ipred[subj_start[j]:subj_end[j],] =\n        pmx_solve_rk45(two_cmt_ir1_ode,\n                       n_cmt,\n                       time[subj_start[j]:subj_end[j]],\n                       amt[subj_start[j]:subj_end[j]],\n                       rate[subj_start[j]:subj_end[j]],\n                       ii[subj_start[j]:subj_end[j]],\n                       evid[subj_start[j]:subj_end[j]],\n                       cmt[subj_start[j]:subj_end[j]],\n                       addl[subj_start[j]:subj_end[j]],\n                       ss[subj_start[j]:subj_end[j]],\n                       params)';\n                      \n      for(k in subj_start[j]:subj_end[j]){\n        if(cmt[k] == 2){\n          dv_ipred[k] = x_ipred[k, 2] / VC[j];\n        }else if(cmt[k] == 4){\n          dv_ipred[k] = x_ipred[k, 4] + r_0;\n        }\n      }    \n\n      x_pred[subj_start[j]:subj_end[j],] =\n        pmx_solve_rk45(two_cmt_ir1_ode, \n                       n_cmt, \n                       time[subj_start[j]:subj_end[j]],\n                       amt[subj_start[j]:subj_end[j]],\n                       rate[subj_start[j]:subj_end[j]],\n                       ii[subj_start[j]:subj_end[j]],\n                       evid[subj_start[j]:subj_end[j]],\n                       cmt[subj_start[j]:subj_end[j]],\n                       addl[subj_start[j]:subj_end[j]],\n                       ss[subj_start[j]:subj_end[j]],\n                       params_tv)';\n      \n      for(k in subj_start[j]:subj_end[j]){\n        if(cmt[k] == 2){\n          dv_pred[k] = x_pred[k, 2] / TVVC;\n        }else if(cmt[k] == 4){\n          dv_pred[k] = x_pred[k, 4] + r_0_tv;\n        }\n      }  \n    }\n\n    pred = dv_pred[i_obs];\n    ipred = dv_ipred[i_obs];\n\n  }\n  \n  for(i in 1:n_obs){\n    if(cmt[i_obs[i]] == 2){\n      real ipred_tmp = ipred[i];\n      real sigma_tmp = sqrt(square(ipred_tmp) * sigma_sq_p + sigma_sq_a + \n                            2*ipred_tmp*sigma_p_a);\n      dv_ppc[i] = normal_lb_rng(ipred_tmp, sigma_tmp, 0.0);\n      if(bloq_obs[i] == 1){\n        log_lik[i] = log_diff_exp(normal_lcdf(lloq_obs[i] | ipred_tmp, sigma_tmp),\n                                  normal_lcdf(0.0 | ipred_tmp, sigma_tmp)) -\n                     normal_lccdf(0.0 | ipred_tmp, sigma_tmp); \n      }else{\n        log_lik[i] = normal_lpdf(dv_obs[i] | ipred_tmp, sigma_tmp) - \n                     normal_lccdf(0.0 | ipred_tmp, sigma_tmp);\n      }\n      res[i] = dv_obs[i] - pred[i];                                                                             \n      ires[i] = dv_obs[i] - ipred[i];\n      wres[i] = res[i]/sigma_tmp;\n      iwres[i] = ires[i]/sigma_tmp;\n    }else if(cmt[i_obs[i]] == 4){\n      dv_ppc[i] = lognormal_rng(log(ipred[i]), sigma_pd);\n      if(bloq_obs[i] == 1){\n        log_lik[i] = lognormal_lcdf(lloq_obs[i] | log(ipred[i]), sigma_pd);\n      }else{\n        log_lik[i] = lognormal_lpdf(dv_obs[i] | log(ipred[i]), sigma_pd);\n      }\n      res[i] = log(dv_obs[i]) - log(pred[i]);\n      ires[i] = log(dv_obs[i]) - log(ipred[i]);\n      wres[i] = res[i]/sigma_pd;\n      iwres[i] = ires[i]/sigma_pd;\n    }\n  }\n  \n}\n\n\n\n\nIf you look at the system of ODEs in Equation 3.3, you will notice that the the first two compartments look like a two-compartment IV model, a system that we have an analytical solution for, In situations like these where our PK model does not depend on our PD model, we can use the coupled ODE model function where the analytical solution for the PK is introduced into the ODE for the PD for numerical integration. This should in theory be simpler and faster computationally due to the smaller system of ODEs that need to be solved numerically. Let’s look at the Stan code:\n\n\nCode\nir1_coupled_model <- cmdstan_model(\n  \"Torsten/Fit/iv_2cmt_ppa_ir1_lognormal_m4_coupled.stan\",\n  cpp_options = list(stan_threads = TRUE))\n\nir1_coupled_model$print()\n\n\n// IV Infusion\n// Two-compartment PK Model with Indirect Response 1 PD Model\n// IIV on CL, VC, Q, VP (full covariance matrix) for PK\n// IIV on KIN, KOUT, IC50 (full covariance matrix) for PD. IMAX is fixed to be 1\n// proportional plus additive error for PK - DV = CP(1 + eps_p_pk) + eps_a_pk\n// lognormal error for PD - DV = f(.)*exp(eps_pd)\n// General ODE solution using Torsten\n// Implements threading for within-chain parallelization\n// Deals with BLOQ values by the \"CDF trick\"\n// Since we have a normal distribution on the error for PK, but the DV for PK \n//   must be > 0, it truncates the likelihood below at 0\n// For PPC for PK, it generates values from a normal that is truncated below at \n//   0.\n\n\nfunctions{\n\n  array[] int sequence(int start, int end) { \n    array[end - start + 1] int seq;\n    for (n in 1:num_elements(seq)) {\n      seq[n] = n + start - 1;\n    }\n    return seq; \n  } \n  \n  int num_between(int lb, int ub, array[] int y){\n    \n    int n = 0;\n    for(i in 1:num_elements(y)){\n      if(y[i] >= lb && y[i] <= ub)\n                    n = n + 1;\n    }\n    return n;\n    \n  }\n  \n  array[] int find_between(int lb, int ub, array[] int y) {\n    // vector[num_between(lb, ub, y)] result;\n    array[num_between(lb, ub, y)] int result;\n    int n = 1;\n    for (i in 1:num_elements(y)) {\n      if (y[i] >= lb && y[i] <= ub) {\n        result[n] = y[i];\n        n = n + 1;\n      }\n    }\n    return result;\n  }\n  \n  vector find_between_vec(int lb, int ub, array[] int idx, vector y) {\n    \n    vector[num_between(lb, ub, idx)] result;\n    int n = 1;\n    if(num_elements(idx) != num_elements(y)) reject(\"illegal input\");\n    for (i in 1:rows(y)) {\n      if (idx[i] >= lb && idx[i] <= ub) {\n        result[n] = y[i];\n        n = n + 1;\n      }\n    }\n    return result;\n  }\n  \n  real normal_lb_rng(real mu, real sigma, real lb){\n    \n    real p_lb = normal_cdf(lb | mu, sigma);\n    real u = uniform_rng(p_lb, 1);\n    real y = mu + sigma * inv_Phi(u);\n    return y;\n\n  }\n  \n  vector two_cmt_ir1_ode(real t, vector y, vector y_pk, \n                         array[] real params, array[] real x_r, \n                         array[] int x_i){\n    \n    real vc = params[3];\n\n    real kin = params[6];\n    real kout = params[7];\n    real ic50 = params[8];\n    real imax = params[9];  // It's fixed to 1 in this particular model\n    real hill = params[10]; // It's fixed to 1 in this particular model\n    real r_0 = params[11];\n    \n    real conc = y_pk[2]/vc;\n    \n    real inh = (imax*pow(conc, hill))/(pow(ic50, hill) + pow(conc, hill));\n    real response = y[1] + r_0;\n    \n    vector[1] dydt;\n\n    dydt[1] = kin*(1 - inh) - kout*response;\n    \n    return dydt;\n  }\n  \n  real partial_sum_lpmf(array[] int seq_subj, int start, int end,\n                        vector dv_obs, array[] int dv_obs_id, array[] int i_obs,\n                        array[] real amt, array[] int cmt, array[] int evid, \n                        array[] real time, array[] real rate, array[] real ii, \n                        array[] int addl, array[] int ss,\n                        array[] int subj_start, array[] int subj_end, \n                        real TVCL, real TVVC, real TVQ, real TVVP, \n                        vector omega, matrix L, matrix Z, \n                        vector sigma, matrix L_Sigma,\n                        real TVKIN, real TVKOUT, real TVIC50, real imax, real hill,\n                        vector omega_pd, matrix L_pd, matrix Z_pd, \n                        real sigma_pd,\n                        vector lloq, array[] int bloq,\n                        int n_cmt, int n_ode, \n                        array[] real bioav, array[] real tlag,\n                        int n_random, int n_subjects, int n_total, \n                        int n_random_pd){\n                           \n    real ptarget = 0;\n    row_vector[n_random] typical_values = to_row_vector({TVCL, TVVC, TVQ, TVVP});\n    \n    matrix[n_subjects, n_random] eta = diag_pre_multiply(omega, L * Z)';\n\n    matrix[n_subjects, n_random] theta =\n                          (rep_matrix(typical_values, n_subjects) .* exp(eta));\n                          \n    matrix[2, 2] R_Sigma = multiply_lower_tri_self_transpose(L_Sigma);\n    matrix[2, 2] Sigma = quad_form_diag(R_Sigma, sigma);\n    \n    row_vector[n_random_pd] typical_values_pd = to_row_vector({TVKIN, TVKOUT, \n                                                               TVIC50});\n    \n    matrix[n_subjects, n_random_pd] eta_pd = diag_pre_multiply(omega_pd, \n                                                               L_pd * Z_pd)';\n\n    matrix[n_subjects, n_random_pd] theta_pd =\n                          (rep_matrix(typical_values_pd, n_subjects) .* \n                              exp(eta_pd));\n    \n                              \n    int N = end - start + 1;        // number of subjects in this slice  \n    vector[n_total] dv_ipred;       \n    matrix[n_total, n_cmt + n_ode] x_ipred; \n  \n\n    int n_obs_slice = num_between(subj_start[start], subj_end[end], i_obs);\n    array[n_obs_slice] int i_obs_slice = find_between(subj_start[start], \n                                                      subj_end[end], i_obs);\n                                                \n    vector[n_obs_slice] dv_obs_slice = find_between_vec(start, end, \n                                                        dv_obs_id, dv_obs);\n    \n    vector[n_obs_slice] ipred_slice;\n    \n    vector[n_obs_slice] lloq_slice = lloq[i_obs_slice];\n    array[n_obs_slice] int bloq_slice = bloq[i_obs_slice];\n    \n    array[n_obs_slice] int cmt_slice = cmt[i_obs_slice];\n    \n    \n    for(n in 1:N){            // loop over subjects in this slice\n    \n      int nn = n + start - 1; // nn is the ID of the current subject\n      \n      row_vector[n_random] theta_nn = theta[nn]; // access the PK parameters for subject nn\n      real cl = theta_nn[1];\n      real vc = theta_nn[2];\n      real q = theta_nn[3];\n      real vp = theta_nn[4];\n      \n      row_vector[n_random_pd] theta_nn_pd = theta_pd[nn]; // access the PD parameters for subject nn\n      real kin = theta_nn_pd[1];\n      real kout = theta_nn_pd[2];\n      real ic50 = theta_nn_pd[3];\n      real r_0 = kin/kout;\n      \n      array[11] real params = {cl, q, vc, vp, 0,  // The 0 is for KA. Skip the absorption\n                               kin, kout, ic50, imax, hill, r_0};\n      \n      x_ipred[subj_start[nn]:subj_end[nn], ] =\n        pmx_solve_twocpt_rk45(two_cmt_ir1_ode,\n                              n_ode,\n                              time[subj_start[nn]:subj_end[nn]],\n                              amt[subj_start[nn]:subj_end[nn]],\n                              rate[subj_start[nn]:subj_end[nn]],\n                              ii[subj_start[nn]:subj_end[nn]],\n                              evid[subj_start[nn]:subj_end[nn]],\n                              cmt[subj_start[nn]:subj_end[nn]],\n                              addl[subj_start[nn]:subj_end[nn]],\n                              ss[subj_start[nn]:subj_end[nn]],\n                              params, bioav, tlag)';\n                      \n      for(k in subj_start[nn]:subj_end[nn]){\n        if(cmt[k] == 2){\n          dv_ipred[k] = x_ipred[k, 2] / vc;\n        }else if(cmt[k] == 4){\n          dv_ipred[k] = x_ipred[k, 4] + r_0;\n        }\n      }       \n    \n    }\n  \n    ipred_slice = dv_ipred[i_obs_slice];\n    \n    for(i in 1:n_obs_slice){\n      if(cmt_slice[i] == 2){\n        real sigma_tmp = sqrt(square(ipred_slice[i]) * Sigma[1, 1] +\n                                           Sigma[2, 2] + \n                                           2*ipred_slice[i]*Sigma[2, 1]);\n        if(bloq_slice[i] == 1){\n          ptarget += log_diff_exp(normal_lcdf(lloq_slice[i] | ipred_slice[i], \n                                                              sigma_tmp),\n                                  normal_lcdf(0.0 | ipred_slice[i], sigma_tmp)) -\n                     normal_lccdf(0.0 | ipred_slice[i], sigma_tmp); \n        }else{\n          ptarget += normal_lpdf(dv_obs_slice[i] | ipred_slice[i], sigma_tmp) -\n                     normal_lccdf(0.0 | ipred_slice[i], sigma_tmp);\n        }\n      }else if(cmt_slice[i] == 4){\n        if(bloq_slice[i] == 1){\n          ptarget += lognormal_lcdf(lloq_slice[i] | log(ipred_slice[i]), sigma_pd);\n        }else{\n          ptarget += lognormal_lpdf(dv_obs_slice[i] | log(ipred_slice[i]), sigma_pd);\n        }\n      }                                         \n    }                          \n    return ptarget;\n                           \n  }\n  \n}\ndata{\n  \n  int n_subjects;\n  int n_total;\n  int n_obs;\n  array[n_obs] int i_obs;\n  array[n_total] int ID;\n  array[n_total] real amt;\n  array[n_total] int cmt;\n  array[n_total] int evid;\n  array[n_total] real rate;\n  array[n_total] real ii;\n  array[n_total] int addl;\n  array[n_total] int ss;\n  array[n_total] real time;\n  vector<lower = 0>[n_total] dv;\n  array[n_subjects] int subj_start;\n  array[n_subjects] int subj_end;\n  vector[n_total] lloq;\n  array[n_total] int bloq;\n  \n  real<lower = 0> scale_tvcl;      // Prior Scale parameter for CL\n  real<lower = 0> scale_tvvc;      // Prior Scale parameter for VC\n  real<lower = 0> scale_tvq;       // Prior Scale parameter for Q\n  real<lower = 0> scale_tvvp;      // Prior Scale parameter for VP\n  \n  real<lower = 0> scale_omega_cl; // Prior scale parameter for omega_cl\n  real<lower = 0> scale_omega_vc; // Prior scale parameter for omega_vc\n  real<lower = 0> scale_omega_q;  // Prior scale parameter for omega_q\n  real<lower = 0> scale_omega_vp; // Prior scale parameter for omega_vp\n  \n  real<lower = 0> lkj_df_omega;   // Prior degrees of freedom for omega cor mat\n  \n  real<lower = 0> scale_sigma_p;  // Prior Scale parameter for proportional error\n  real<lower = 0> scale_sigma_a;  // Prior Scale parameter for additive error\n  real<lower = 0> lkj_df_sigma;   // Prior degrees of freedom for sigma cor mat\n \n  real<lower = 0> scale_tvkin;    // Prior Scale parameter for CL\n  real<lower = 0> scale_tvkout;   // Prior Scale parameter for VC\n  real<lower = 0> scale_tvic50;   // Prior Scale parameter for Q\n  \n  real<lower = 0> scale_omega_kin;  // Prior scale parameter for omega_kin\n  real<lower = 0> scale_omega_kout; // Prior scale parameter for omega_kout\n  real<lower = 0> scale_omega_ic50; // Prior scale parameter for omega_ic50\n  \n  real<lower = 0> lkj_df_omega_pd;  // Prior degrees of freedom for omega cor mat for PD\n  \n  real<lower = 0> scale_sigma_pd;   // Prior Scale parameter for lognormal error for PD\n \n}\ntransformed data{ \n  \n  int grainsize = 1;\n  \n  vector<lower = 0>[n_obs] dv_obs = dv[i_obs];\n  array[n_obs] int dv_obs_id = ID[i_obs];\n  \n  vector[n_obs] lloq_obs = lloq[i_obs];\n  array[n_obs] int bloq_obs = bloq[i_obs];\n  \n  int n_random = 4;                    // Number of random effects for PK\n  int n_random_pd = 3;                 // Number of random effects for PD\n  \n  array[n_random] real scale_omega = {scale_omega_cl, scale_omega_vc, \n                                      scale_omega_q, scale_omega_vp}; \n  array[2] real scale_sigma = {scale_sigma_p, scale_sigma_a};\n  \n  array[n_random_pd] real scale_omega_pd = {scale_omega_kin, scale_omega_kout, \n                                            scale_omega_ic50}; \n  \n  array[n_subjects] int seq_subj = sequence(1, n_subjects); // reduce_sum over subjects\n  \n  int n_cmt = 3; // number of compartments in PK model (depot, central, peripheral)\n  int n_ode = 1; // number of ODEs in PD system\n  \n  real imax = 1.0;\n  real hill = 1.0;\n  \n  array[n_cmt] real bioav = rep_array(1.0, 10);\n  array[n_cmt] real tlag = rep_array(0.0, 10);\n}\nparameters{ \n  \n  real<lower = 0> TVCL;       \n  real<lower = 0> TVVC; \n  real<lower = 0> TVQ;\n  real<lower = 0> TVVP; \n  \n  vector<lower = 0>[n_random] omega;\n  cholesky_factor_corr[n_random] L;\n  \n  vector<lower = 0>[2] sigma;\n  cholesky_factor_corr[2] L_Sigma;\n  \n  matrix[n_random, n_subjects] Z;\n  \n  real<lower = 0> TVKIN;       \n  real<lower = 0> TVKOUT; \n  real<lower = 0> TVIC50;\n  \n  vector<lower = 0>[n_random_pd] omega_pd;\n  cholesky_factor_corr[n_random_pd] L_pd;\n  \n  real<lower = 0> sigma_pd;\n  \n  matrix[n_random_pd, n_subjects] Z_pd;\n  \n}\n\nmodel{ \n  \n  // Priors\n  TVCL ~ cauchy(0, scale_tvcl);\n  TVVC ~ cauchy(0, scale_tvvc);\n  TVQ ~ cauchy(0, scale_tvq);\n  TVVP ~ cauchy(0, scale_tvvp);\n\n  omega ~ normal(0, scale_omega);\n  L ~ lkj_corr_cholesky(lkj_df_omega);\n  \n  sigma ~ normal(0, scale_sigma);\n  L_Sigma ~ lkj_corr_cholesky(lkj_df_sigma);\n  \n  to_vector(Z) ~ std_normal();\n  \n  TVKIN ~ cauchy(0, scale_tvkin);\n  TVKOUT ~ cauchy(0, scale_tvkout);\n  TVIC50 ~ cauchy(0, scale_tvic50);\n  \n  omega_pd ~ normal(0, scale_omega_pd);\n  L_pd ~ lkj_corr_cholesky(lkj_df_omega_pd);\n  \n  sigma_pd ~ normal(0, scale_sigma_pd);\n  \n  to_vector(Z_pd) ~ std_normal();\n  \n  // Likelihood\n  target += reduce_sum(partial_sum_lupmf, seq_subj, grainsize,\n                       dv_obs, dv_obs_id, i_obs,\n                       amt, cmt, evid, time, \n                       rate, ii, addl, ss, subj_start, subj_end, \n                       TVCL, TVVC, TVQ, TVVP, omega, L, Z,\n                       sigma, L_Sigma, \n                       TVKIN, TVKOUT, TVIC50, imax, hill,\n                       omega_pd, L_pd, Z_pd, \n                       sigma_pd,\n                       lloq, bloq, n_cmt, n_ode, bioav, tlag,\n                       n_random, n_subjects, n_total, n_random_pd);\n}\ngenerated quantities{\n  \n  real<lower = 0> sigma_p = sigma[1];\n  real<lower = 0> sigma_a = sigma[2];\n  \n  real<lower = 0> sigma_sq_p = square(sigma_p);\n  real<lower = 0> sigma_sq_a = square(sigma_a); \n\n  real<lower = 0> omega_cl = omega[1];\n  real<lower = 0> omega_vc = omega[2];\n  real<lower = 0> omega_q = omega[3];\n  real<lower = 0> omega_vp = omega[4];\n\n  real<lower = 0> omega_sq_cl = square(omega_cl);\n  real<lower = 0> omega_sq_vc = square(omega_vc);\n  real<lower = 0> omega_sq_q = square(omega_q);\n  real<lower = 0> omega_sq_vp = square(omega_vp);\n\n  real<lower = 0> omega_kin = omega_pd[1];\n  real<lower = 0> omega_kout = omega_pd[2];\n  real<lower = 0> omega_ic50 = omega_pd[3];\n\n  real<lower = 0> omega_sq_kin = square(omega_kin);\n  real<lower = 0> omega_sq_kout = square(omega_kout);\n  real<lower = 0> omega_sq_ic50 = square(omega_ic50);\n\n  real cor_cl_vc;\n  real cor_cl_q;\n  real cor_cl_vp;\n  real cor_vc_q;\n  real cor_vc_vp;\n  real cor_q_vp;\n  real omega_cl_vc;\n  real omega_cl_q;\n  real omega_cl_vp;\n  real omega_vc_q;\n  real omega_vc_vp;\n  real omega_q_vp;\n\n  real cor_p_a;\n  real sigma_p_a;\n\n  vector[n_subjects] eta_cl;\n  vector[n_subjects] eta_vc;\n  vector[n_subjects] eta_q;\n  vector[n_subjects] eta_vp;\n  vector[n_subjects] CL;\n  vector[n_subjects] VC;\n  vector[n_subjects] Q;\n  vector[n_subjects] VP;\n  \n  real cor_kin_kout;\n  real cor_kin_ic50;\n  real cor_kout_ic50;\n  real omega_kin_kout;\n  real omega_kin_ic50;\n  real omega_kout_ic50;\n  \n  vector[n_subjects] eta_kin;\n  vector[n_subjects] eta_kout;\n  vector[n_subjects] eta_ic50;\n  vector[n_subjects] KIN;\n  vector[n_subjects] KOUT;\n  vector[n_subjects] IC50;\n  \n  vector[n_obs] ipred;\n  vector[n_obs] pred;\n  vector[n_obs] dv_ppc;\n  vector[n_obs] log_lik;\n  vector[n_obs] res;\n  vector[n_obs] wres;\n  vector[n_obs] ires;\n  vector[n_obs] iwres;\n \n  {\n    row_vector[n_random] typical_values = to_row_vector({TVCL, TVVC, TVQ, TVVP});\n\n    matrix[n_random, n_random] R = multiply_lower_tri_self_transpose(L);\n    matrix[n_random, n_random] Omega = quad_form_diag(R, omega);\n\n    matrix[2, 2] R_Sigma = multiply_lower_tri_self_transpose(L_Sigma);\n    matrix[2, 2] Sigma = quad_form_diag(R_Sigma, sigma);\n\n    matrix[n_subjects, n_random] eta = diag_pre_multiply(omega, L * Z)';\n\n    matrix[n_subjects, n_random] theta =\n                          (rep_matrix(typical_values, n_subjects) .* exp(eta));\n                          \n    row_vector[n_random_pd] typical_values_pd = to_row_vector({TVKIN, TVKOUT, \n                                                               TVIC50});\n    \n    matrix[n_random_pd, n_random_pd] R_pd = \n                                        multiply_lower_tri_self_transpose(L_pd);\n    matrix[n_random_pd, n_random_pd] Omega_pd = quad_form_diag(R_pd, omega_pd);\n    \n    matrix[n_subjects, n_random_pd] eta_pd = diag_pre_multiply(omega_pd, \n                                                               L_pd * Z_pd)';\n\n    matrix[n_subjects, n_random_pd] theta_pd =\n                          (rep_matrix(typical_values_pd, n_subjects) .* \n                              exp(eta_pd));\n\n    vector[n_total] dv_pred;\n    matrix[n_total, n_cmt + n_ode] x_pred;\n    vector[n_total] dv_ipred;\n    matrix[n_total, n_cmt + n_ode] x_ipred;\n    \n    real r_0_tv = TVKIN/TVKOUT;\n    \n    array[11] real params_tv = {TVCL, TVQ, TVVC, TVVP, 0,  // 0 is for KA to skip absorption\n                                TVKIN, TVKOUT, TVIC50, imax, hill,\n                                r_0_tv};\n    \n    eta_cl = col(eta, 1);\n    eta_vc = col(eta, 2);\n    eta_q = col(eta, 3);\n    eta_vp = col(eta, 4);\n    \n    CL = col(theta, 1);\n    VC = col(theta, 2);\n    Q = col(theta, 3);\n    VP = col(theta, 4);\n\n    cor_cl_vc = R[1, 2];\n    cor_cl_q = R[1, 3];\n    cor_cl_vp = R[1, 4];\n    cor_vc_q = R[2, 3];\n    cor_vc_vp = R[2, 4];\n    cor_q_vp = R[3, 4];\n    omega_cl_vc = Omega[1, 2];\n    omega_cl_q = Omega[1, 3];\n    omega_cl_vp = Omega[1, 4];\n    omega_vc_q = Omega[2, 3];\n    omega_vc_vp = Omega[2, 4];\n    omega_q_vp = Omega[3, 4];\n\n    cor_p_a = R_Sigma[2, 1];\n    sigma_p_a = Sigma[2, 1];\n    \n    eta_kin = col(eta_pd, 1);\n    eta_kout = col(eta_pd, 2);\n    eta_ic50 = col(eta_pd, 3);\n    \n    KIN = col(theta_pd, 1);\n    KOUT = col(theta_pd, 2);\n    IC50 = col(theta_pd, 3);\n\n    cor_kin_kout = R_pd[1, 2];\n    cor_kin_ic50 = R_pd[1, 3];\n    cor_kout_ic50 = R_pd[2, 3];\n    \n    omega_kin_kout = Omega_pd[1, 2];\n    omega_kin_ic50 = Omega_pd[1, 3];\n    omega_kout_ic50 = Omega_pd[2, 3];\n    \n    \n    for(j in 1:n_subjects){\n      \n      real r_0 = KIN[j]/KOUT[j];\n      array[11] real params = {CL[j], Q[j], VC[j], VP[j], 0,\n                               KIN[j], KOUT[j], IC50[j], imax, hill,\n                               r_0};\n      \n      x_ipred[subj_start[j]:subj_end[j],] =\n        pmx_solve_twocpt_rk45(two_cmt_ir1_ode,\n                              n_ode,\n                              time[subj_start[j]:subj_end[j]],\n                              amt[subj_start[j]:subj_end[j]],\n                              rate[subj_start[j]:subj_end[j]],\n                              ii[subj_start[j]:subj_end[j]],\n                              evid[subj_start[j]:subj_end[j]],\n                              cmt[subj_start[j]:subj_end[j]],\n                              addl[subj_start[j]:subj_end[j]],\n                              ss[subj_start[j]:subj_end[j]],\n                              params, bioav, tlag)';\n                      \n      for(k in subj_start[j]:subj_end[j]){\n        if(cmt[k] == 2){\n          dv_ipred[k] = x_ipred[k, 2] / VC[j];\n        }else if(cmt[k] == 4){\n          dv_ipred[k] = x_ipred[k, 4] + r_0;\n        }\n      }    \n\n      x_pred[subj_start[j]:subj_end[j],] =\n        pmx_solve_twocpt_rk45(two_cmt_ir1_ode, \n                              n_ode, \n                              time[subj_start[j]:subj_end[j]],\n                              amt[subj_start[j]:subj_end[j]],\n                              rate[subj_start[j]:subj_end[j]],\n                              ii[subj_start[j]:subj_end[j]],\n                              evid[subj_start[j]:subj_end[j]],\n                              cmt[subj_start[j]:subj_end[j]],\n                              addl[subj_start[j]:subj_end[j]],\n                              ss[subj_start[j]:subj_end[j]],\n                              params_tv, bioav, tlag)';\n      \n      for(k in subj_start[j]:subj_end[j]){\n        if(cmt[k] == 2){\n          dv_pred[k] = x_pred[k, 2] / TVVC;\n        }else if(cmt[k] == 4){\n          dv_pred[k] = x_pred[k, 4] + r_0_tv;\n        }\n      }  \n    }\n\n    pred = dv_pred[i_obs];\n    ipred = dv_ipred[i_obs];\n\n  }\n  \n  for(i in 1:n_obs){\n    if(cmt[i_obs[i]] == 2){\n      real ipred_tmp = ipred[i];\n      real sigma_tmp = sqrt(square(ipred_tmp) * sigma_sq_p + sigma_sq_a + \n                            2*ipred_tmp*sigma_p_a);\n      dv_ppc[i] = normal_lb_rng(ipred_tmp, sigma_tmp, 0.0);\n      if(bloq_obs[i] == 1){\n        log_lik[i] = log_diff_exp(normal_lcdf(lloq_obs[i] | ipred_tmp, sigma_tmp),\n                                  normal_lcdf(0.0 | ipred_tmp, sigma_tmp)) -\n                     normal_lccdf(0.0 | ipred_tmp, sigma_tmp); \n      }else{\n        log_lik[i] = normal_lpdf(dv_obs[i] | ipred_tmp, sigma_tmp) - \n                     normal_lccdf(0.0 | ipred_tmp, sigma_tmp);\n      }\n      res[i] = dv_obs[i] - pred[i];                                                                             \n      ires[i] = dv_obs[i] - ipred[i];\n      wres[i] = res[i]/sigma_tmp;\n      iwres[i] = ires[i]/sigma_tmp;\n    }else if(cmt[i_obs[i]] == 4){\n      dv_ppc[i] = lognormal_rng(log(ipred[i]), sigma_pd);\n      if(bloq_obs[i] == 1){\n        log_lik[i] = lognormal_lcdf(lloq_obs[i] | log(ipred[i]), sigma_pd);\n      }else{\n        log_lik[i] = lognormal_lpdf(dv_obs[i] | log(ipred[i]), sigma_pd);\n      }\n      res[i] = log(dv_obs[i]) - log(pred[i]);\n      ires[i] = log(dv_obs[i]) - log(ipred[i]);\n      wres[i] = res[i]/sigma_pd;\n      iwres[i] = ires[i]/sigma_pd;\n    }\n  }\n  \n}"
  },
  {
    "objectID": "poppk.html#session-info",
    "href": "poppk.html#session-info",
    "title": "3  A Bayesian Approach to PK/PD using Stan and Torsten",
    "section": "3.6 Session Info",
    "text": "3.6 Session Info\n\n\nCode\nsessionInfo()\n\n\nR version 4.1.3 (2022-03-10)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19044)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=English_United States.1252 \n[2] LC_CTYPE=English_United States.1252   \n[3] LC_MONETARY=English_United States.1252\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.1252    \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets \n[6] methods   base     \n\nother attached packages:\n [1] cmdstanr_0.5.3        posterior_1.3.1      \n [3] loo_2.5.1             tidybayes_3.0.2      \n [5] bayesplot_1.9.0       gganimate_1.0.8      \n [7] forcats_0.5.2         stringr_1.4.1        \n [9] dplyr_1.0.10          purrr_0.3.5          \n[11] readr_2.1.3           tidyr_1.2.1          \n[13] tibble_3.1.8          ggplot2_3.3.6        \n[15] tidyverse_1.3.2       patchwork_1.1.2      \n[17] mrgsolve_1.0.6        collapsibleTree_0.1.7\n\nloaded via a namespace (and not attached):\n [1] matrixStats_0.62.0   fs_1.5.2            \n [3] lubridate_1.8.0      progress_1.2.2      \n [5] httr_1.4.4           data.tree_1.0.0     \n [7] tensorA_0.36.2       tools_4.1.3         \n [9] backports_1.4.1      utf8_1.2.2          \n[11] R6_2.5.1             DBI_1.1.3           \n[13] colorspace_2.0-3     ggdist_3.2.0        \n[15] withr_2.5.0          processx_3.8.0      \n[17] tidyselect_1.2.0     prettyunits_1.1.1   \n[19] compiler_4.1.3       cli_3.4.1           \n[21] rvest_1.0.3          arrayhelpers_1.1-0  \n[23] xml2_1.3.3           scales_1.2.1        \n[25] checkmate_2.1.0      ggridges_0.5.4      \n[27] digest_0.6.30        rmarkdown_2.17      \n[29] pkgconfig_2.0.3      htmltools_0.5.3     \n[31] dbplyr_2.2.1         fastmap_1.1.0       \n[33] highr_0.9            htmlwidgets_1.5.4   \n[35] rlang_1.0.6          readxl_1.4.1        \n[37] rstudioapi_0.14      farver_2.1.1        \n[39] generics_0.1.3       svUnit_1.0.6        \n[41] jsonlite_1.8.3       googlesheets4_1.0.1 \n[43] distributional_0.3.1 magrittr_2.0.3      \n[45] Rcpp_1.0.9           munsell_0.5.0       \n[47] fansi_1.0.3          abind_1.4-5         \n[49] lifecycle_1.0.3      stringi_1.7.8       \n[51] yaml_2.3.6           grid_4.1.3          \n[53] parallel_4.1.3       crayon_1.5.2        \n[55] lattice_0.20-45      haven_2.5.1         \n[57] hms_1.1.2            magick_2.7.3        \n[59] ps_1.7.2             knitr_1.40          \n[61] pillar_1.8.1         reprex_2.0.2        \n[63] glue_1.6.2           evaluate_0.17       \n[65] modelr_0.1.9         vctrs_0.5.0         \n[67] tzdb_0.3.0           tweenr_2.0.2        \n[69] cellranger_1.1.0     gtable_0.3.1        \n[71] assertthat_0.2.1     xfun_0.34           \n[73] broom_1.0.1          coda_0.19-4         \n[75] googledrive_2.0.0    gargle_1.2.1        \n[77] ellipsis_0.3.2"
  },
  {
    "objectID": "logisticregression.html",
    "href": "logisticregression.html",
    "title": "4  Logistic Regression Using brms",
    "section": "",
    "text": "library(summarytools)\nlibrary(tidyverse) \nlibrary(brms)\nlibrary(bayesplot)\nlibrary(tidybayes) \nlibrary(gridExtra) \nlibrary(patchwork)"
  },
  {
    "objectID": "logisticregression.html#logistic-regression",
    "href": "logisticregression.html#logistic-regression",
    "title": "4  Logistic Regression Using brms",
    "section": "4.2 Logistic Regression",
    "text": "4.2 Logistic Regression\nThe goal of logistic regression is to find the best fitting model to describe the relationship between the dichotomous characteristic of interest (response or outcome) and a set of independent (predictor or explanatory) variables.\nData for this exercise heart_cleveland_upload.csv was obtained here (https://www.kaggle.com/datasets/cherngs/heart-disease-cleveland-uci). It is a multivariate dataset composed of 14 columns shown below:\n\nage: age in years\nsex: sex [1 = male; 0 = female]\ncp: chest pain type [0 = typical angina; 1 = atypical angina; 2 = non-anginal pain; 3 = asymptomatic]\ntrestbps: resting blood pressure in mm Hg on admission to the hospital\nchol: serum cholestoral in mg/dl\nfbs: fasting blood sugar > 120 mg/dl [1 = true; 0 = false]\nrestecg: resting electrocardiographic results [0 = normal; 1 = having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV); 2 = showing probable or definite left ventricular hypertrophy by Estes’ criteria]\nthalach: maximum heart rate achieved\nexang: exercise induced angina [1 = yes; 0 = no]\noldpeak = ST depression induced by exercise relative to rest\nslope: the slope of the peak exercise ST segment [0 = upsloping; 1 = flat; 2 = downsloping]\nca: number of major vessels (0-3) colored by flourosopy\nthal: thallium stress test result [0 = normal; 1 = fixed defect; 2 = reversible defect]\ncondition: 0 = no disease, 1 = disease"
  },
  {
    "objectID": "logisticregression.html#import-data",
    "href": "logisticregression.html#import-data",
    "title": "4  Logistic Regression Using brms",
    "section": "4.3 Import Data",
    "text": "4.3 Import Data\n\nlrDataRaw <- read.csv(\"data/heart_cleveland_upload.csv\") %>% \n  as_tibble()"
  },
  {
    "objectID": "logisticregression.html#data-processing",
    "href": "logisticregression.html#data-processing",
    "title": "4  Logistic Regression Using brms",
    "section": "4.4 Data Processing",
    "text": "4.4 Data Processing\nConvert categorical explanatory variables to factors\n\nlrData <- lrDataRaw %>% \n  mutate(sex = factor(sex, levels = c(0, 1), labels = c(\"female\", \"male\")),\n         cp = factor(cp, levels = 0:3, \n                     labels = c(\"typical angina\", \"atypical angina\", \"non-anginal pain\", \"asymptomatic\")),\n         fbs = factor(fbs, levels = c(0, 1), labels = c(\"false\", \"true\")),\n         restecg = factor(restecg, levels = 0:2, labels = c(\"normal\", \"abnormal ST\", \"LV hypertrophy\")),\n         exang = factor(exang, levels = c(0, 1), labels = c(\"no\", \"yes\")),\n         slope = factor(slope, levels = 0:2, labels = c(\"upsloping\", \"flat\", \"downsloping\")),\n         thal = factor(thal, levels = 0:2, labels = c(\"normal\", \"fixed defect\", \"reversable defect\")))\nlrData\n\n# A tibble: 297 x 14\n     age sex    cp       trest~1  chol fbs   restecg thalach exang oldpeak slope\n   <int> <fct>  <fct>      <int> <int> <fct> <fct>     <int> <fct>   <dbl> <fct>\n 1    69 male   typical~     160   234 true  LV hyp~     131 no        0.1 flat \n 2    69 female typical~     140   239 false normal      151 no        1.8 upsl~\n 3    66 female typical~     150   226 false normal      114 no        2.6 down~\n 4    65 male   typical~     138   282 true  LV hyp~     174 no        1.4 flat \n 5    64 male   typical~     110   211 false LV hyp~     144 yes       1.8 flat \n 6    64 male   typical~     170   227 false LV hyp~     155 no        0.6 flat \n 7    63 male   typical~     145   233 true  LV hyp~     150 no        2.3 down~\n 8    61 male   typical~     134   234 false normal      145 no        2.6 flat \n 9    60 female typical~     150   240 false normal      171 no        0.9 upsl~\n10    59 male   typical~     178   270 false LV hyp~     145 no        4.2 down~\n# ... with 287 more rows, 3 more variables: ca <int>, thal <fct>,\n#   condition <int>, and abbreviated variable name 1: trestbps"
  },
  {
    "objectID": "logisticregression.html#data-summary",
    "href": "logisticregression.html#data-summary",
    "title": "4  Logistic Regression Using brms",
    "section": "4.5 Data Summary",
    "text": "4.5 Data Summary\n\nprint(summarytools::dfSummary(lrData,\n          varnumbers = FALSE,\n          valid.col = FALSE,\n          graph.magnif = 0.76),\n      method = \"render\")\n\n\n\nData Frame Summary\n\nlrData\n\n\nDimensions: 297 x 14\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      age\n[integer]\n      Mean (sd) : 54.5 (9)min ≤ med ≤ max:29 ≤ 56 ≤ 77IQR (CV) : 13 (0.2)\n      41 distinct values\n      \n      0\n(0.0%)\n    \n    \n      sex\n[factor]\n      1. female2. male\n      96(32.3%)201(67.7%)\n      \n      0\n(0.0%)\n    \n    \n      cp\n[factor]\n      1. typical angina2. atypical angina3. non-anginal pain4. asymptomatic\n      23(7.7%)49(16.5%)83(27.9%)142(47.8%)\n      \n      0\n(0.0%)\n    \n    \n      trestbps\n[integer]\n      Mean (sd) : 131.7 (17.8)min ≤ med ≤ max:94 ≤ 130 ≤ 200IQR (CV) : 20 (0.1)\n      50 distinct values\n      \n      0\n(0.0%)\n    \n    \n      chol\n[integer]\n      Mean (sd) : 247.4 (52)min ≤ med ≤ max:126 ≤ 243 ≤ 564IQR (CV) : 65 (0.2)\n      152 distinct values\n      \n      0\n(0.0%)\n    \n    \n      fbs\n[factor]\n      1. false2. true\n      254(85.5%)43(14.5%)\n      \n      0\n(0.0%)\n    \n    \n      restecg\n[factor]\n      1. normal2. abnormal ST3. LV hypertrophy\n      147(49.5%)4(1.3%)146(49.2%)\n      \n      0\n(0.0%)\n    \n    \n      thalach\n[integer]\n      Mean (sd) : 149.6 (22.9)min ≤ med ≤ max:71 ≤ 153 ≤ 202IQR (CV) : 33 (0.2)\n      91 distinct values\n      \n      0\n(0.0%)\n    \n    \n      exang\n[factor]\n      1. no2. yes\n      200(67.3%)97(32.7%)\n      \n      0\n(0.0%)\n    \n    \n      oldpeak\n[numeric]\n      Mean (sd) : 1.1 (1.2)min ≤ med ≤ max:0 ≤ 0.8 ≤ 6.2IQR (CV) : 1.6 (1.1)\n      40 distinct values\n      \n      0\n(0.0%)\n    \n    \n      slope\n[factor]\n      1. upsloping2. flat3. downsloping\n      139(46.8%)137(46.1%)21(7.1%)\n      \n      0\n(0.0%)\n    \n    \n      ca\n[integer]\n      Mean (sd) : 0.7 (0.9)min ≤ med ≤ max:0 ≤ 0 ≤ 3IQR (CV) : 1 (1.4)\n      0:174(58.6%)1:65(21.9%)2:38(12.8%)3:20(6.7%)\n      \n      0\n(0.0%)\n    \n    \n      thal\n[factor]\n      1. normal2. fixed defect3. reversable defect\n      164(55.2%)18(6.1%)115(38.7%)\n      \n      0\n(0.0%)\n    \n    \n      condition\n[integer]\n      Min  : 0Mean : 0.5Max  : 1\n      0:160(53.9%)1:137(46.1%)\n      \n      0\n(0.0%)"
  },
  {
    "objectID": "logisticregression.html#data-exploration",
    "href": "logisticregression.html#data-exploration",
    "title": "4  Logistic Regression Using brms",
    "section": "4.6 Data Exploration",
    "text": "4.6 Data Exploration\nFirst lets explore some relationships between categorical explanatory variables and outcome variable\n\nlrData %>% \n  select(sex, cp, fbs, restecg, exang, slope, thal, condition) %>% \n  pivot_longer(cols = c(sex, cp, fbs, restecg, exang, slope, thal),   \n               values_to = \"value\") %>% \n  group_by(name, value) %>% \n  summarize(condition = sum(condition))\n\n# A tibble: 19 x 3\n# Groups:   name [7]\n   name    value             condition\n   <chr>   <fct>                 <int>\n 1 cp      typical angina            7\n 2 cp      atypical angina           9\n 3 cp      non-anginal pain         18\n 4 cp      asymptomatic            103\n 5 exang   no                       63\n 6 exang   yes                      74\n 7 fbs     false                   117\n 8 fbs     true                     20\n 9 restecg normal                   55\n10 restecg abnormal ST               3\n11 restecg LV hypertrophy           79\n12 sex     female                   25\n13 sex     male                    112\n14 slope   upsloping                36\n15 slope   flat                     89\n16 slope   downsloping              12\n17 thal    normal                   37\n18 thal    fixed defect             12\n19 thal    reversable defect        88"
  },
  {
    "objectID": "logisticregression.html#model-fit",
    "href": "logisticregression.html#model-fit",
    "title": "4  Logistic Regression Using brms",
    "section": "4.7 Model Fit",
    "text": "4.7 Model Fit\nWe will start with a Bayesian binary logistic regression with non-informative priors.\nbrm function is used to fit Bayesian generalized (non-)linear multivariate multilevel models using Stan for full Bayesian inference.\nThe brm has three basic arguments: formula, data, and family. warmup specifies the burn-in period (i.e. number of iterations that should be discarded); iter specifies the total number of iterations (including the burn-in iterations); chains specifies the number of chains; inits specifies the starting values of the iterations (normally you can either use the maximum likelihood esimates of the parameters as starting values, or simply ask the algorithm to start with zeros); cores specifies the number of cores used for the algorithm; seed specifies the random seed, allowing for replication of results.\n\nlrfit1 <-  brm(condition ~ age + sex + cp + trestbps + chol + fbs + restecg + thalach + exang + oldpeak + slope + ca + thal,\n           data = lrData,\n           family = bernoulli(),\n           chains = 4,\n           warmup = 1000,\n           iter = 2000,\n           seed = 12345,\n           refresh = 0,\n           backend = \"cmdstanr\")"
  },
  {
    "objectID": "logisticregression.html#model-evaluation",
    "href": "logisticregression.html#model-evaluation",
    "title": "4  Logistic Regression Using brms",
    "section": "4.8 Model Evaluation",
    "text": "4.8 Model Evaluation\n\n4.8.1 Summary\nBelow is the summary of the logistic regression model fit:\n\nsummary(lrfit1)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: condition ~ age + sex + cp + trestbps + chol + fbs + restecg + thalach + exang + oldpeak + slope + ca + thal \n   Data: lrData (Number of observations: 297) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept               -6.78      3.04   -12.81    -0.89 1.00     4434\nage                     -0.02      0.03    -0.07     0.03 1.00     4079\nsexmale                  1.73      0.55     0.68     2.84 1.00     3858\ncpatypicalangina         1.46      0.82    -0.16     3.07 1.00     3348\ncpnonManginalpain        0.30      0.70    -1.06     1.67 1.00     3631\ncpasymptomatic           2.41      0.70     1.07     3.80 1.00     2878\ntrestbps                 0.03      0.01     0.01     0.05 1.00     5297\nchol                     0.01      0.00    -0.00     0.01 1.00     5351\nfbstrue                 -0.65      0.65    -1.95     0.60 1.00     4801\nrestecgabnormalST        1.20      2.22    -2.74     5.66 1.00     4867\nrestecgLVhypertrophy     0.54      0.41    -0.25     1.34 1.00     3893\nthalach                 -0.02      0.01    -0.04     0.00 1.00     4119\nexangyes                 0.78      0.46    -0.12     1.68 1.00     4387\noldpeak                  0.43      0.25    -0.04     0.92 1.00     4092\nslopeflat                1.25      0.52     0.23     2.30 1.00     3410\nslopedownsloping         0.47      0.99    -1.56     2.31 1.00     3793\nca                       1.47      0.30     0.92     2.10 1.00     4288\nthalfixeddefect         -0.01      0.84    -1.70     1.61 1.00     4322\nthalreversabledefect     1.55      0.45     0.69     2.44 1.00     4912\n                     Tail_ESS\nIntercept                3102\nage                      3412\nsexmale                  3303\ncpatypicalangina         3069\ncpnonManginalpain        3379\ncpasymptomatic           3052\ntrestbps                 2891\nchol                     3134\nfbstrue                  3118\nrestecgabnormalST        2843\nrestecgLVhypertrophy     3196\nthalach                  2805\nexangyes                 2849\noldpeak                  2933\nslopeflat                2601\nslopedownsloping         2841\nca                       3626\nthalfixeddefect          2726\nthalreversabledefect     3112\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nLooking at the 95% credible intervals for some of the estimates, they are very wide and include zero suggesting very uncertain estimates. Based on this finding, let’s update the model and keep only sex, cp, trestbps, slope, ca, and thal as covariates.\n\nlrfit2 <-  brm(condition ~ sex + cp + trestbps + slope + ca + thal,\n           data = lrData,\n           family = bernoulli(),\n           chains = 4,\n           warmup = 1000,\n           iter = 2000,\n           seed = 12345,\n           refresh = 0,\n           backend = \"cmdstanr\")\n\nBelow is the summary of the updated logistic regression fit:\n\nsummary(lrfit2)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: condition ~ sex + cp + trestbps + slope + ca + thal \n   Data: lrData (Number of observations: 297) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept               -8.77      1.88   -12.49    -5.08 1.00     2629\nsexmale                  1.51      0.50     0.58     2.51 1.00     3479\ncpatypicalangina         1.08      0.79    -0.47     2.67 1.00     2046\ncpnonManginalpain        0.22      0.69    -1.12     1.59 1.00     1989\ncpasymptomatic           2.64      0.68     1.35     4.01 1.00     1955\ntrestbps                 0.03      0.01     0.01     0.05 1.00     4664\nslopeflat                1.95      0.44     1.10     2.83 1.00     3344\nslopedownsloping         1.43      0.73     0.04     2.89 1.00     3519\nca                       1.41      0.26     0.93     1.93 1.00     3633\nthalfixeddefect          0.09      0.75    -1.39     1.54 1.00     3626\nthalreversabledefect     1.64      0.42     0.83     2.48 1.00     4102\n                     Tail_ESS\nIntercept                2749\nsexmale                  3054\ncpatypicalangina         2519\ncpnonManginalpain        2670\ncpasymptomatic           2641\ntrestbps                 2928\nslopeflat                3188\nslopedownsloping         3028\nca                       2956\nthalfixeddefect          3112\nthalreversabledefect     2862\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n4.8.2 Model Convergence\nThe posterior distributions of the parameters: density and trace plots of the MCMC chains:\n\nplot(lrfit2)\n\n\n\n\n\n\n\n\n\n\nThe chains seem to be well mixed for all the parameters.\nbayesplot package gives us a bit more control on the plotting features.\nTrace Plots:\n\npost <- as_draws_df(lrfit2, add_chain = T)\nnames(post)\n\n [1] \"b_Intercept\"            \"b_sexmale\"              \"b_cpatypicalangina\"    \n [4] \"b_cpnonManginalpain\"    \"b_cpasymptomatic\"       \"b_trestbps\"            \n [7] \"b_slopeflat\"            \"b_slopedownsloping\"     \"b_ca\"                  \n[10] \"b_thalfixeddefect\"      \"b_thalreversabledefect\" \"lprior\"                \n[13] \"lp__\"                   \".chain\"                 \".iteration\"            \n[16] \".draw\"                 \n\n## Example with a few select parameters\nmcmc_trace(post[,c(\"b_sexmale\", \"b_trestbps\", \"b_ca\", \n                   \".chain\", \".iteration\", \".draw\")],\n           facet_args = list(ncol = 2)) +\ntheme_bw()\n\n\n\n\nAutocorrelation Plots:\n\nmcmc_acf(post, pars = c(\"b_sexmale\", \"b_trestbps\", \"b_ca\")) +\n  theme_bw()"
  },
  {
    "objectID": "logisticregression.html#frequentist-approach",
    "href": "logisticregression.html#frequentist-approach",
    "title": "4  Logistic Regression Using brms",
    "section": "4.9 Frequentist Approach",
    "text": "4.9 Frequentist Approach\nJust for fun, let’s compare the estimation process using a frequentist approach using glm.\n\nlrfit3 <- glm(formula = condition ~ sex + cp+ trestbps + slope + ca + thal, \n     family = \"binomial\", \n     data = lrData)\n\n\nsummary(lrfit3)\n\n\nCall:\nglm(formula = condition ~ sex + cp + trestbps + slope + ca + \n    thal, family = \"binomial\", data = lrData)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.8277  -0.4972  -0.1416   0.4734   2.8011  \n\nCoefficients:\n                      Estimate Std. Error z value Pr(>|z|)    \n(Intercept)           -8.18943    1.77935  -4.602 4.17e-06 ***\nsexmale                1.42390    0.47448   3.001 0.002691 ** \ncpatypical angina      1.00877    0.75051   1.344 0.178911    \ncpnon-anginal pain     0.20579    0.66020   0.312 0.755265    \ncpasymptomatic         2.47223    0.64521   3.832 0.000127 ***\ntrestbps               0.02460    0.01013   2.427 0.015205 *  \nslopeflat              1.83180    0.41358   4.429 9.46e-06 ***\nslopedownsloping       1.36450    0.70070   1.947 0.051494 .  \nca                     1.32207    0.24861   5.318 1.05e-07 ***\nthalfixed defect       0.06788    0.71713   0.095 0.924585    \nthalreversable defect  1.54348    0.40110   3.848 0.000119 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 409.95  on 296  degrees of freedom\nResidual deviance: 205.97  on 286  degrees of freedom\nAIC: 227.97\n\nNumber of Fisher Scoring iterations: 6\n\n\nComparing the model estimates:\n\nt1 <- summary(lrfit3)$coefficients[, 1:2]\nt2 <- fixef(lrfit2)[, c(1, 2, 3, 4)]\ngridExtra::grid.arrange(arrangeGrob(tableGrob(round(t1, 4), rows = NULL), top = \"Frequentist\"), \n                        arrangeGrob(tableGrob(round(t2, 4), rows = NULL), top = \"Bayesian\"), ncol = 2)\n\n\n\n\nFrom the estimates above, the Bayesian model estimates are very close to those of the frequentist model. The interpretation of these estimates is the same between these approaches. However, the interpretation of the uncertainty intervals is not the same between the two models.\nWith the frequentist model, the 95% uncertainty interval also called the confidence interval suggests that under repeated sampling, 95% of the resulting uncertainty intervals would cover the true population value. This is different from saying that there is a 95% chance that the confidence interval contains the true population value (not probability statements).\nWith the Bayesian model, the 95% uncertainty interval also called the credibility interval is more interpretable and states that there is 95% chance that the true population value falls within this interval. When the 95% credibility intervals do not contain zero, we conclude that the respective model parameters are less uncertain and likely more meaningful."
  },
  {
    "objectID": "logisticregression.html#priors",
    "href": "logisticregression.html#priors",
    "title": "4  Logistic Regression Using brms",
    "section": "4.10 Priors",
    "text": "4.10 Priors\nPrior specifications are useful in Bayesian modeling as they provide a means to include existing information on parameters of interest. As an example, if we are interested in learning about new population (e.g. pediatrics) and have the adult information on estimated parameters, including prior distributions based on adult parameters give us flexibility to explicitly apply our understanding on the estimation of such parameters for the pediatric population.\nTo see a list of all the priors that can be specified, we can use get_prior.\n\nget_prior(condition ~ sex + cp + trestbps + slope + ca + thal,\n           data = lrData)\n\n                prior     class                 coef group resp dpar nlpar lb\n               (flat)         b                                              \n               (flat)         b                   ca                         \n               (flat)         b       cpasymptomatic                         \n               (flat)         b     cpatypicalangina                         \n               (flat)         b    cpnonManginalpain                         \n               (flat)         b              sexmale                         \n               (flat)         b     slopedownsloping                         \n               (flat)         b            slopeflat                         \n               (flat)         b      thalfixeddefect                         \n               (flat)         b thalreversabledefect                         \n               (flat)         b             trestbps                         \n student_t(3, 0, 2.5) Intercept                                              \n student_t(3, 0, 2.5)     sigma                                             0\n ub       source\n         default\n    (vectorized)\n    (vectorized)\n    (vectorized)\n    (vectorized)\n    (vectorized)\n    (vectorized)\n    (vectorized)\n    (vectorized)\n    (vectorized)\n    (vectorized)\n         default\n         default\n\n\n\n4.10.1 Set Up Priors\nLet’s set some priors and let’s assume we know precisely one of the priors thalfixeddefect.\n\nprior1 <- c(set_prior(\"normal(5, 100)\", class = \"b\", coef = \"sexmale\"),\n            set_prior(\"normal(0.1, 0.03)\", class = \"b\", coef = \"thalfixeddefect\"),\n            set_prior(\"normal(5, 100)\", class = \"b\", coef = \"thalreversabledefect\"))\n\n\n\n4.10.2 Model Fit with Priors\nWe can incorporate the priors into the model as follows:\n\nlrfit4 <-  brm(condition ~ sex + cp + trestbps + slope + ca + thal,\n           data = lrData,\n           family = bernoulli(),\n           prior = prior1,\n           chains = 4,\n           warmup = 1000,\n           iter = 2000,\n           seed = 12345,\n           refresh = 0, \n           backend = \"cmdstanr\",\n           sample_prior = TRUE)\n\nTo see how the priors have been updated in the model, we use prior_summary:\n\nprior_summary(lrfit4)\n\n                prior     class                 coef group resp dpar nlpar lb\n               (flat)         b                                              \n               (flat)         b                   ca                         \n               (flat)         b       cpasymptomatic                         \n               (flat)         b     cpatypicalangina                         \n               (flat)         b    cpnonManginalpain                         \n       normal(5, 100)         b              sexmale                         \n               (flat)         b     slopedownsloping                         \n               (flat)         b            slopeflat                         \n    normal(0.1, 0.03)         b      thalfixeddefect                         \n       normal(5, 100)         b thalreversabledefect                         \n               (flat)         b             trestbps                         \n student_t(3, 0, 2.5) Intercept                                              \n ub       source\n         default\n    (vectorized)\n    (vectorized)\n    (vectorized)\n    (vectorized)\n            user\n    (vectorized)\n    (vectorized)\n            user\n            user\n    (vectorized)\n         default\n\n\n\nsummary(lrfit4)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: condition ~ sex + cp + trestbps + slope + ca + thal \n   Data: lrData (Number of observations: 297) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept               -8.78      1.84   -12.53    -5.28 1.00     2760\nsexmale                  1.51      0.47     0.61     2.44 1.00     3450\ncpatypicalangina         1.12      0.77    -0.36     2.67 1.00     2617\ncpnonManginalpain        0.26      0.69    -1.08     1.61 1.00     2529\ncpasymptomatic           2.66      0.67     1.40     4.02 1.00     2378\ntrestbps                 0.03      0.01     0.01     0.05 1.00     4305\nslopeflat                1.95      0.41     1.16     2.77 1.00     3480\nslopedownsloping         1.45      0.72     0.08     2.86 1.00     4050\nca                       1.41      0.26     0.93     1.97 1.00     3831\nthalfixeddefect          0.10      0.03     0.04     0.16 1.00     4923\nthalreversabledefect     1.64      0.40     0.84     2.43 1.00     4412\n                     Tail_ESS\nIntercept                2667\nsexmale                  3235\ncpatypicalangina         2902\ncpnonManginalpain        2960\ncpasymptomatic           2460\ntrestbps                 2811\nslopeflat                2927\nslopedownsloping         2639\nca                       2790\nthalfixeddefect          2850\nthalreversabledefect     2905\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n4.10.3 Compare Prior and Posterior Samples\nLet’s compare the prior and posterior distributions for a non-informative prior sexmale and a highly informative prior thalfixeddefect\n\npriorSamples <- prior_draws(lrfit4, c(\"b_sexmale\", \"b_thalfixeddefect\")) \nposteriorSamples <- as_draws_df(lrfit4, c(\"b_sexmale\", \"b_thalfixeddefect\"))\np1 <- ggplot() +\n  geom_density(data = priorSamples, aes(x = b_sexmale, fill = \"prior\"), alpha = 0.5) +\n  geom_density(data = posteriorSamples, aes(x = b_sexmale, fill = \"posterior\"), alpha = 0.5) +\n  scale_fill_manual(name = \"\", values = c(\"prior\" = \"lightblue\", \"posterior\" = \"darkblue\")) +\n  scale_x_continuous (limits = c(-10, 10)) + theme_bw()\np2 <- ggplot() +\n  geom_density(data = priorSamples, aes(x = b_thalfixeddefect, fill = \"prior\"), alpha = 0.5) +\n  geom_density(data = posteriorSamples, aes(x = b_thalfixeddefect, fill = \"posterior\"), alpha = 0.5) +\n  scale_fill_manual(name = \"\", values = c(\"prior\" = \"lightblue\", \"posterior\" = \"darkblue\")) +\n  scale_x_continuous(limits = c(-0.1, 0.3)) + theme_bw() + theme(legend.position = \"top\")\np3 <- p1 + p2 & theme(legend.position = \"top\")\np3 + plot_layout(guides = \"collect\")\n\nWarning: Removed 3696 rows containing non-finite values (stat_density)."
  },
  {
    "objectID": "logisticregression.html#hands-on-example",
    "href": "logisticregression.html#hands-on-example",
    "title": "4  Logistic Regression Using brms",
    "section": "4.11 Hands-On Example",
    "text": "4.11 Hands-On Example\nA simulated dataset for this exercise simlrcovs.csv was developed. It has the following columns:\n\nDOSE: Dose of drug in mg [20, 50, 100, 200 mg]\nCAVG: Average concentration until the time of the event (mg/L)\nECOG: ECOG performance status [0 = Fully active; 1 = Restricted in physical activity]\nRACE: Race [1 = Others; 2 = White]\nSEX: Sex [1 = Female; 2 = Male]\nBRNMETS: Brain metastasis [1 = Yes; 0 = No]\nDV: Event [1 = Yes; 0 = No]\n\n\n4.11.1 Import Dataset\n\n# Read the dataset\nhoRaw <- read.csv(\"data/simlrcovs.csv\") %>% \n  as_tibble()\n\n\n\n4.11.2 Data Processing\nConvert categorical explanatory variables to factors\n\nhoData <- hoRaw %>% \n  mutate(ECOG = factor(ECOG, levels = c(0, 1), labels = c(\"Active\", \"Restricted\")),\n         RACE = factor(RACE, levels = c(0, 1), labels = c(\"White\", \"Others\")),\n         SEX = factor(SEX, levels = c(0, 1), labels = c(\"Male\", \"Female\")),\n         BRNMETS = factor(BRNMETS, levels = c(0, 1), labels = c(\"No\", \"Yes\")))\nhoData\n\n# A tibble: 200 x 7\n    DOSE  CAVG ECOG       RACE   SEX    BRNMETS    DV\n   <int> <dbl> <fct>      <fct>  <fct>  <fct>   <int>\n 1    20  203. Active     White  Female Yes         0\n 2    20  202. Restricted White  Female No          0\n 3    20  287. Restricted Others Female No          0\n 4    20  174. Restricted Others Male   Yes         0\n 5    20  270. Active     Others Male   Yes         0\n 6    20  265. Active     Others Female No          1\n 7    20  206. Restricted Others Female No          0\n 8    20  253. Active     Others Male   No          1\n 9    20  186. Active     White  Male   No          0\n10    20  186. Restricted Others Female No          1\n# ... with 190 more rows\n\n\n\n\n4.11.3 Data Summary\n\n  xxxxxx\n\n\n\n4.11.4 Model Fit\nWith all covariates except DOSE (since we have exposure as a driver)\n\nhofit1 <-  brm(xxxxxx ~ xxxxxx,\n           data = xxxxxx,\n           family = xxxxxx,\n           chains = 4,\n           warmup = 1000,\n           iter = 2000,\n           seed = 12345,\n           refresh = 0,\n           backend = \"cmdstanr\")\n\n\n\n4.11.5 Model Evaluation\nGet the summary of the model and look at the fixed efects\n\nxxxxxx\n\n\n\n4.11.6 Final Model\nRefit the model with meaningful covariates\n\nhofit2 <-  brm(xxxxxx ~ xxxxxx,\n           data = xxxxxx,\n           family = xxxxxx,\n           chains = 4,\n           warmup = 1000,\n           iter = 2000,\n           seed = 12345,\n           refresh = 0,\n           backend = \"cmdstanr\")\n\n\n\n4.11.7 Summary\n\nxxxxxx\n\n\n\n4.11.8 Model Convergence\n\nhopost <- as_draws_df(xxxxxx, add_chain = T)\nmcmc_trace(xxxxxx) +\n  theme_bw()\n\n\nmcmc_acf(xxxxxx) + \n  theme_bw()\n\n\n\n4.11.9 Visual Interpretation of the Model (Bonus Points!)\nWe can do this two ways.\n\n4.11.9.1 Generate Posterior Probabilities Manually\nGenerate posterior probability of the event using the estimates and their associated posterior distributions\n\nout <- hofit2 %>%\n  spread_draws(b_Intercept, b_CAVG, b_RACEOthers) %>% \n  mutate(CAVG = list(seq(100, 4000, 10))) %>% \n  unnest(cols = c(CAVG)) %>%\n  mutate(RACE = list(0:1)) %>% \n  unnest(cols = c(RACE)) %>% \n  mutate(PRED = exp(b_Intercept + b_CAVG * CAVG + b_RACEOthers * RACE)/(1 + exp(b_Intercept + b_CAVG * CAVG + b_RACEOthers * RACE))) %>%\n  group_by(CAVG, RACE) %>%\n  summarise(pred_m = mean(PRED, na.rm = TRUE),\n            pred_low = quantile(PRED, prob = 0.025),\n            pred_high = quantile(PRED, prob = 0.975)) %>% \n  mutate(RACE = factor(RACE, levels = c(0, 1), labels = c(\"White\", \"Others\")))\n\nPlot The Probability of the Event vs Average Concentration\n\nout %>%\n  ggplot(aes(x = CAVG, y = pred_m, color = factor(RACE))) +\n  geom_line() +\n  geom_ribbon(aes(ymin = pred_low, ymax = pred_high, fill = factor(RACE)), alpha = 0.2) +\n  ylab(\"Predicted Probability of the Event\\n\") +\n  xlab(\"\\nAverage Concentration until the Event (mg/L)\") +\n  theme_bw() + \n  scale_fill_discrete(\"\") +\n  scale_color_discrete(\"\") +\n  theme(legend.position = \"top\")\n\n\n\n4.11.9.2 Generate Posterior Probabilities Using Helper Functions from brms and tidybayes\nGenerate posterior probability of the event using the estimates and their associated posterior distributions\n\nout2 <- hofit2 %>%\n  epred_draws(newdata = expand_grid(CAVG = seq(100, 4000, by = 10), \n                                    RACE = c(\"White\", \"Others\")),\n              value = \"PRED\") %>% \n  ungroup() %>% \n  mutate(RACE = factor(RACE, levels = c(\"White\", \"Others\"), \n                       labels = c(\"White\", \"Others\")))\n\nPlot The Probability of the Event vs Average Concentration\n\nout2 %>% \n  ggplot() +\n  stat_lineribbon(aes(x = CAVG, y = PRED, color = RACE, fill = RACE), \n                  .width = 0.95, alpha = 0.25) +\n  ylab(\"Predicted Probability of the Event\\n\") +\n  xlab(\"\\nAverage Concentration until the Event (mg/L)\") +\n  theme_bw() + \n  scale_fill_discrete(\"\") +\n  scale_color_discrete(\"\") +\n  theme(legend.position = \"top\") +\n  ylim(c(0, 1))"
  },
  {
    "objectID": "installation.html",
    "href": "installation.html",
    "title": "Installation",
    "section": "",
    "text": "Necessary packages (including cmdstanr)\nCmdStan, the backend for using pure Stan (this part is optional, but it will install the newest version)\nTorsten\n\nAs mentioned, CmdStan on its own is optional since the Torsten installation also installs CmdStan, but it’s nice to have the most up-to-date version of CmdStan available sometimes.\nIt will also run some tests to make sure everything is installed and running properly.\nDISCLAIMER: These aren’t fool-proof. Installation and setup can be a bit tricky, particularly with Windows. There is a lot of help online and on the message boards.\n\nWindowsMac (and Linux I think)Message-Passing Interface on Metworx\n\n\nFor Windows, we’ve had trouble using Torsten with the newest versions of R (>= 4.2) so we recommend R Version 4.1.3 and RTools 4.0.\n\n# INSTRUCTIONS - GO THROUGH THIS SCRIPT AFTER OPENING R IN A FRESH R SESSION \n# AND NOT WITHIN ANY R PROJECT (.Rproj)\n\n# Make sure necessary packages are installed\nif(!require(\"tidyverse\", character.only = TRUE)){\n  install.packages(\"tidyverse\", dependencies = TRUE)\n  library(\"tidyverse\",  character.only = TRUE)\n}\n\nif(any(grepl(\"tidyverse\", search()))){\n  cat(\"'tidyverse' is attached. Continue.\" )\n}else{\n  warning(strwrap(\"'tidyverse' is not attached. Go back and make sure it is \n                  installed and attached.\"))\n}\n\n## TODO: We can add more packages if necessary\npackages <- c(\"bayesplot\", \"brms\", \"collapsibleTree\", \"patchwork\", \"posterior\",\n              \"rstanarm\", \"tidybayes\", \"ggforce\", \"gganimate\", \"gifski\", \n              \"ggpubr\", \"latex2exp\")\n\nwalk(packages, .f = function(.x){\n  if(!require(.x, character.only = TRUE)){\n    install.packages(.x, dependencies = TRUE)\n    library(.x, character.only = TRUE)\n  }\n})\n\n# Install cmdstanr\n## we recommend running this in a fresh R session or restarting your current \n## session\nif(!require(\"cmdstanr\", character.only = TRUE)){\n  install.packages(\"cmdstanr\", repos = c(\"https://mc-stan.org/r-packages/\", \n                                         getOption(\"repos\")))\n  library(\"cmdstanr\",  character.only = TRUE)\n}\n\nwalk(c(packages, \"cmdstanr\"), \n     .f = function(.x) {\n       if(any(str_detect(search(), .x))){\n         cat(str_c(\"'\", .x, \"'\", \" is attached. Continue.\\n\"))\n       }else{\n         warning(str_wrap(str_c(.x, \" is not attached. Go back and make sure it \n         is installed and attached.\")))\n       }\n     })\n\n# Make sure C++ toolchain is installed and working properly. For Windows this\n# means RTools. It should be fine in Mac and Linux\ncheck_cmdstan_toolchain(fix = TRUE, quiet = TRUE)\n\n# Install CmdStan  (optional)\n## Change parallel::detectCores() to whatever number you want to use. \n## More cores = faster\ninstall_cmdstan(cores = parallel::detectCores()) \n\n# Check if CmdStan is working properly\nmodel_cmdstan <- cmdstan_model(file.path(cmdstan_path(), \"examples\", \n                                         \"bernoulli\", \"bernoulli.stan\"))\nmodel_cmdstan$print()\ndata_list <- list(N = 10, y = c(0,1,0,0,0,0,0,0,0,1))\n\nfit_cmdstan <- model_cmdstan$sample(data = data_list, \n                                    seed = 345, \n                                    chains = 4, \n                                    parallel_chains = 4,\n                                    refresh = 500)\n\nfit_cmdstan$summary()\n\n# Install Torsten (mandatory)\nsystem(\"git clone https://github.com/metrumresearchgroup/Torsten.git\")\nshell(\"cd Torsten/cmdstan && mingw32-make build\")\n\nset_cmdstan_path(\"~/Torsten/cmdstan/\")\ncmdstan_version()\n\n\n# Check to see if Torsten is working properly (mandatory)\n## First with a simple model in pure Stan code\nmodel_torsten <- cmdstan_model(file.path(cmdstan_path(), \"examples\", \n                                         \"bernoulli\", \"bernoulli.stan\"))\nmodel_torsten$print()\ndata_list <- list(N = 10, y = c(0,1,0,0,0,0,0,0,0,1))\n\nfit_torsten <- model_torsten$sample(data = data_list, \n                                    seed = 345, \n                                    chains = 4, \n                                    parallel_chains = 4,\n                                    refresh = 500)\n\nfit_torsten$summary()\n\n## Now with a simple two-compartment model that actually uses Torsten functions\nfile_path_base <- file.path(\"~\", \"Torsten\", \"example-models\", \"pk2cpt\")\nmodel_pk2cpt <- cmdstan_model(file.path(file_path_base, \"pk2cpt.stan\"))\n\nfit_pk2cpt <- model_pk2cpt$sample(data = file.path(file_path_base, \n                                                   \"pk2cpt.data.R\"),\n                                  seed = 345, \n                                  iter_warmup = 1000,\n                                  iter_sampling = 1000,\n                                  chains = 4,\n                                  parallel_chains = 4,\n                                  refresh = 500)\n\nfit_pk2cpt$summary()\n\n\n\nInstallation on Mac/Linux is easy, since a C++ compiler comes pre-installed.\n\n# INSTRUCTIONS - GO THROUGH THIS SCRIPT AFTER OPENING R IN A FRESH R SESSION \n# AND NOT WITHIN ANY R PROJECT (.Rproj)\n\n# Make sure necessary packages are installed\nif(!require(\"tidyverse\", character.only = TRUE)){\n  install.packages(\"tidyverse\", dependencies = TRUE)\n  library(\"tidyverse\",  character.only = TRUE)\n}\n\nif(any(grepl(\"tidyverse\", search()))){\n  cat(\"'tidyverse' is attached. Continue.\" )\n}else{\n  warning(strwrap(\"'tidyverse' is not attached. Go back and make sure it is \n                  installed and attached.\"))\n}\n\n## You can add more packages if necessary\npackages <- c(\"bayesplot\", \"brms\", \"collapsibleTree\", \"patchwork\", \"posterior\",\n              \"rstanarm\", \"tidybayes\", \"ggforce\", \"gganimate\", \"gifski\", \n              \"ggpubr\", \"latex2exp\")\n\nwalk(packages, .f = function(.x){\n  if(!require(.x, character.only = TRUE)){\n    install.packages(.x, dependencies = TRUE)\n    library(.x, character.only = TRUE)\n  }\n})\n\n# Install cmdstanr\n## we recommend running this in a fresh R session or restarting your current \n## session\nif(!require(\"cmdstanr\", character.only = TRUE)){\n  install.packages(\"cmdstanr\", repos = c(\"https://mc-stan.org/r-packages/\", \n                                         getOption(\"repos\")))\n  library(\"cmdstanr\",  character.only = TRUE)\n}\n\nwalk(c(packages, \"cmdstanr\"), \n     .f = function(.x) {\n       if(any(str_detect(search(), .x))){\n         cat(str_c(\"'\", .x, \"'\", \" is attached. Continue.\\n\"))\n       }else{\n         warning(str_wrap(str_c(.x, \" is not attached. Go back and make sure it \n         is installed and attached.\")))\n       }\n     })\n     \n# Make sure C++ toolchain is installed and working properly. For Windows this\n# means RTools. It should be fine in Mac and Linux\ncheck_cmdstan_toolchain(fix = TRUE, quiet = TRUE)\n\n# Install CmdStan  (optional)\n## Change parallel::detectCores() to whatever number you want to use. \n## More cores = faster\ninstall_cmdstan(cores = parallel::detectCores()) \n\n# Check if CmdStan is working properly\nmodel_cmdstan <- cmdstan_model(file.path(cmdstan_path(), \"examples\", \n                                         \"bernoulli\", \"bernoulli.stan\"))\nmodel_cmdstan$print()\ndata_list <- list(N = 10, y = c(0,1,0,0,0,0,0,0,0,1))\n\nfit_cmdstan <- model_cmdstan$sample(data = data_list, \n                                    seed = 345, \n                                    chains = 4, \n                                    parallel_chains = 4,\n                                    refresh = 500)\n\nfit_cmdstan$summary()\n\n# Install Torsten (mandatory)\nsystem(\"git clone https://github.com/metrumresearchgroup/Torsten.git\")\n\nset_cmdstan_path(\"~/Torsten/cmdstan/\")\ncmdstan_version()\n\n\n# Check to see if Torsten is working properly (mandatory)\n## First with a simple model in pure Stan code\nmodel_torsten <- cmdstan_model(file.path(cmdstan_path(), \"examples\", \n                                 \"bernoulli\", \"bernoulli.stan\"))\nmodel_torsten$print()\ndata_list <- list(N = 10, y = c(0,1,0,0,0,0,0,0,0,1))\n\nfit_torsten <- model_torsten$sample(data = data_list, \n                    seed = 345, \n                    chains = 4, \n                    parallel_chains = 4,\n                    refresh = 500)\n\nfit_torsten$summary()\n\n## Now with a simple two-compartment model that actually uses Torsten functions\nfile_path_base <- file.path(cmdstan_path(), \"..\", \"example-models\", \"pk2cpt\")\nmodel_pk2cpt <- cmdstan_model(file.path(file_path_base, \"pk2cpt.stan\"))\n\nfit_pk2cpt <- model_pk2cpt$sample(data = file.path(file_path_base, \n                                                   \"pk2cpt.data.R\"),\n                                  seed = 345, \n                                  iter_warmup = 1000,\n                                  iter_sampling = 1000,\n                                  chains = 4,\n                                  parallel_chains = 4,\n                                  refresh = 500)\n\nfit_pk2cpt$summary()\n\n\n\nThe following goes in a .sh file and should setup a message-passing interface (MPI).\n\n# 1) Download MPICH and unzip\nwget https://www.mpich.org/static/downloads/4.0.2/mpich-4.0.2.tar.gz\ntar xfz mpich-4.0.2.tar.gz\n\n# 2) Make installation directory and temporary build directory\nmkdir mpich-install\nmkdir mpich-4.0.2-temp-build\n\n# 3) Configure MPICH\nexport MAINDIR=$(pwd)\ncd mpich-4.0.2-temp-build\n../mpich-4.0.2/configure -prefix=$MAINDIR/mpich-install 2>&1 | tee c.txt\n\n# 4) Build MPICH (takes at least 50 minutes)\ntime make 2>&1 | tee m.txt\n\n# 5) Install MPICH commands\nmake install 2>&1 | tee mi.txt\n\n# 6) Add to Install Path\nexport PATH=$MAINDIR/mpich-install/bin:$PATH\n\n# 7) Create new make/local file to tell Torsten which MPI to use\ncd ../Torsten/cmdstan\necho \"TORSTEN_MPI=1\" > make/local\necho \"TBB_CXX_TYPE=gcc\" >> make/local\necho \"CXX=mpicxx\" >> make/local\necho \"CC=mpicc\" >> make/local\necho \"CXXFLAGS += -isystem $MAINDIR/mpich-install/include\" >> make/local\n\n# 8) Make Stan model with MPI\nmake clean-all\nmake ../example-models/twocpt_population/twocpt_population\n\n# 9) Run\ncd ../example-models/twocpt_population\nmpiexec -n 2 ./twocpt_population \\\n  sample num_samples=50 num_warmup=50 algorithm=hmc engine=nuts max_depth=1 \\\n  data file=twocpt_population.data.R init=twocpt_population.init.R\n  \n\n# 10) Create hostfile and test on worker nodes\nqconf -sel > hostfile\n\nmpiexec -n 4 -bind-to core -f hostfile -l ./twocpt_population \\\n  sample num_samples=50 num_warmup=50 algorithm=hmc engine=nuts max_depth=1 \\\n  data file=twocpt_population.data.R init=twocpt_population.init.R"
  },
  {
    "objectID": "logreg_ho_solution.html",
    "href": "logreg_ho_solution.html",
    "title": "Logistic Regression Hands-On With Solution",
    "section": "",
    "text": "library(summarytools)\nlibrary(tidyverse) \nlibrary(brms)\nlibrary(bayesplot)\nlibrary(tidybayes) \nlibrary(gridExtra) \nlibrary(patchwork) \n\nA simulated dataset for this exercise simlrcovs.csv was developed. It has the following columns:\n\nDOSE: Dose of drug in mg [20, 50, 100, 200 mg]\nCAVG: Average concentration until the time of the event (mg/L)\nECOG: ECOG performance status [0 = Fully active; 1 = Restricted in physical activity]\nRACE: Race [1 = Others; 2 = White]\nSEX: Sex [1 = Female; 2 = Male]\nBRNMETS: Brain metastasis [1 = Yes; 0 = No]\nDV: Event [1 = Yes; 0 = No]"
  },
  {
    "objectID": "logreg_ho_solution.html#import-dataset",
    "href": "logreg_ho_solution.html#import-dataset",
    "title": "Logistic Regression Hands-On With Solution",
    "section": "Import Dataset",
    "text": "Import Dataset\n\n# Read the dataset\nhoRaw <- read.csv(\"data/simlrcovs.csv\") %>% \n  as_tibble()"
  },
  {
    "objectID": "logreg_ho_solution.html#data-processing",
    "href": "logreg_ho_solution.html#data-processing",
    "title": "Logistic Regression Hands-On With Solution",
    "section": "Data Processing",
    "text": "Data Processing\nConvert categorical explanatory variables to factors\n\nhoData <- hoRaw %>% \n  mutate(ECOG = factor(ECOG, levels = c(0, 1), labels = c(\"Active\", \"Restricted\")),\n         RACE = factor(RACE, levels = c(0, 1), labels = c(\"White\", \"Others\")),\n         SEX = factor(SEX, levels = c(0, 1), labels = c(\"Male\", \"Female\")),\n         BRNMETS = factor(BRNMETS, levels = c(0, 1), labels = c(\"No\", \"Yes\")))\nhoData\n\n# A tibble: 200 x 7\n    DOSE  CAVG ECOG       RACE   SEX    BRNMETS    DV\n   <int> <dbl> <fct>      <fct>  <fct>  <fct>   <int>\n 1    20  203. Active     White  Female Yes         0\n 2    20  202. Restricted White  Female No          0\n 3    20  287. Restricted Others Female No          0\n 4    20  174. Restricted Others Male   Yes         0\n 5    20  270. Active     Others Male   Yes         0\n 6    20  265. Active     Others Female No          1\n 7    20  206. Restricted Others Female No          0\n 8    20  253. Active     Others Male   No          1\n 9    20  186. Active     White  Male   No          0\n10    20  186. Restricted Others Female No          1\n# ... with 190 more rows"
  },
  {
    "objectID": "logreg_ho_solution.html#data-summary",
    "href": "logreg_ho_solution.html#data-summary",
    "title": "Logistic Regression Hands-On With Solution",
    "section": "Data Summary",
    "text": "Data Summary\n\nprint(summarytools::dfSummary(hoData,\n          varnumbers = FALSE,\n          valid.col = FALSE,\n          graph.magnif = 0.76),\n      method = \"render\")\n\n\n\nData Frame Summary\n\nhoData\n\n\nDimensions: 200 x 7\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      DOSE\n[integer]\n      Mean (sd) : 92.5 (68.5)min ≤ med ≤ max:20 ≤ 75 ≤ 200IQR (CV) : 82.5 (0.7)\n      20:50(25.0%)50:50(25.0%)100:50(25.0%)200:50(25.0%)\n      \n      0\n(0.0%)\n    \n    \n      CAVG\n[numeric]\n      Mean (sd) : 1155.1 (918.3)min ≤ med ≤ max:164 ≤ 928.6 ≤ 3791.7IQR (CV) : 1423.5 (0.8)\n      200 distinct values\n      \n      0\n(0.0%)\n    \n    \n      ECOG\n[factor]\n      1. Active2. Restricted\n      122(61.0%)78(39.0%)\n      \n      0\n(0.0%)\n    \n    \n      RACE\n[factor]\n      1. White2. Others\n      70(35.0%)130(65.0%)\n      \n      0\n(0.0%)\n    \n    \n      SEX\n[factor]\n      1. Male2. Female\n      90(45.0%)110(55.0%)\n      \n      0\n(0.0%)\n    \n    \n      BRNMETS\n[factor]\n      1. No2. Yes\n      145(72.5%)55(27.5%)\n      \n      0\n(0.0%)\n    \n    \n      DV\n[integer]\n      Min  : 0Mean : 0.5Max  : 1\n      0:95(47.5%)1:105(52.5%)\n      \n      0\n(0.0%)"
  },
  {
    "objectID": "logreg_ho_solution.html#model-fit",
    "href": "logreg_ho_solution.html#model-fit",
    "title": "Logistic Regression Hands-On With Solution",
    "section": "Model Fit",
    "text": "Model Fit\nWith all covariates except DOSE (since we have exposure as a driver)\n\nhofit1 <-  brm(DV ~ CAVG + ECOG + RACE + SEX + BRNMETS,\n           data = hoData,\n           family = bernoulli(),\n           chains = 4,\n           warmup = 1000,\n           iter = 2000,\n           seed = 12345,\n           refresh = 0,\n           backend = \"cmdstanr\")\n# freqhofit <-  glm(DV ~ CAVG + ECOG + RACE + SEX + BRNMETS,\n#                family = \"binomial\",\n#                data = hoData)\n# summary(freqhofit)"
  },
  {
    "objectID": "logreg_ho_solution.html#model-evaluation",
    "href": "logreg_ho_solution.html#model-evaluation",
    "title": "Logistic Regression Hands-On With Solution",
    "section": "Model Evaluation",
    "text": "Model Evaluation\n\nsummary(hofit1)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: DV ~ CAVG + ECOG + RACE + SEX + BRNMETS \n   Data: hoData (Number of observations: 200) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept         -1.67      0.44    -2.56    -0.84 1.00     4103     3331\nCAVG               0.00      0.00     0.00     0.00 1.00     4075     2528\nECOGRestricted    -0.47      0.34    -1.14     0.20 1.00     3892     2734\nRACEOthers         1.45      0.36     0.76     2.18 1.00     3795     2389\nSEXFemale         -0.21      0.32    -0.84     0.41 1.00     3662     2737\nBRNMETSYes         0.20      0.37    -0.51     0.93 1.00     4029     2623\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nfixef(hofit1)\n\n                    Estimate    Est.Error         Q2.5        Q97.5\nIntercept      -1.6728321010 0.4356726064 -2.557614750 -0.840452675\nCAVG            0.0009840576 0.0002037094  0.000591061  0.001394632\nECOGRestricted -0.4703165395 0.3379044752 -1.136986750  0.199271800\nRACEOthers      1.4457574770 0.3599390856  0.756435900  2.179535750\nSEXFemale      -0.2140195943 0.3187733320 -0.840274025  0.406492000\nBRNMETSYes      0.2011028320 0.3668881875 -0.506824900  0.928938150"
  },
  {
    "objectID": "logreg_ho_solution.html#final-model",
    "href": "logreg_ho_solution.html#final-model",
    "title": "Logistic Regression Hands-On With Solution",
    "section": "Final Model",
    "text": "Final Model\n\nhofit2 <-  brm(DV ~ CAVG + RACE,\n           data = hoData,\n           family = bernoulli(),\n           chains = 4,\n           warmup = 1000,\n           iter = 2000,\n           seed = 12345,\n           refresh = 0,\n           backend = \"cmdstanr\")"
  },
  {
    "objectID": "logreg_ho_solution.html#summary",
    "href": "logreg_ho_solution.html#summary",
    "title": "Logistic Regression Hands-On With Solution",
    "section": "Summary",
    "text": "Summary\n\nsummary(hofit2)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: DV ~ CAVG + RACE \n   Data: hoData (Number of observations: 200) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     -1.81      0.37    -2.55    -1.10 1.00     1962     2405\nCAVG           0.00      0.00     0.00     0.00 1.00     4324     3163\nRACEOthers     1.33      0.33     0.67     2.02 1.00     1732     1299\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nfixef(hofit2)\n\n                Estimate    Est.Error          Q2.5        Q97.5\nIntercept  -1.8104092685 0.3672660010 -2.5532470000 -1.095532250\nCAVG        0.0009570336 0.0002078047  0.0005667638  0.001378035\nRACEOthers  1.3294440837 0.3334109622  0.6721790500  2.015080000"
  },
  {
    "objectID": "logreg_ho_solution.html#model-convergence",
    "href": "logreg_ho_solution.html#model-convergence",
    "title": "Logistic Regression Hands-On With Solution",
    "section": "Model Convergence",
    "text": "Model Convergence\n\nhopost <- as_draws_df(hofit2, add_chain = T)\nmcmc_trace(hopost[, -4],\n           facet_args = list(ncol = 2)) +\n  theme_bw()\n\n\n\n\n\nmcmc_acf(hopost[, -4]) + \n  theme_bw()"
  },
  {
    "objectID": "logreg_ho_solution.html#visual-interpretation-of-the-model-bonus-points",
    "href": "logreg_ho_solution.html#visual-interpretation-of-the-model-bonus-points",
    "title": "Logistic Regression Hands-On With Solution",
    "section": "Visual Interpretation of the Model (Bonus Points!)",
    "text": "Visual Interpretation of the Model (Bonus Points!)\nWe can do this two ways.\n\nGenerate Posterior Probabilities Manually\nGenerate posterior probability of the event using the estimates and their associated posterior distributions\n\nout <- hofit2 %>%\n  spread_draws(b_Intercept, b_CAVG, b_RACEOthers) %>% \n  mutate(CAVG = list(seq(100, 4000, 10))) %>% \n  unnest(cols = c(CAVG)) %>%\n  mutate(RACE = list(0:1)) %>% \n  unnest(cols = c(RACE)) %>% \n  mutate(PRED = exp(b_Intercept + b_CAVG * CAVG + b_RACEOthers * RACE)/(1 + exp(b_Intercept + b_CAVG * CAVG + b_RACEOthers * RACE))) %>%\n  group_by(CAVG, RACE) %>%\n  summarise(pred_m = mean(PRED, na.rm = TRUE),\n            pred_low = quantile(PRED, prob = 0.025),\n            pred_high = quantile(PRED, prob = 0.975)) %>% \n  mutate(RACE = factor(RACE, levels = c(0, 1), labels = c(\"White\", \"Others\")))\n\nPlot The Probability of the Event vs Average Concentration\n\nout %>%\n  ggplot(aes(x = CAVG, y = pred_m, color = factor(RACE))) +\n  geom_line() +\n  geom_ribbon(aes(ymin = pred_low, ymax = pred_high, fill = factor(RACE)), alpha = 0.2) +\n  ylab(\"Predicted Probability of the Event\\n\") +\n  xlab(\"\\nAverage Concentration until the Event (mg/L)\") +\n  theme_bw() + \n  scale_fill_discrete(\"\") +\n  scale_color_discrete(\"\") +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\nGenerate Posterior Probabilities Using Helper Functions from brms and tidybayes\nGenerate posterior probability of the event using the estimates and their associated posterior distributions\n\nout2 <- hofit2 %>%\n  epred_draws(newdata = expand_grid(CAVG = seq(100, 4000, by = 10), \n                                    RACE = c(\"White\", \"Others\")),\n              value = \"PRED\") %>% \n  ungroup() %>% \n  mutate(RACE = factor(RACE, levels = c(\"White\", \"Others\"), \n                       labels = c(\"White\", \"Others\")))\n\nPlot The Probability of the Event vs Average Concentration\n\nout2 %>% \n  ggplot() +\n  stat_lineribbon(aes(x = CAVG, y = PRED, color = RACE, fill = RACE), \n                  .width = 0.95, alpha = 0.25) +\n  ylab(\"Predicted Probability of the Event\\n\") +\n  xlab(\"\\nAverage Concentration until the Event (mg/L)\") +\n  theme_bw() + \n  scale_fill_discrete(\"\") +\n  scale_color_discrete(\"\") +\n  theme(legend.position = \"top\") +\n  ylim(c(0, 1))"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About the Authors",
    "section": "",
    "text": "Yasong Lu\n\n\n\n\n\nYasong was trained as a quantitative toxicologist with specialty in PBPK/PD modeling at Colorado State University. After his PhD training, Yasong worked in the pharmaceutical industry as a PK/PD modeler or pharmacometrician supporting drug discovery (at Pfizer) and development programs (at BMS, Novartis, and currently Daiichi Sankyo). He started learning Bayesian analysis and applying the technique to PopPK and PK/PD analysis from about 10 years ago. Currently he’s learning Stan and trying to apply it to his work.\n\n\nArya Pourzanjani\n\n\n\n\n\nArya Pourzanjani is a senior scientist in the Clinical Pharmacology Modeling and Simulation (CPMS) department at Amgen. He specializes in pharmacometrics, data science, machine learning, and Bayesian applications to these areas.\n\n\nPavan Vaddady\n\n\n\n\n\nPavan Vaddady currently serves as the Head of Advanced Pharmacometrics within the Quantitative Clinical Pharmacology Department at Daiichi Sankyo, Inc. During his career, he led several early and late-stage development programs across multiple therapeutic areas both as a clinical pharmacologist and a pharmacometrician and applied model informed approaches to impact key drug development decisions. His current role involves developing a team of scientists for advanced pharmacometrics aspects including complex pharmacometrics modeling and simulation, disease progression, AI/ML, Bayesian approaches, MBMA across a portfolio of compounds. He is passionate about teaching and mentoring colleagues and has delivered comprehensive courses and tutorials on NONMEM, R, Stan, and Shiny for pharmacometricians. He obtained his B. Pharm. (Hons.), and M. Pharm. from BITS Pilani, India and his Ph.D. in pharmaceutical sciences from the University of Tennessee Health Science Center, Memphis, USA."
  }
]